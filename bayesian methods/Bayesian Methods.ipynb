{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistics  \n",
    "## Parametric Family of Densities\n",
    "- A parametric family of densities is a set  \n",
    "$$\\{p(y \\mid \\theta): \\theta \\in \\Theta\\}$$  \n",
    "where $p(y|\\theta)$ is a density on sample space $Y$, $\\theta$ is a parameter in a finite dimension space $\\Theta$  \n",
    "\n",
    "## Frequentist or “Classical” Statistics\n",
    "- Assume that $p(y | \\theta)$ governs the world we are observing, for some $\\theta \\in \\Theta$.  \n",
    "- If we knew the right $θ\\in \\Theta$, there would be no need for statistics  \n",
    "- Instead of $\\theta$, we have data $D: y_1,...,y_n$ sampled i.i.d. $p(y | \\theta)$.  \n",
    "- Statistics is about how to get by with $D$ in place of $\\theta$.  \n",
    "\n",
    "## Point Estimation\n",
    "- One type of statistical problem is point estimation.  \n",
    "- A statistic $s = s(D)$ is any function of the data.  \n",
    "- A statistic $\\hat{\\theta} = \\hat{\\theta}(D)$ taking values in $\\Theta$ is a point estimator of $\\theta$.   \n",
    "  - A good point estimator will have $\\hat{\\theta} \\approx \\theta$.  \n",
    "  \n",
    "## Desirable Properties of Point Estimators\n",
    "- **Consistency**: As data size $n \\to \\infty$, we get $\\hat{\\theta}_n \\to \\theta$.  \n",
    "- **Efficiency**: (Roughly speaking) $\\hat{\\theta}_n$ is as accurate as we can get from a sample of size $n$.  \n",
    "eg.  Maximum likelihood estimators are consistent and eﬃcient under reasonable conditions  \n",
    "\n",
    "## The Likelihood Function\n",
    "- For parametric family $\\{p(y | \\theta):\\theta \\in \\Theta \\}$ and i.i.d. sample $D = (y_1,...,y_n)$\n",
    "- The density for sample $D$ for $\\theta \\in \\Theta$ is  \n",
    "$$p(\\mathcal{D} \\mid \\theta)=\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\theta\\right)$$  \n",
    "- $p(D | \\theta)$ is a function of $D$ and $\\theta$.  \n",
    "- For ﬁxed $θ$,$ p(D | \\theta)$ is a density function on $Y^n$  \n",
    "- For ﬁxed $D$, the function $\\theta \\mapsto p(D | \\theta)$ is called the **likelihood function**  \n",
    "$$L_{\\mathcal{D}}(\\theta):=p(\\mathcal{D} \\mid \\theta)$$  \n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "Deﬁnition:  \n",
    "- The **maximum likelihood estimator (MLE)** for $\\theta$ in the model $\\{p(y,\\theta) | \\theta \\in \\Theta \\}$ is  \n",
    "$$\\hat{\\theta}_{\\mathrm{MLE}}=\\underset{\\theta \\in \\Theta}{\\arg \\max } L_{\\mathcal{D}}(\\theta)$$  \n",
    "- Maximum likelihood is just one approach to getting a point estimator for $θ$.  \n",
    "- Method of moments is another general approach one learns about in statistics.  \n",
    "- Later we’ll talk about MAP and posterior mean as approaches to point estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin Flipping: Setup\n",
    "Parametric family of mass functions:\n",
    "$$p(\\text { Heads } \\mid \\theta)=\\theta$$  \n",
    "for $\\theta \\in \\Theta = (0,1)$  \n",
    "Note that every $\\theta \\in \\Theta$ gives us a diﬀerent probability model for a coin  \n",
    "\n",
    "## Coin Flipping: Likelihood function\n",
    "- Data $D = (H,H,T,T,T,T,T,H,...,T)$  \n",
    "  - $n_h$: number of heads  \n",
    "  - $n_t$: number of tails  \n",
    "- Likelihood function for data $D$:  \n",
    "$$L_{\\mathcal{D}}(\\theta)=p(\\mathcal{D} \\mid \\theta)=\\theta^{n_{h}}(1-\\theta)^{n_{t}}$$  \n",
    "\n",
    "## Coin Flipping: MLE\n",
    "- As usual, easier to maximize the log-likelihood function:\n",
    "$$\\begin{aligned}\n",
    "\\hat{\\theta}_{\\mathrm{MLE}} &=\\underset{\\theta \\in \\Theta}{\\arg \\max } \\log L_{\\mathcal{D}}(\\theta) \\\\\n",
    "&=\\underset{\\theta \\in \\Theta}{\\arg \\max }\\left[n_{h} \\log \\theta+n_{t} \\log (1-\\theta)\\right]\n",
    "\\end{aligned}$$  \n",
    "- First order condition:  \n",
    "$$\\begin{aligned}\n",
    "\\frac{n_{h}}{\\theta} &-\\frac{n_{t}}{1-\\theta}=0 \\\\\n",
    "& \\Longleftrightarrow \\theta=\\frac{n_{h}}{n_{h}+n_{t}}\n",
    "\\end{aligned}$$  \n",
    "- So $\\hat{\\theta}_{MLE}$ is the empirical fraction of heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics: Introduction  \n",
    "## Bayesian Statistics\n",
    "- Introduces a new ingredient: the **prior distribution**  \n",
    "- A prior distribution $p(\\theta)$ is a distribution on parameter space $\\Theta$.  \n",
    "- A prior reﬂects our belief about $\\theta$, before seeing any data..\n",
    "\n",
    "## A Bayesian Model\n",
    "- A Bayesian model consists of two pieces  \n",
    "  -  a parametric family of densities\n",
    "$$\\{p(\\mathcal{D} \\mid \\theta) \\mid \\theta \\in \\Theta\\}$$  \n",
    "  - A prior distribution $p(\\theta)$ on parameter space $\\Theta$.  \n",
    "- Putting pieces together, we get a joint density on $\\theta$ and $D$:  \n",
    "$$p(\\mathcal{D}, \\theta)=p(\\mathcal{D} \\mid \\theta) p(\\theta)$$  \n",
    "\n",
    "## The Posterior Distribution\n",
    "- The posterior distribution for $\\theta$ is $p(\\theta | D)$.  \n",
    "- Prior represents belief about $\\theta$ before observing data $D$.  \n",
    "- Posterior represents the rationally “updated” beliefs after seeing $D$.\n",
    "\n",
    "## Expressing the Posterior Distribution  \n",
    "- By Bayes rule, can write the posterior distribution as\n",
    "$$p(\\theta \\mid \\mathcal{D})=\\frac{p(\\mathcal{D} \\mid \\theta) p(\\theta)}{p(\\mathcal{D})}$$  \n",
    "- Let’s consider both sides as functions of $\\theta$ for ﬁxed $D$.  \n",
    "- Then both sides are densities on $\\Theta$ and we can write  \n",
    "$$\\underbrace{p(\\theta \\mid \\mathcal{D})}_{\\text {posterior }} \\propto \\underbrace{p(\\mathcal{D} \\mid \\theta)}_{\\text {likelihood prior }} \\underbrace{p(\\theta)}$$  \n",
    "\n",
    "## Coin Flipping: Bayesian Model\n",
    "- Parametric family of mass functions:\n",
    "$$p(\\text { Heads } \\mid \\theta)=\\theta$$  \n",
    "- Need a prior distribution $p(\\theta) on \\Theta = (0,1)$.  \n",
    "- A distribution from the Beta family will do the trick...\n",
    "\n",
    "## Coin Flipping: Beta Prior\n",
    "- Prior:  \n",
    "$$\\begin{aligned}\n",
    "\\theta & \\sim \\operatorname{Beta}(\\alpha, \\beta) \\\\\n",
    "p(\\theta) & \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n",
    "\\end{aligned}$$  \n",
    "<div align=\"center\"><img src = \"./beta.jpg\" width = '500' height = '100' align = center /></div>    \n",
    "- Mean of Beta distribution:  \n",
    "$$\\mathbb{E} \\theta=\\frac{h}{h+t}$$   \n",
    "- Mode of Beta distribution:\n",
    "$$\\underset{\\theta}{\\arg \\max } p(\\theta)=\\frac{h-1}{h+t-2}$$  \n",
    "for $h,t > 1$  \n",
    "- Likelihood model  \n",
    "$$p(\\mathcal{D} \\mid \\theta)=\\theta^{n_{h}}(1-\\theta)^{n_{t}}$$  \n",
    "- Posterior density  \n",
    "$$\\begin{aligned}\n",
    "p(\\theta \\mid \\mathcal{D}) & \\propto p(\\theta) p(\\mathcal{D} \\mid \\theta) \\\\\n",
    "& \\propto \\theta^{h-1}(1-\\theta)^{t-1} \\times \\theta^{n_{h}}(1-\\theta)^{n_{t}} \\\\\n",
    "&=\\theta^{h-1+n_{h}}(1-\\theta)^{t-1+n_{t}}\n",
    "\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior is Beta\n",
    "- Posterior is in the beta family:   \n",
    "$$\\theta \\mid \\mathcal{D} \\sim \\operatorname{Beta}\\left(h+n_{h}, t+n_{t}\\right)$$  \n",
    "- Interpretation:   \n",
    "  - Prior initializes our counts with $h$ heads and $t$ tails  \n",
    "  - Posterior increments counts by observed $n_h$ and $n_t$  \n",
    "  \n",
    "## Sidebar: Conjugate Priors\n",
    "- Interesting that posterior is in same distribution family as prior.  \n",
    "- Let $\\pi$ be a family of prior distributions on $\\Theta$.  \n",
    "- Let $P$ parametric family of distributions with parameter space $\\Theta$.  \n",
    "\n",
    "Deﬁnition:  \n",
    "A family of distributions $\\pi$ is conjugate to parametric model $P$ if for any prior in $\\pi$, the posterior is always in $\\pi$.  \n",
    "- The beta family is conjugate to the coin-ﬂipping (i.e. Bernoulli) model.  \n",
    "- The family of all probability distributions is conjugate to any parametric model. [Trvially]\n",
    "\n",
    "## Example: Coin Flipping - Concrete Example\n",
    "- Suppose we have a coin, possibly biased (**parametric probability model**):   \n",
    "$$p(\\text{Heads}| \\theta) = \\theta$$  \n",
    "  - Parameter space $\\theta \\in \\Theta = [0,1]$.  \n",
    "  - Prior distribution: $ \\theta ∼ \\text{Beta}(2,2)$\n",
    "<div align=\"center\"><img src = \"./beta2.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Example: Coin Flipping\n",
    "- Next, we gather some data $D = \\{H,H,T,T,T,T,T,H,...,T\\}$   \n",
    "  - Heads: 75 Tails: 60  \n",
    "  - $\\hat{\\theta}_{MLE} = \\frac{75} {75+60} \\approx 0.556$  \n",
    "- Posterior distribution: $\\theta | D ∼ \\text{Beta}(77,62)$:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Point Estimates\n",
    "- So we have posterior $\\theta | D$...  \n",
    "- But we want a point estimate $\\hat{\\theta}$ for $\\theta$.  \n",
    "- Common options:  \n",
    "   - **posterior mean** $\\hat{\\theta} = \\mathbb{E}[\\theta | D]$  \n",
    "   - **maximum a posteriori (MAP) estimate**  $\\hat{\\theta} = \\arg\\max_{\\theta}p(\\theta | D)$  \n",
    "     - Note: this is the **mode** of the posterior distribution\n",
    "\n",
    "## What else can we do with a posterior?\n",
    "- Extract “credible set” for $\\theta$ (a Bayesian conﬁdence interval).   \n",
    "  - e.g. Interval $[a,b]$ is a 95% credible set if  \n",
    "$$\n",
    "\\mathbb{P}(\\theta \\in[a, b] \\mid \\mathcal{D}) \\geqslant 0.95\n",
    "$$  \n",
    "- The most “Bayesian” approach is Bayesian decision theory:   \n",
    "  - Choose a loss function.  \n",
    "  - Find action minimizing **expected risk w.r.t. posterior**  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Decision Theory  \n",
    "- Ingredients:   \n",
    "  - Parameter space $\\Theta$.  \n",
    "  - Prior: Distribution $p(\\theta)$ on $\\Theta$.  \n",
    "  - Action space $A$.   \n",
    "  - Loss function: $l : A×\\Theta \\to R$.  \n",
    "- The posterior risk of an action $a\\in A$ is  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "r(a) &:=\\mathbb{E}[\\ell(\\theta, a) \\mid \\mathcal{D}] \\\\\n",
    "&=\\int \\ell(\\theta, a) p(\\theta \\mid \\mathcal{D}) d \\theta\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- It’s the expected loss under the posterior.  \n",
    "- A Bayes action $a^*$ is an action that minimizes posterior risk:  \n",
    "$$\n",
    "r\\left(a^{*}\\right)=\\min _{a \\in \\mathcal{A}} r(a)\n",
    "$$  \n",
    "\n",
    "## Bayesian Point Estimation  \n",
    "- General Setup:  \n",
    "  - Data $D$ generated by $p(y | \\theta)$, for unknown $\\theta \\in \\Theta$.   \n",
    "  - Want to produce a **point estimate** for $\\theta$.  \n",
    "- Choose the following:  \n",
    "  - **Loss:** $\\ell(\\hat{\\theta}, \\theta)=(\\theta-\\hat{\\theta})^{2}$  \n",
    "  - **Prior:** $p(\\theta)$ on $\\Theta$  \n",
    "- Find action $\\hat{\\theta} \\in \\Theta$ that minimizes posterior risk:   \n",
    "$$\n",
    "\\begin{aligned}\n",
    "r(\\hat{\\theta}) &=\\mathbb{E}\\left[(\\theta-\\hat{\\theta})^{2} \\mid \\mathcal{D}\\right] \\\\\n",
    "&=\\int(\\theta-\\hat{\\theta})^{2} p(\\theta \\mid \\mathcal{D}) d \\theta\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "## Bayesian Point Estimation: Square Loss\n",
    "- Find action $\\hat{\\theta} \\in \\Theta$ that minimizes **posterior risk**  \n",
    "$$\n",
    "r(\\hat{\\theta})=\\int(\\theta-\\hat{\\theta})^{2} p(\\theta \\mid \\mathcal{D}) d \\theta\n",
    "$$  \n",
    "- Differentiate:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d r(\\hat{\\theta})}{d \\hat{\\theta}} &=-\\int 2(\\theta-\\hat{\\theta}) p(\\theta \\mid \\mathcal{D}) d \\theta \\\\\n",
    "&=-2 \\int \\theta p(\\theta \\mid \\mathcal{D}) d \\theta+2 \\hat{\\theta} \\int_{=1} p(\\theta \\mid \\mathcal{D}) d \\theta \\\\\n",
    "&=-2 \\int \\theta p(\\theta \\mid \\mathcal{D}) d \\theta+2 \\hat{\\theta}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- First order condition $\\frac{d r(\\hat{\\theta})}{d \\hat{\\theta}}=0$ gives:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\theta} &=\\int \\theta p(\\theta \\mid \\mathcal{D}) d \\theta \\\\\n",
    "&=\\mathbb{E}[\\theta \\mid \\mathcal{D}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Bayes action for square loss is the posterior mean.\n",
    "\n",
    "## Bayesian Point Estimation: Absolute Loss\n",
    "**Loss:** $\\ell(\\theta, \\hat{\\theta})=|\\theta-\\hat{\\theta}|$  \n",
    "- **Bayesian Action** for **Absolute loss** is **posterior median**  \n",
    "\n",
    "## Bayesian Point Estimation: Zero-One Loss\n",
    "- Suppose $\\Theta$ is discrete  \n",
    "- **Zero-one loss:** $\\ell(\\theta, \\hat{\\theta})=1(\\theta \\neq \\hat{\\theta})$  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "r(\\hat{\\theta}) &=\\mathbb{E}[1(\\theta \\neq \\hat{\\theta}) \\mid \\mathcal{D}] \\\\\n",
    "&=\\mathbb{P}(\\theta \\neq \\hat{\\theta} \\mid \\mathcal{D}) \\\\\n",
    "&=1-\\mathbb{P}(\\theta=\\hat{\\theta} \\mid \\mathcal{D}) \\\\\n",
    "&=1-p(\\hat{\\theta} \\mid \\mathcal{D})\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- Bayesian Action is:  \n",
    "$$\n",
    "\\hat{\\theta}=\\underset{\\theta \\in \\Theta}{\\arg \\max } p(\\theta \\mid \\mathcal{D})\n",
    "$$  \n",
    "- This $\\hat{\\theta}$ is called the maximum a posteriori (MAP) estimate  \n",
    "- The MAP estimate is the mode of the posterior distribution.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary  \n",
    "- Prior represents belief about $\\theta$ before observing data $D$.  \n",
    "- Posterior represents the rationally “updated” beliefs after seeing $D$.  \n",
    "- All inferences and action-taking are based on the posterior distribution.  \n",
    "- In the Bayesian approach,   \n",
    "  - No issue of “choosing a procedure” or justifying an estimator.   \n",
    "  - Only choices are the prior and the likelihood model. \n",
    "  - For decision making, need a loss function. \n",
    "  - Everything after that is computation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
