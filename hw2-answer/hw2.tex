%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[ruled]{article}
\usepackage{courier}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{color}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=true]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\title{Machine Learning and Computational Statistics, Spring 2017\\
Homework 2: Lasso Regression} 

%\date{Due: February $4^{th}$, 4pm}




\usepackage{amsfonts}\usepackage{capt-of}
%\usepackage{url}
\usepackage{color}
\usepackage{bbm}
\usepackage{enumerate}
\newcommand{\carlos}[1]{\textcolor{red}{Carlos: #1}}
\newcommand{\field}[1]{\mathbb{#1}} 
\newcommand{\hide}[1]{#1}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\providecommand{\m}[1]{\mathbf{#1}}
\providecommand{\norm}[1]{\left\|#1\right\|}
\providecommand{\sign}[1]{\text{sign}\left(#1\right)}
\DeclareMathOperator*{\argmin}{arg\,min}
\providecommand{\what}{\m{\hat{w}}}
\providecommand{\dw}{\Delta w}
\providecommand{\dmw}{\Delta \m{w}}
\providecommand{\hy}{\hat{y}}

\makeatother

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
 \global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
 \global\long\def\ca{\mathcal{A}}
 \global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
 \global\long\def\ce{\mathcal{E}}
 \global\long\def\cf{\mathcal{F}}
 \global\long\def\cg{\mathcal{G}}
 \global\long\def\ch{\mathcal{H}}
 \global\long\def\ci{\mathcal{I}}
 \global\long\def\cj{\mathcal{J}}
 \global\long\def\ck{\mathcal{K}}
 \global\long\def\cl{\mathcal{L}}
 \global\long\def\cm{\mathcal{M}}
 \global\long\def\cn{\mathcal{N}}
 \global\long\def\co{\mathcal{O}}
 \global\long\def\cp{\mathcal{P}}
 \global\long\def\cq{\mathcal{Q}}
 \global\long\def\calr{\mathcal{R}}
 \global\long\def\cs{\mathcal{S}}
 \global\long\def\ct{\mathcal{T}}
 \global\long\def\cu{\mathcal{U}}
 \global\long\def\cv{\mathcal{V}}
 \global\long\def\cw{\mathcal{W}}
 \global\long\def\cx{\mathcal{X}}
 \global\long\def\cy{\mathcal{Y}}
 \global\long\def\cz{\mathcal{Z}}
 \global\long\def\ind#1{1(#1)}
 \global\long\def\pr{\mathbb{P}}
 \global\long\def\predsp{\cy}
 \global\long\def\outsp{\cy}
 \global\long\def\prxy{P_{\cx\times\cy}}
 \global\long\def\prx{P_{\cx}}
 \global\long\def\prygivenx{P_{\cy\mid\cx}}
 \global\long\def\ex{\mathbb{E}}
 \global\long\def\var{\textrm{Var}}
 \global\long\def\cov{\textrm{Cov}}
 \global\long\def\sgn{\textrm{sgn}}
 \global\long\def\sign{\textrm{sign}}
 \global\long\def\kl{\textrm{KL}}
 \global\long\def\law{\mathcal{L}}
 \global\long\def\eps{\varepsilon}
 \global\long\def\as{\textrm{ a.s.}}
 \global\long\def\io{\textrm{ i.o.}}
 \global\long\def\ev{\textrm{ ev.}}
 \global\long\def\convd{\stackrel{d}{\to}}
 \global\long\def\eqd{\stackrel{d}{=}}
 \global\long\def\del{\nabla}
 \global\long\def\loss{\ell}
 \global\long\def\risk{R}
 \global\long\def\emprisk{\hat{R}_{\ell}}
 \global\long\def\lossfnl{L}
 \global\long\def\emplossfnl{\hat{L}}
 \global\long\def\empminimizer#1{\hat{#1}_{\ell}}
 \global\long\def\minimizer#1{#1_{*}}
 \global\long\def\etal{\textrm{et. al.}}
 \global\long\def\tr{\operatorname{tr}}
 \global\long\def\trace{\operatorname{trace}}
 \global\long\def\diag{\text{diag}}
 \global\long\def\rank{\text{rank}}
 \global\long\def\linspan{\text{span}}
 \global\long\def\proj{\text{Proj}}
 \global\long\def\argmax{\operatornamewithlimits{arg\, max}}
 \global\long\def\argmin{\operatornamewithlimits{arg\, min}}
 \global\long\def\bfx{\mathbf{x}}
 \global\long\def\bfy{\mathbf{y}}
 \global\long\def\bfl{\mathbf{\lambda}}
 \global\long\def\bfm{\mathbf{\mu}}
 \global\long\def\calL{\mathcal{L}}
 \global\long\def\vw{\boldsymbol{w}}
 \global\long\def\vx{\boldsymbol{x}}
 \global\long\def\vxi{\boldsymbol{\xi}}
 \global\long\def\valpha{\boldsymbol{\alpha}}
 \global\long\def\vbeta{\boldsymbol{\beta}}
 \global\long\def\vsigma{\boldsymbol{\sigma}}
 \global\long\def\vmu{\boldsymbol{\mu}}
 \global\long\def\vtheta{\boldsymbol{\theta}}
 \global\long\def\vd{\boldsymbol{d}}
 \global\long\def\vs{\boldsymbol{s}}
 \global\long\def\vt{\boldsymbol{t}}
 \global\long\def\vh{\boldsymbol{h}}
 \global\long\def\ve{\boldsymbol{e}}
 \global\long\def\vf{\boldsymbol{f}}
 \global\long\def\vg{\boldsymbol{g}}
 \global\long\def\vz{\boldsymbol{z}}
 \global\long\def\vk{\boldsymbol{k}}
 \global\long\def\va{\boldsymbol{a}}
 \global\long\def\vb{\boldsymbol{b}}
 \global\long\def\vv{\boldsymbol{v}}
 \global\long\def\vy{\boldsymbol{y}}
 \global\long\def\hil{\ch}
 \global\long\def\rkhs{\hil}
 \maketitle

\textbf{Due: Monday, February 13, 2017, at 10pm (Submit via Gradescope)}

\textbf{Instructions}: Your answers to the questions below, including
plots and mathematical work, should be submitted as a single PDF file.
It's preferred that you write your answers using software that typesets
mathematics (e.g. \LaTeX{}, \LyX{}, or MathJax via iPython), though
if you need to you may scan handwritten work. You may find the \href{https://github.com/gpoore/minted}{minted}
package convenient for including source code in your \LaTeX{} document.

\section{Introduction}

In this homework you will investigate regression with $\ell_{1}$
regularization, both implementation techniques and theoretical properties.
On the methods side, you'll work on coordinate descent (the ``shooting
algorithm''), homotopy methods, and {[}optionally{]} projected SGD.
On the theory side you'll derive the explicit solution to the coordinate
minimizers used in coordinate descent, you'll derive the largest $\ell_{1}$
regularization parameter you'll ever need to try, you'll investigate
what happens with ridge and lasso regression when you have two copies
of the same feature, and you'll {[}optionally{]} work out the details
of the classic picture that ``explains'' why $\ell_{1}$ regularization
leads to sparsity.

For the experiments, we constructed a small dataset with known properties.
The file \texttt{generate\_data.py} contains the code used to generate
the data. Section \ref{subsec:Dataset-construction} describes the
steps followed to construct the dataset. 

\subsection{\label{subsec:Dataset-construction}Dataset construction}

Start by creating a design matrix for regression with $m=150$ examples,
each of dimension $d=75$. We will choose a true weight vector $\boldsymbol{\theta}$
that has only a few non-zero components: 
\begin{enumerate}
\item Let $X\in\reals^{m\times d}$ be the ``design matrix,'' where the
$i$'th row of $X$ is $x_{i}\in\reals^{d}$. Construct a random design
matrix $X$ using \texttt{numpy.random.rand()} function. 
\item Construct a true weight vector $\boldsymbol{\theta}\in\reals^{d\times1}$
as follows: Set the first 10 components of $\boldsymbol{\theta}$
to 10 or -10 arbitrarily, and all the other components to zero. 
\item Let $y=\left(y_{1},\ldots,y_{m}\right)^{T}\in\reals^{m\times1}$ be
the response. Construct the vector $y=X\boldsymbol{\theta}+\epsilon$,
where $\epsilon$ is an $m\times1$ random noise vector where each
entry is sampled i.i.d. from a normal distribution with mean $0$
and standard deviation $0.1$. You can use \texttt{numpy.random.randn()}
to generate $\epsilon$ in Python.
\item Split the dataset by taking the first $80$ points for training, the
next $20$ points for validation, and the last $50$ points for testing. 
\end{enumerate}
Note that we are not adding an extra feature for the bias in this
problem. By construction, the true model does not have a bias term.

\section{Ridge Regression}

By construction, we know that our dataset admits a sparse solution.
Here, we want to evaluate the performance of ridge regression (i.e.
$\ell_{2}$-regularized linear regression) on this dataset.
\begin{enumerate}
\item Run ridge regression on this dataset. Choose the $\lambda$ that minimizes
the square loss on the validation set. For the chosen $\lambda$,
examine the model coefficients. Report on how many components with
true value $0$ have been estimated to be non-zero, and vice-versa
(don't worry if they are all nonzero). Now choose a small threshold
(say $10^{-3}$ or smaller), count anything with magnitude smaller
than the threshold as zero, and repeat the report. (For running ridge
regression, you may either use your code from HW1, or you may use
\texttt{scipy.optimize.minimize} (see the demo code provided for guidance).
For debugging purposes, you are welcome, even encouraged, to compare
your results to what you get from \texttt{sklearn.linear\_model.Ridge}.) 
\end{enumerate}

\section{\label{subsec:Shooting-algorithm}Coordinate Descent for Lasso (a.k.a.
The Shooting algorithm)}

The Lasso optimization problem can be formulated as 
\[
\hat{w}={\displaystyle \argmin_{w\in\reals^{d}}\sum_{i=1}^{m}(h_{w}(x_{i})-y_{i})^{2}+\lambda\|w\|_{1}},
\]
where $h_{w}(x)=w^{T}x$, and $\|w\|_{1}=\sum_{i=1}^{d}|w_{i}|$.
Since the $\ell_{1}$-regularization term in the objective function
is non-differentiable, it's not clear how gradient descent or SGD
could be used to solve this optimization problem. (In fact, as we'll
see in the next homework on SVMs, we can use ``subgradient'' methods
when the objective function is not differentiable.)

Another approach to solving optimization problems is coordinate descent,
in which at each step we optimize over one component of the unknown
parameter vector, fixing all other components. The descent path so
obtained is a sequence of steps each of which is parallel to a coordinate
axis in $\reals^{d}$, hence the name. It turns out that for the Lasso
optimization problem, we can find a closed form solution for optimization
over a single component fixing all other components. This gives us
the following algorithm, known as the \textbf{shooting algorithm}:

\begin{center}
\includegraphics[width=1\textwidth]{/Users/drosen/Dropbox/repos/mlcourse-homework/hw2-lasso/shooting_algo}
(Source: Murphy, Kevin P. Machine learning: a probabilistic perspective.
MIT press, 2012.) 
\par\end{center}

The ``soft thresholding'' function is defined as
\[
\text{soft}\left(a,\delta\right)=\text{sign}(a)\left(\left|a\right|-\delta\right)_{+}.
\]

NOTE: Algorithm 13.1 does not account for the case that $a_{j}=c_{j}=0$,
which occurs when the $j$th column of $X$ is  identically $0$.
One can either eliminate the column (as it cannot possibly help the
solution), or you can set $w_{j}=0$ in that case since it is, as
you can easily verify, the coordinate minimizer. Note also that Murphy
is suggesting to initialize the optimization with the ridge regession
solution. Although theoretically this is not necessary (with exact
computations and enough time, coordinate descent will converge for
lasso from any starting point), in practice it's helpful to start
as close to the solution as we're able.

There are a few tricks that can make selecting the hyperparameter
$\lambda$ easier and faster. First, you can show that for any $\lambda>2\|X^{T}(y-\bar{y})\|_{\infty}$,
the estimated weight vector $\hat{w}$ is entirely zero, where $\bar{y}$
is the mean of values in the vector $y$, and $\|\cdot\|_{\infty}$
is the infinity norm (or supremum norm), which is the maximum absolute
value of any component of the vector. Thus we need to search for an
optimal $\lambda$ in $\left[0,\lambda_{\text{max}}\right]$, where
$\lambda_{\text{max}}=2\|X^{T}(y-\bar{y})\|_{\infty}$. (Note: This
expression for $\lambda_{\text{max}}$ assumes we have an unregularized
bias term in our model. That is, our decision functions are $h_{w,b}(x)=w^{T}x+b$.
For the experiments, you can exclude the bias term, in which case
$\lambda_{\text{max}}=2\|X^{T}y\|_{\infty}$.)

The second trick is to use the fact that when $\lambda$ and $\lambda'$
are close, the corresponding solutions $\hat{w}(\lambda)$ and $\hat{w}(\lambda')$
are also close. Start by finding $\hat{w}(\lambda_{\text{max}})$
and initialize the optimization at $w=0$. Next, $\lambda$ is reduced
(e.g. by a constant factor), and the optimization problem is solved
using the previous optimal point as the starting point. This is called
\textbf{warm starting} the optimization. The technique of computing
a set of solutions for a chain of nearby $\lambda$'s is called a
\textbf{continuation }or\textbf{ homotopy method}. The resulting set
of parameter values $\hat{w}(\lambda)$ as $\lambda$ ranges over
$\left[0,\lambda_{\text{max }}\right]$ is known as a \textbf{regularization
path}.

\subsection{Experiments with the Shooting Algorithm}
\begin{enumerate}
\item Write a function that computes the Lasso solution for a given $\lambda$
using the shooting algorithm described above. This function should
take a starting point for the optimization as a parameter. Run it
on the dataset constructed in (1.1), and select the $\lambda$ that
minimizes the square error on the validation set. Report the optimal
value of $\lambda$ found, and the corresponding test error. Plot
the validation error vs $\lambda$. {[}Don't use the homotopy method
in this part, as we want to measure the speed improvement of homotopy
methods in question \ref{enu:homotopy}. Also, no need to vectorize
the calculations until question \ref{enu:vectorization}, where again
we'll compare the speedup. In any case, having two different implementations
of the same thing is a good way to check your work.{]}
\item Analyze the sparsity\footnote{One might hope that the solution will a sparsity pattern that is similar
to the ground truth. Estimators that preserve the sparsity pattern
(with enough training data) are said to be \textbf{``sparsistent''}
(sparse + consistent). Formally, an estimator $\hat{\beta}$ of parameter
$\beta$ is said to be consistent if the estimator $\hat{\beta}$
converges to the true value $\beta$ in probability as our sample
size goes to infinity. Analogously, if we define the support of a
vector $\beta$ as the indices with non-zero components, i.e. $\text{Supp}(\beta)=\{j\mid\beta_{j}\neq0\}$,
then an estimator $\hat{\beta}$ is said to be sparsistent if as the
number of samples becomes large, the support of $\hat{\beta}$ converges
to the support of $\beta$, or ${\displaystyle \lim_{m\rightarrow\infty}P[\text{Supp}(\hat{\beta}_{m})=\text{Supp}(\beta)]}$
= 1. } of your solution, reporting how many components with true value zero
have been estimated to be non-zero, and vice-versa. 
\item \label{enu:homotopy}Implement the homotopy method described above.
Compare the runtime for computing the full regularization path (for
the same set of $\lambda$'s you tried in the first question above)
using the homotopy method compared to the basic shooting algorithm.
\item \label{enu:vectorization}The algorithm as described above is not
ready for a large dataset (at least if it has been implemented in
basic Python) because of the implied loop over the dataset (i.e. where
we sum over the training set). By using matrix and vector operations,
we can eliminate the loops. This is called ``vectorization'' and
can lead to dramatic speedup in languages such as Python, Matlab,
and R. Derive matrix expressions for computing $a_{j}$ and $c_{j}$.
(Hint: A matlab version of this vectorized method can be found in
the assignment zip file.) Implement the matrix expressions and measure
the speedup in computing the regularization path. 
\end{enumerate}

\subsection{Deriving the Coordinate Minimizer for Lasso}

This problem is to derive the expressions for the coordinate minimizers
used in the Shooting algorithm. This is often \href{http://davidrosenberg.github.io/mlcourse/Archive/2015/Lectures/2.Lab.subgradient-descent.pdf\#page=15}{derived using subgradients (slide 15)},
but here we will take a bare hands approach (which is essentially
equivalent). 

In each step of the shooting algorithm, we would like to find the
$w_{j}$ minimizing
\begin{eqnarray*}
f(w_{j}) & = & \sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}+\lambda\left|w\right|_{1}\\
 & = & \sum_{i=1}^{n}\left[w_{j}x_{ij}+\sum_{k\neq j}w_{k}x_{ik}-y_{i}\right]^{2}+\lambda\left|w_{j}\right|+\lambda\sum_{k\neq j}\left|w_{k}\right|,
\end{eqnarray*}
where we've written $x_{ij}$ for the $j$th entry of the vector $x_{i}$.
This function is strictly convex in $w_{j}$, and thus it has a unique
minimum. The only thing keeping $f$ from being differentiable is
the term with $\left|w_{j}\right|$. So $f$ is differentiable everywhere
except $w_{j}=0$. We'll break this problem into 3 cases: $w_{j}>0$,
$w_{j}<0$, and $w_{j}=0$. In the first two cases, we can simply
differentiate $f$ w.r.t. $w_{j}$ to get optimality conditions. For
the last case, we'll use the fact that since $f:\reals\to\reals$
is convex, 0 is a minimizer of $f$ iff
\[
\lim_{\eps\downarrow0}\frac{f(\eps)-f(0)}{\eps}\ge0\quad\mbox{and}\quad\lim_{\eps\downarrow0}\frac{f(-\eps)-f(0)}{\eps}\ge0.
\]
This is a special case of the optimality conditions described in \href{http://davidrosenberg.github.io/mlcourse/Archive/2015/Lectures/5.Lab.misc.pdf\#page=12}{slide 12 here},
where now the ``direction'' $v$ is simply taken to be the scalars
$1$ and $-1$, respectively. 
\begin{enumerate}
\item First let's get a trivial case out of the way. If $x_{ij}=0$ for
$i=1,\ldots,n$, what is the coordinate minimizer $w_{j}$? In the
remaining questions below, you may assume that $\sum_{i=1}^{n}x_{ij}^{2}>0$.
\item Give an expression for the derivative $f(w_{j})$ for $w_{j}\neq0$.
It will be convenient to write your expression in terms of the following
definitions:
\begin{eqnarray*}
\sign(w_{j}) & := & \begin{cases}
1 & w_{j}>0\\
0 & w_{j}=0\\
-1 & w_{j}<0
\end{cases}\\
a_{j} & := & 2\sum_{i=1}^{n}x_{ij}^{2}\\
c_{j} & := & 2\sum_{i=1}^{n}x_{ij}\left(y_{i}-\sum_{k\neq j}w_{k}x_{ik}\right).
\end{eqnarray*}
\item If $w_{j}>0$ and minimizes $f$, show that $w_{j}=\frac{1}{a_{j}}\left(c_{j}-\lambda\right)$.
Similarly, if $w_{j}<0$ and minimizes $f$, show that $w_{j}=\frac{1}{a_{j}}\left(c_{j}+\lambda\right)$.
Give conditions on $c_{j}$ that imply that a minimizer $w_{j}$ is
positive and conditions for which a minimizer $w_{j}$ is negative.
\item Derive expressions for the two one-sided derivatives at $f(0)$, and
show that $c_{j}\in\left[-\lambda,\lambda\right]$ implies that $w_{j}=0$
is a minimizer.
\item Putting together the preceding results, we conclude the following:
\[
w_{j}=\begin{cases}
\frac{1}{a_{j}}\left(c_{j}-\lambda\right) & c_{j}>\lambda\\
0 & c_{j}\in[-\lambda,\lambda]\\
\frac{1}{a_{j}}\left(c_{j}+\lambda\right) & c_{j}<-\lambda
\end{cases}
\]
Show that this is equivalent to the expression given in \ref{subsec:Shooting-algorithm}.\\
\end{enumerate}

\section{Lasso Properties}

\subsection{Deriving $\lambda_{\mbox{max}}$}

In this problem we will derive an expression for $\lambda_{\text{max}}$.
For the first three parts, use the Lasso objective function excluding
the bias term i.e, $L(w)=\left\Vert Xw-y\right\Vert _{2}^{2}+\lambda\left\Vert w\right\Vert _{1}$.
Show that for any $\lambda\geq2\|X^{T}y\|_{\infty}$, the estimated
weight vector $\hat{w}$ is entirely zero, where $\|\cdot\|_{\infty}$
is the infinity norm (or supremum norm), which is the maximum absolute
value of any component of the vector. 
\begin{enumerate}
\item The one-sided directional derivative of $f(x)$ at $x$ in the direction
$v$ is defined as:
\[
f'(x;v)=\lim_{h\downarrow0}\frac{f(x+hv)-f(x)}{h}
\]
Compute $L'(0;v)$. That is, compute the one-sided directional derivative
of $L(w)$ at $w=0$ in the direction $v$. {[}Hint: the result should
be in terms of $X,y,\lambda,\text{ and }v$.{]}\\
\textbf{}\\
\item Since the Lasso objective is convex, for $w^{*}$ to be a minimizer
of $L(w)$ we must have that the directional derivative $L'(w^{*};v)\geq0$
for all $v$. Starting from the condition $L'(0;v)\ge0$, rearrange
terms to get a lower bounds on $\lambda$. {[}Hint: this should be
in terms of $X,y,\text{{and} }v$.{]} \textbf{}\\
\item In the previous problem, we get a different lower bound on $\lambda$
for each choice of $v$. Compute the maximum lower bound of $\lambda$
by maximizing the expression over $v.$ Show that this expression
is equivalent to $\lambda_{\text{max}}=2\|X^{T}y\|_{\infty}$.\\
\item {[}Optional{]} Show that for $L(w,b)=\left\Vert Xw+b\mathbf{1}-y\right\Vert _{2}^{2}+\lambda\left\Vert w\right\Vert _{1}$,
$\lambda_{\text{max}}=2\|X^{T}(y-\bar{y})\|_{\infty}$where $\bar{y}$
is the mean of values in the vector $y$, and $\mathbf{1}\in\reals^{n}$
is a column vector of $1$'s .
\end{enumerate}

\subsection{Feature Correlation}

In this problem, we will examine and compare the behavior of the Lasso
and ridge regression in the case of an exactly repeated feature. That
is, consider the design matrix $X\in\reals^{m\times d}$, where $X_{\cdot i}=X_{\cdot j}$
for some $i$ and $j$, where $X_{\cdot i}$ is the $i^{th}$ column
of $X$. We will see that ridge regression divides the weight equally
among identical features, while Lasso divides the weight arbitrarily.
In an optional part to this problem, we will consider what changes
when $X_{\cdot i}$ and $X_{\cdot j}$ are highly correlated (e.g.
exactly the same except for some small random noise) rather than exactly
the same. 
\begin{enumerate}
\item Without a loss of generality, assume the first two colums of $X$
are our repeated features. Partition $X$ and $\theta$ as follows:\\
\[
X=\left(\begin{array}{ccc}
x_{1} & x_{2} & X_{r}\end{array}\right),\theta=\left(\begin{array}{c}
\theta_{1}\\
\theta_{2}\\
\theta_{r}
\end{array}\right)
\]
We can write the Lasso objective function as:
\begin{align*}
L(\theta)= & \left\Vert X\theta-y\right\Vert _{2}^{2}+\lambda\left\Vert \theta\right\Vert _{1}\\
= & \left\Vert x_{1}\theta_{1}+x_{2}\theta_{2}+X_{r}\theta_{r}-y\right\Vert _{2}^{2}+\lambda\vert\theta_{1}\vert+\lambda\vert\theta_{2}\vert+\lambda\left\Vert \theta_{r}\right\Vert _{1}
\end{align*}
With repeated features, there will be multiple minimizers of $L(\theta)$.
Suppose that 
\[
\hat{\theta}=\begin{pmatrix}a\\
b\\
r
\end{pmatrix}
\]
is a minimizer of $L(\theta)$. Give conditions on $c$ and $d$ such
that $\left(c,d,r^{T}\right)^{T}$ is also a minimizer of $L(\theta$).
{[}Hint: First show that $a$ and $b$ must have the same sign, or
at least one of them is zero. Then, using this result, rewrite the
optimization problem to derive a relation between $a$ and $b$.{]}\\
\item Using the same notation as the previous problem, suppose 
\[
\hat{\theta}=\begin{pmatrix}a\\
b\\
r
\end{pmatrix}
\]
minimizes the ridge regression objective function. What is the relationship
between $a$ and $b$, and why? \\
\item {[}Optional{]} What do you think would happen with Lasso and ridge
when $X_{\cdot i}$ and $X_{\cdot j}$ are highly correlated, but
not exactly the same. You may investigate this experimentally or theoretically.
\end{enumerate}

\section{{[}Optional{]} The Ellipsoids in the $\ell_{1}/\ell_{2}$ regularization
picture}

Recall the famous picture purporting to explain why $\ell_{1}$ regularization
leads to sparsity, while $\ell_{2}$ regularization does not. Here's
the instance from Hastie et al's \emph{The Elements of Statistical
Learning:}
\begin{center}
\includegraphics[width=0.5\paperwidth]{L1-L2-corner-pic-ESL-Fig3\lyxdot 11} 
\par\end{center}

(While Hastie et al. use $\beta$ for the parameters, we'll continue
to use $w$.) 

In this problem we'll show that the level sets of the empirical risk
are indeed ellipsoids centered at the empirical risk minimizer $\hat{w}$.

Consider linear prediction functions of the form $x\mapsto w^{T}x$.
Then the empirical risk for $f(x)=w^{T}x$ under the square loss is
\begin{eqnarray*}
\hat{R}_{n}(w) & = & \frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}\\
 & = & \frac{1}{n}\left(Xw-y\right)^{T}\left(Xw-y\right).
\end{eqnarray*}

\begin{enumerate}
\item {[}Optional{]} Let $\hat{w}=\left(X^{T}X\right)^{-1}X^{T}y$. Show
that $\hat{w}$ has empirical risk given by 
\[
\hat{R}_{n}(\hat{w})=\frac{1}{n}\left(-y^{T}X\hat{w}+y^{T}y\right)
\]
\\
\item {[}Optional{]} \label{enu:emprisk-ellipsoid}Show that for any $w$
we have
\[
\hat{R}_{n}(w)=\frac{1}{n}\left(w-\hat{w}\right)^{T}X^{T}X\left(w-\hat{w}\right)+\hat{R}_{n}(\hat{w}).
\]
Note that the RHS (i.e. ``right hand side'') has one term that's
quadratic in $w$ and one term that's independent of $w$. In particular,
the RHS does not have any term that's linear in $w$. On the LHS (i.e.
``left hand side''), we have $\hat{R}_{n}(w)=\frac{1}{n}\left(Xw-y\right)^{T}\left(Xw-y\right)$.
After expanding this out, you'll have terms that are quadratic, linear,
and constant in $w$. Completing the square is the tool for rearranging
an expression to get rid of the linear terms. The following ``completing
the square'' identity is easy to verify just by multiplying out the
expressions on the RHS:
\begin{eqnarray*}
x^{T}Mx-2b^{T}x & = & \left(x-M^{-1}b\right)^{T}M(x-M^{-1}b)-b^{T}M^{-1}b
\end{eqnarray*}
\\
\item {[}Optional{]} Using the expression derived for $\hat{R}_{n}(w)$
in \ref{enu:emprisk-ellipsoid}, give a very short proof that $\hat{w}=\left(X^{T}X\right)^{-1}X^{T}y$
is the empirical risk minimizer. That is: 
\[
\hat{w}=\argmin_{w}\hat{R}_{n}(w).
\]
Hint: Note that $X^{T}X$ is positive semidefinite and, by definition,
a symmetric matrix $M$ is positive semidefinite iff for all $x\in\reals^{d}$,
$x^{T}Mx\ge0$.\\
 
\item {[}Optional{]} Give an expression for the set of $w$ for which the
empirical risk exceeds the minimum empirical risk $\hat{R}_{n}(\hat{w})$
by an amount $c>0$. If $X$ is full rank, then $X^{T}X$ is positive
definite, and this set is an ellipse \textendash{} what is its center?\\
\end{enumerate}

\section{{[}Optional{]} Projected SGD via Variable Splitting}

In this question, we consider another general technique that can be
used on the Lasso problem. We first use the variable splitting method
to transform the Lasso problem to a smooth problem with linear inequality
constraints, and then we can apply a variant of SGD.

Representing the unknown vector $\theta$ as a difference of two non-negative
vectors $\theta^{+}$ and $\theta^{-}$, the $\ell{}_{1}$-norm of
$\theta$ is given by ${\displaystyle \sum_{i=1}^{d}{\theta_{i}^{+}}}$
+ ${\displaystyle \sum_{i=1}^{d}{\theta_{i}^{-}}}$. Thus, the optimization
problem can be written as 
\begin{gather*}
(\hat{\theta}^{+},\hat{\theta}^{-})={\displaystyle \argmin_{\theta^{+},\theta^{-}\in\reals^{d}}{\sum_{i=1}^{m}{(h_{\theta^{+},\theta^{-}}(x_{i})-y_{i})^{2}}}+\lambda\sum_{i=1}^{d}{\theta_{i}^{+}}+\lambda\sum_{i=1}^{d}{\theta_{i}^{-}}}\\
\mbox{such that }\theta^{+}\ge0\mbox{ and }\theta^{-}\ge0,
\end{gather*}
where $h_{\theta^{+},\theta^{-}}(x)=(\theta^{+}-\theta^{-})^{T}x$.
The original parameter $\theta$ can then be estimated as $\hat{\theta}=(\hat{\theta}^{+}-\hat{\theta}^{-})$.

This is a convex optimization problem with a differentiable objective
and linear inequality constraints. We can approach this problem using
projected stochastic gradient descent, as discussed in lecture. Here,
after taking our stochastic gradient step, we project the result back
into the feasible set by setting any negative components of $\theta^{+}$
and $\theta^{-}$ to zero.
\begin{enumerate}
\item {[}Optional{]} Implement projected SGD to solve the above optimization
problem for the same $\lambda$'s as used with the shooting algorithm.
Since the two optimization algorithms should find essentially the
same solutions, you can check the algorithms against each other. Report
the differences in validation loss for each $\lambda$ between the
two optimization methods. (You can make a table or plot the differences.) 
\item {[}Optional{]} Choose the $\lambda$ that gives the best performance
on the validation set. Describe the solution $\hat{w}$ in term of
its sparsity. How does the sparsity compare to the solution from the
shooting algorithm? 
\end{enumerate}

\end{document}
