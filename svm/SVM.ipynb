{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin  and Hinge Loss\n",
    "- Definition  \n",
    "The margin (or functional margin) for predicted score $\\hat{y}$ and true class $y \\in \\{-1,1\\}$ is $\\hat{y} y$.  \n",
    "  - The margin is a measure of how correct we are  \n",
    "  - We want to **maximize the margin**  \n",
    "- Hinge Loss  \n",
    "$$\\ell_{\\text {Hinge }}=\\max \\{1-m, 0\\}=(1-m)_{+}$$\n",
    "<div align=\"center\"><img src = \"./hinge.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "Hinge is a convex, upper bound on $0-1$ loss. Not differentiable at $m = 1$.  We have a “margin error” when $m < 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM  \n",
    "- Simplest Case:  \n",
    "  - Hypothesis space $\\mathcal{F}=\\left\\{f(x)=w^{T} x+b \\mid w \\in \\mathbf{R}^{d}, b \\in \\mathbf{R}\\right\\}$  \n",
    "  - $l_2$ regularization(Tikhonov Style)  \n",
    "  - Loss:  Hinge Loss, $l(\\hat{y}, y) = max(0, 1 - y\\hat{y}) = (1 - y\\hat{y})_+$  \n",
    "  - The SVM prediction function is the solution to   \n",
    "$$\\min _{w \\in \\mathbf{R}^{d}, b \\in \\mathbf{R}} \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\max \\left(0,1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)$$  \n",
    "## SVM Optimization Problem (Tikhonov Version)\n",
    "- unconstrained optimization  \n",
    "- not differentiable  \n",
    "- What can we do?  \n",
    "\n",
    "## SVM Equivalent Form  \n",
    "- Idea: Because **max** is not differentiable, we have to think of a method to eliminate it.  \n",
    "$$\\begin{array}{ll}\n",
    "\\operatorname{minimize} & \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\xi_{i} \\\\\n",
    "\\text { subject to } & \\xi_{i} \\geqslant \\max \\left(0,1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)\n",
    "\\end{array}$$  \n",
    "which is also equivalent to:  \n",
    "$$\\begin{aligned}\n",
    "&\\operatorname{minimize} \\quad \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\xi_{i}\\\\\n",
    "&\\text { subject to } \\quad \\xi_{i} \\geqslant\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right) \\text { for } i=1, \\ldots, n\\\\\n",
    "&\\xi_{i} \\geqslant 0 \\text { for } i=1, \\ldots, n\n",
    "\\end{aligned}$$  \n",
    "- Why equivalent?  \n",
    "Apparently, if we want to minimize the target, $\\xi_i$ tends to be as small as possible, but it cant be less than $\\max \\left(0,1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)$  \n",
    "## SVM as a quadratic program  \n",
    "- Differentiable objective function  \n",
    "- n + d + 1 unknowns and 2n affine constraints  \n",
    "## SVM dual problem  \n",
    "Let's write the objective program in Lagrangian form  \n",
    "$$L(w, b, \\xi, \\alpha, \\lambda)=\\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\xi_{i}+\\sum_{i=1}^{n} \\alpha_{i}\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]-\\xi_{i}\\right)+\\sum_{i=1}^{n} \\lambda_{i}\\left(-\\xi_{i}\\right)$$  \n",
    "- Reformulate it  \n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline \\text { Lagrange Multiplier } & \\text { Constraint } \\\\\n",
    "\\hline \\hline \\lambda_{i} & -\\xi_{i} \\leqslant 0 \\\\\n",
    "\\hline \\alpha_{i} & \\left(1-y_{i} f\\left(x_{i}\\right)\\right)-\\xi_{i} \\leqslant 0 \\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "$$\\begin{aligned}\n",
    "& L(w, b, \\xi, \\alpha, \\lambda) \\\\\n",
    "=& \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\varepsilon_{i}+\\sum_{i=1}^{n} \\alpha_{i}\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]-\\xi_{i}\\right)-\\sum_{i} \\lambda_{i} \\xi_{i} \\\\\n",
    "=& \\frac{1}{2} w^{T} w+\\sum_{i=1}^{n} \\xi_{i}\\left(\\frac{c}{n}-\\alpha_{i}-\\lambda_{i}\\right)+\\sum_{i=1}^{n} \\alpha_{i}\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)\n",
    "\\end{aligned}$$  \n",
    "- Then we can write the primal and dual form  \n",
    "$$\\begin{aligned}\n",
    "p^{*} &=\\inf _{w, \\xi, b} \\sup _{\\alpha, \\lambda \\succeq 0} L(w, b, \\xi, \\alpha, \\lambda) \\\\\n",
    "& \\geqslant \\sup _{\\alpha, \\lambda \\succeq 0} \\inf _{w, b, \\xi} L(w, b, \\xi, \\alpha, \\lambda)=d^{*}\n",
    "\\end{aligned}$$  \n",
    "\n",
    "## Strong Duality by Slater’s constraint qualiﬁcation\n",
    "- Convex problem + affine constraints $\\to$ strong duality if and only if the problem is feasible  \n",
    "- Is this problem feasible?  \n",
    "  - Yes, as long as we take $w = b = 0$ and $\\xi_i = 1$ for all $i = 1,2...n$\n",
    "  - Then strong duality exists  \n",
    "\n",
    "## Lagrangian dual function\n",
    "$$\\begin{aligned}\n",
    "& g(\\alpha, \\lambda)=\\inf _{w, b, \\xi} L(w, b, \\xi, \\alpha, \\lambda) \\\\\n",
    "=& \\inf _{w, b, \\xi}\\left[\\frac{1}{2} w^{T} w+\\sum_{i=1}^{n} \\xi_{i}\\left(\\frac{c}{n}-\\alpha_{i}-\\lambda_{i}\\right)+\\sum_{i=1}^{n} \\alpha_{i}\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)\\right]\n",
    "\\end{aligned}$$  \n",
    "- We can differentiate it and get $\\partial_{w} L=0, \\partial_{b} L=0, \\partial_{\\xi} L=0$  \n",
    "\n",
    "## SVM Dual Function: First Order Conditions\n",
    "$$\\begin{aligned}\n",
    "& g(\\alpha, \\lambda)=\\inf _{w, b, \\xi} L(w, b, \\xi, \\alpha, \\lambda) \\\\\n",
    "=& \\inf _{w, b, \\xi}\\left[\\frac{1}{2} w^{T} w+\\sum_{i=1}^{n} \\xi_{i}\\left(\\frac{c}{n}-\\alpha_{i}-\\lambda_{i}\\right)+\\sum_{i=1}^{n} \\alpha_{i}\\left(1-y_{i}\\left[w^{T} x_{i}+b\\right]\\right)\\right]\n",
    "\\end{aligned}$$  \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\partial_{w} L=0 \\quad \\Longleftrightarrow \\quad w-\\sum_{i=1}^{n} \\alpha_{i} y_{i} x_{i}=0 \\\\\n",
    "\\partial_{b} L=0 \\quad \\Longleftrightarrow \\quad-\\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\quad \\\\\n",
    "\\partial_{\\xi_{i}} L=0 \\quad \\Longleftrightarrow \\quad \\frac{c}{n}-\\alpha_{i}-\\lambda_{i}=0\n",
    "\\end{array}$$  \n",
    "plug these quations to $g(\\alpha, \\lambda)$  \n",
    "$$\\begin{aligned}\n",
    "g(\\alpha, \\lambda)&=\\inf _{w, b, \\xi} L(w, b, \\xi, \\alpha, \\lambda) \\\\  \n",
    "&= \\frac{1}{2}w^Tw + 0 + \\sum_{i=1}^n\\alpha_i - \\sum_{i=1}^n(\\alpha_iy_i)(w^Tx_i+b) \\\\  \n",
    "&= \\frac{1}{2}w^Tw + \\sum_{i = 1}^n\\alpha_iy_iw^Tx_i - \\sum_{i = 1}^n\\alpha_iy_ib\\\\  \n",
    "&= \\frac{1}{2}w^Tw + \\sum_{i = 1}^n\\alpha_i - w^T\\sum_{i=1}^n\\alpha_iy_ix_i\\\\  \n",
    "&= \\sum_{i=1}^n\\alpha_i - \\frac{1}{2}w^Tw\\\\  \n",
    "&= \\sum_{i = 1}^n\\alpha_i - \\frac{1}{2}\\sum_{i,j}^n\\alpha_i\\alpha_jy_iy_jx_j^Tx_i\n",
    "\\end{aligned}$$  \n",
    "then we have  \n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha} \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i} \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\\\\n",
    " \\alpha_{i} \\in\\left[0, \\frac{c}{n}\\right] i=1, \\ldots, n\n",
    "\\end{array}$$  \n",
    "\n",
    "- Given the solution $\\alpha^*$ to dual, the primal solution $w^*$ is $w^* = \\sum_{i=1}^n\\alpha_i^*y_ix_i$  \n",
    "- **$w^*$ is the \"span of data\"**  \n",
    "- Note $\\alpha_i \\in [0, \\frac{c}{n}]$, C controls max weights in each example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights from complementary slackness  \n",
    "## Support Vectors and Margin   \n",
    "Define $f^*(x) = X^Tw^* + b^*$\n",
    "- Incorrect classification: $yf^*(x) \\leq 0$  \n",
    "- Margin error: $yf^*(x) \\le 1$  \n",
    "- On the margin: $yf^*(x) = 1$  \n",
    "- Good side of the margin: $yf^*(x) \\ge 1$  \n",
    "\n",
    "Recall **slack variable** $\\xi_{i}^{*}=\\max \\left(0,1-y_{i} f^{*}\\left(x_{i}\\right)\\right)$ is the hinge loss on each $(x_i, y_i)$  \n",
    "- Suppose $\\xi^*_i = 0$  \n",
    "- then $y_{i} f^{*}\\left(x_{i}\\right) \\geqslant 1$  \n",
    "\n",
    "## Complementary Slackness  \n",
    "- Recall  \n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline \\text { Lagrange Multiplier } & \\text { Constraint } \\\\\n",
    "\\hline \\hline \\lambda_{i} & -\\xi_{i} \\leqslant 0 \\\\\n",
    "\\hline \\alpha_{i} & \\left(1-y_{i} f\\left(x_{i}\\right)\\right)-\\xi_{i} \\leqslant 0 \\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "- By strong duality, we must have  \n",
    "$$\\begin{array}{l}\n",
    "\\alpha_{i}^{*}\\left(1-y_{i} f^{*}\\left(x_{i}\\right)-\\xi_{i}^{*}\\right)=0 \\\\\n",
    "\\lambda_{i}^{*} \\xi_{i}^{*}=\\left(\\frac{c}{n}-\\alpha_{i}^{*}\\right) \\xi_{i}^{*}=0\n",
    "\\end{array}$$  \n",
    "- Discussion  \n",
    "  - If $y_if^*(x_I) > 1$, then $\\xi^*_i = 0$, and $\\alpha^*_i = 0$  \n",
    "  - If $y_if^*(x_I) < 1$, then $\\xi^*_i > 0$, and $\\alpha^*_i = \\frac{c}{n}$  \n",
    "  - If $\\alpha^*_i = 0$, then $\\xi^*_i = 0$, and then $y_if^*(x_I) > 1$   \n",
    "  - If $\\alpha_{i}^{*} \\in\\left(0, \\frac{c}{n}\\right)$, then $\\xi^*_i = 0$, which implies $1-y_{i} f^{*}\\left(x_{i}\\right)=0$  \n",
    "- Summary  \n",
    "$$\\begin{aligned}\n",
    "\\alpha_{i}^{*}=0 & \\Longrightarrow y_{i} f^{*}\\left(x_{i}\\right) \\geqslant 1 \\\\\n",
    "\\alpha_{i}^{*} \\in\\left(0, \\frac{c}{n}\\right) & \\Longrightarrow y_{i} f^{*}\\left(x_{i}\\right)=1 \\\\\n",
    "\\alpha_{i}^{*}=\\frac{c}{n} & \\Longrightarrow y_{i} f^{*}\\left(x_{i}\\right) \\leqslant 1 \\\\\n",
    "y_{i} f^{*}\\left(x_{i}\\right)<1 & \\Longrightarrow \\alpha_{i}^{*}=\\frac{c}{n} \\\\\n",
    "y_{i} f^{*}\\left(x_{i}\\right)=1 & \\Longrightarrow \\alpha_{i}^{*} \\in\\left[0, \\frac{c}{n}\\right] \\\\\n",
    "y_{i} f^{*}\\left(x_{i}\\right)>1 & \\Longrightarrow \\alpha_{i}^{*}=0\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vectors  \n",
    "- If $\\alpha^*_i$ is the solution to the dual problem, then the primal solution is  \n",
    "$$w^{*}=\\sum_{i=1}^{n} \\alpha_{i}^{*} y_{i} x_{i}$$  \n",
    "with $\\alpha_{i}^{*} \\in\\left[0, \\frac{c}{n}\\right]$  \n",
    "- The $x_i$'s corresponding to $\\alpha^*_i$ are called support vectors  \n",
    "- Few Margin errors or on the margin $\\to$ sparsity in examples  \n",
    "- As discussed previously, if $y_if^*(x_i) > 1$, which means (x_i, y_i) is correctly classified, $\\alpha^*_i$ becaomes 0, and its corresponding contribution to weight $w$ is 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complementary Slackness to get $b^*$  \n",
    "## The bias term: b \n",
    "- We can write the complementary slackness in this form  \n",
    "$$\\begin{aligned}\n",
    "\\alpha_{i}^{*}\\left(1-y_{i}\\left[x_{i}^{T} w^{*}+b\\right]-\\xi_{i}^{*}\\right) &=0 \\\\\n",
    "\\lambda_{i}^{*} \\xi_{i}^{*}=\\left(\\frac{c}{n}-\\alpha_{i}^{*}\\right) \\xi_{i}^{*} &=0\n",
    "\\end{aligned}$$  \n",
    "- Suppose there is an $\\alpha^*_i \\in (0,\\frac{c}{n})$\n",
    "- we have $y_if^*(x_i) = 1$  \n",
    "$$\\begin{aligned}\n",
    "& y_{i}\\left[x_{i}^{T} w^{*}+b^{*}\\right]=1 \\\\\n",
    "\\Longleftrightarrow & x_{i}^{T} w^{*}+b^{*}=y_{i}\\left(\\text { use } y_{i} \\in\\{-1,1\\}\\right) \\\\\n",
    "\\Longleftrightarrow & b^{*}=y_{i}-x_{i}^{T} w^{*}\n",
    "\\end{aligned}$$  \n",
    "- The optimal b is  \n",
    "$$b^{*}=y_{i}-x_{i}^{T} w^{*}$$  \n",
    "- We get the same $b^*$ for any choice of i  with $\\alpha^*_i \\in (0,\\frac{c}{n})$  \n",
    "  - So $b_i$ could be calculated by using only 1 eligible ($x_i,y_i$) after obtaing the optimal $w^*$\n",
    "- With calculation error, more robust to calculate the average over all eligible i's:  \n",
    "$$b^{*}=\\operatorname{mean}\\left\\{y_{i}-x_{i}^{T} w^{*} \\mid \\alpha_{i}^{*} \\in\\left(0, \\frac{c}{n}\\right)\\right\\}$$  \n",
    "- If no such eligible i?  \n",
    "  - Then we say a degenerate SVM training problem ($w^* = 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaser for Kernelization  \n",
    "- Till now we have not talked about methods to get the exact $\\alpha_i$, we assume we have the optimal solution, and we use complementary slackness to discuss under different situations.  \n",
    "- SVM dual problem  \n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha} \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i} \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\\\\n",
    " \\alpha_{i} \\in\\left[0, \\frac{c}{n}\\right] i=1, \\ldots, n\n",
    "\\end{array}$$  \n",
    "Note that all dependence on inputs $x_i$ and $x_j$ is through their inner product  \n",
    "We can replace $x^T_j x_i$ by any other inner product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
