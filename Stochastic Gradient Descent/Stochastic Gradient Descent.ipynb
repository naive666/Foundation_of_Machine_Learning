{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed step size\n",
    "- Initialize x = 0\n",
    "- repeat:\n",
    "  - x $\\leftarrow$ x - $\\eta \\nabla f(x)$, where $\\eta$ is the step size\n",
    "  - until the stop cretia is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, fixed step size is not good choice in many situations.\n",
    "## Covergence Theorem for Fixed Step Size\n",
    "Suppose $f: \\mathbb{R}^{d} \\to \\mathbb{R}$ is convex and differentiable, and $\\nabla f$ is **Lipschitz** continuous with constant $L > 0$, i.e.   \n",
    "$$\\lVert \\nabla f(x) - \\nabla f(y)\\rVert \\leq L \\lVert x - y \\rVert $$ $\\forall x, y \\in \\mathbb{R}^d$  \n",
    "Then the gradient descent with fixed step size $\\eta \\leq \\frac{1}{L}$ converges. In particular, we have:  \n",
    "$$f(x^{(k)}) - f(x^{*}) \\leq \\frac{\\lVert x^{(0)} - x^* \\rVert^2}{2\\eta k} $$  \n",
    "<br/>\n",
    "\n",
    "proof:  \n",
    "$x^{(1)}=x^{(0)}-\\eta \\nabla f\\left(x^{(0)}\\right)$  \n",
    "$x^{(2)}=x^{(1)}-\\eta \\nabla f\\left(x^{(1)}\\right)$   \n",
    ".  \n",
    ".  \n",
    "$x^{(k)}=x^{(k - 1)}-\\eta \\nabla f\\left(x^{(k - 1)}\\right)$  \n",
    "we want to prove the sequence $\\{x^{(0)}, x^{(1)}, ... , x^{(k)} \\}$ is convergent, we show that it is Cauchy.  \n",
    "let $m, n \\in \\mathbb{R}^{d}$, and $m > n$,  \n",
    "$\\begin{aligned} \n",
    "\\| x^{(m)}-x^{(n)} &\\| =\\| x^{(m-1)}-\\eta \\nabla f\\left(x^{(m-1)}\\right)-x^{(n-1)}+\\eta \\nabla f\\left(x^{(n-1)}\\right) \\| \\\\ &=\\left\\|x^{(m-1)}-x^{(n-1)}+\\eta \\nabla f\\left(x^{(n-1)}\\right)-\\eta \\nabla f\\left(x^{(m-1)}\\right)\\right\\| \\\\ \n",
    "&\\leqslant \\| x^{(m-1)}-x^{(n-1)} \\|+\\eta\\| \\nabla f\\left(x^{(m-1)}\\right)-\\nabla f\\left(x^{(n-1)}\\right) \\| \\\\ \n",
    "&\\leqslant \\| x^{(m-1)}-x^{(n-1)} \\|+\\eta L \\|x^{(m-1)}-x^{(n-1)} \\|\\\\ \n",
    "&\\leqslant 2\\|x^{(m-1)}-x^{(m-1)}\\| \n",
    "\\end{aligned}$  \n",
    "if $\\eta \\leqslant \\frac{1}{L}$,  \n",
    "$\\Rightarrow\\|x^{(m)}-x^{(n)}\\| \\leqslant 2^{n}\\|x^{(m-n)}-x^{(0)}\\|<\\epsilon$, if we take some $N$ and $m, n \\ge N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Stop?\n",
    "- Wait until $\\|\\nabla f(x)\\| \\leqslant \\epsilon$, for some $\\epsilon$ you choose\n",
    "- Stop when the result worsen or not improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Empirical Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Squares Regression\n",
    "**Setup**  \n",
    "- Input Space $\\mathcal{X} = \\mathcal{R}^d$  \n",
    "- Output Space $\\mathcal{y} = \\mathcal{R}$\n",
    "- Action Space $\\mathcal{y} = \\mathcal{R}$  \n",
    "- loss $\\ell(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$  \n",
    "- Hypothesis Space $\\mathcal{F} = \\{\\mathcal{f}: \\mathcal{R}^d \\to \\mathcal{R}|w^Tx, w\\in \\mathcal{R}^d \\}$  \n",
    "\n",
    "**Empirical Risk**  \n",
    "$$\\hat{R}_{n}(w)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(w^{T} x_{i}-y_{i}\\right)^{2}$$  \n",
    "where $w \\in \\mathcal{R}^d$ parameterizes the hypothesis space $\\mathcal{F}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Empirical Risk Averages\n",
    "Generally, our hypothesis space is $\\mathcal{F} = \\{\\mathcal{f_w}: \\mathcal{X} \\to \\mathcal{A}| w\\in \\mathcal{R}^d \\}$, and ERM is to find $w$ minimizing \n",
    "$$\\hat{R}_{n}(w)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell(f_{w}(x_{i})-y_{i})$$  \n",
    "suppose $\\ell(f_{w}(x_{i})-y_{i})$ is differentiable with respect to $w$, then we can do gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How does it scale with n?\n",
    "$$\\nabla \\hat{R}_{n}(w)=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{w} \\ell\\left(f_{w}\\left(x_{i}\\right), y_{i}\\right)$$\n",
    "- At each single iteration, we have to touch $n$ points, it is comuptational expensive.  \n",
    "- Can we make progress without looking at all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch Gradient\n",
    "Suppose $\\mathcal{D_{n}} = \\{(x_1, y_1),...,(x_n, y_n)\\}$ is our full dataset, let's make a subsample of size $N$.  \n",
    "$\\{(x_{m_1}, y_{m_1}),...,(x_{m_N}, y_{m_N})\\}$,  \n",
    "then the minibatch gradient descent is \n",
    "$$\\nabla \\hat{R}_{N}(w)=\\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{w} \\ell\\left(f_{w}\\left(x_{m_{i}}\\right), y_{m_{i}}\\right)$$  \n",
    "\n",
    "## What can we say about the minibatch descent?\n",
    "- What is the expected value?\n",
    "$$\\begin{aligned} \\mathbb{E}\\left[\\nabla \\hat{R}_{N}(w)\\right] &=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\nabla_{w} \\ell\\left(f_{w}\\left(x_{m_{i}}\\right), y_{m_{i}}\\right)\\right] \\\\ \n",
    "&=\\mathbb{E}\\left[\\nabla_{w} \\ell\\left(f_{w}\\left(x_{m_{1}}\\right), y_{m_{1}}\\right)\\right] \\\\\n",
    "&= \\sum_{i = 1}^{n}P(m_{1} = i)\\nabla_{w}\\ell(f_w(x_i), y_i) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i = 1}^{n}\\nabla_{w}\\ell(f_w(x_i), y_i) \\\\\n",
    "&= \\nabla \\hat{R}_n(w)\n",
    "\\end{aligned}$$ \n",
    "- Minibatch gradient is an **unbiased estimator** for the full batch gradient.\n",
    "- Tradeoï¬€s of minibatch size:\n",
    "  - Bigger $N$, better estimate of gradient, but slower\n",
    "  - Smaller $N$, quicker but worse estimate\n",
    "- Even N = 1 works, it's called **stochastic gradient descent (SGD)**\n",
    "\n",
    "## Minibatch Algorithm\n",
    "- Initialize w = 0  \n",
    "- Repeat \n",
    "   - Randomly choose a subsample with $N$ points\n",
    "   - $w \\leftarrow w-\\eta\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{w} \\ell\\left(f_{w}\\left(x_{i}\\right), y_{i}\\right)\\right]$\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- Initialize w = 0\n",
    "- Repeat \n",
    "   - Randomly choose a training point $(x_i, y_i)$ \n",
    "   - $w \\leftarrow w-\\eta \\nabla_{w} \\ell\\left(f_{w}\\left(x_{i}\\right), y_{i}\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
