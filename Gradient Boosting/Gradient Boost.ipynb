{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "## Nonlinear Regression\n",
    "- Suppose we have the following regression problem:  \n",
    "<div align=\"center\"><img src = \"./nonlinear.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "- What are some options?   \n",
    "- basis functions, kernel methods, trees, neural nets, ...  \n",
    "\n",
    "## Linear Model with Basis Functions\n",
    "- Choose some basis functions on input space $X$  \n",
    "$$\n",
    "g_{1}, \\ldots, g_{M}: X \\rightarrow \\mathrm{R}\n",
    "$$  \n",
    "- Predict with linear combination of basis functions:  \n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M} v_{m} g_{m}(x)\n",
    "$$  \n",
    "- Can ﬁt this using standard methods for linear models (e.g. least squares, lasso, ridge, etc.)\n",
    "- In ML parlance, basis functions are called features or feature functions.\n",
    "- $f (x)$ is a number — for regression, it’s exactly what we’re looking for.   \n",
    "- Otherwise, $f (x)$ is often called a score function  \n",
    "- It can be   \n",
    "  - thresholded to get a classiﬁcation  \n",
    "  - transformed to get a probability   \n",
    "  - transformed to get a parameter of a probability distribution (e.g. Poisson regression)  \n",
    "  - used for ranking search results\n",
    "\n",
    "## Adaptive Basis Function Model  \n",
    "- Base hypothesis space $H$ consisting of functions $h : X \\to R$.  \n",
    "  - We will choose our “basis functions” or “features” from this set of functions  \n",
    "- An adaptive basis function expansion over $H$ is  \n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M} v_{m} h_{m}(x)\n",
    "$$  \n",
    "where $v_m \\in R$ and $h_m \\in H$ are chosen based on training data.\n",
    "\n",
    "- Combined hypothesis space: $F_M$:  \n",
    "$$\n",
    "\\mathcal{F}_{M}=\\left\\{\\sum_{m=1}^{M} v_{m} h_{m}(x) \\mid v_{m} \\in \\mathbf{R}, h_{m} \\in \\mathcal{H}, m=1, \\ldots, M\\right\\}\n",
    "$$  \n",
    "- Suppose we’re given some data $D = ((x_1,y_1),...,(x_n,y_n))$.  \n",
    "- Learning is choosing $v_1,...,v_M \\in R$ and $h_1,...,h_M \\in H$ to ﬁt $D$.  \n",
    "\n",
    "## Empirical Risk Minimization\n",
    "- We’ll consider learning by **empirical risk minimization**:  \n",
    "$$\n",
    "\\hat{f}=\\underset{f \\in \\mathcal{F}_{M}}{\\operatorname{argmin}} \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(y_{i}, f\\left(x_{i}\\right)\\right)\n",
    "$$  \n",
    "for some function $l(\\hat(y),y)$  \n",
    "- Write ERM objective function as  \n",
    "$$\n",
    "J\\left(v_{1}, \\ldots, v_{M}, h_{1}, \\ldots, h_{M}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(y_{i}, \\sum_{m=1}^{M} v_{m} h_{m}(x)\\right)\n",
    "$$  \n",
    "- How to optimize J? i.e. how to learn?\n",
    "\n",
    "## Gradient-Based Methods\n",
    "- Suppose our base hypothesis space is parameterized by $\\Theta =R^b$:(Linear Base space)  \n",
    "$$\n",
    "J\\left(v_{1}, \\ldots, v_{M}, \\theta_{1}, \\ldots, \\theta_{M}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(y_{i}, \\sum_{m=1}^{M} v_{m} h\\left(x ; \\theta_{m}\\right)\\right)\n",
    "$$  \n",
    "- Can we can diﬀerentiate $J$ w.r.t. $v_m$’s and $\\theta_m$’s? Optimize with SGD?  \n",
    "- For some hypothesis spaces and typical loss functions, yes!   \n",
    "- Neural networks fall into this category! ($h_1,...,h_M$ are neurons of last hidden layer.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if Gradient Based Methods Don’t Apply?  \n",
    "- What if base hypothesis space $H$ consists of decision trees?  \n",
    "- Can we even parameterize trees with $\\Theta = R^b$?  \n",
    "- Even if we could for some set of trees,   \n",
    "  - predictions would not change continuously w.r.t. $\\theta \\in \\Theta$,  \n",
    "  - and so certainly not diﬀerentiable  \n",
    "- Today we’ll discuss **gradient boosting**. It applies whenever   \n",
    "  - our loss function is [sub]diﬀerentiable w.r.t. training predictions $f (x_i)$, and   \n",
    "  - we can do regression with the base hypothesis space $H $(e.g. regression trees).  \n",
    "\n",
    "## Overview  \n",
    "- Forward stagewise additive modeling (FSAM)   \n",
    "  - example: L2 Boosting \n",
    "  - example: exponential loss gives AdaBoost \n",
    "  - Not clear how to do it with many other losses, including logistic loss   \n",
    "- Gradient Boosting example:   \n",
    "  - logistic loss gives BinomialBoost   \n",
    "- Variations on Gradient Boosting   \n",
    "  - step size selection   \n",
    "  - stochastic row/column selection   \n",
    "  - XGBoost  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward stagewise additive modeling (FSAM)    \n",
    "- FSAM is an iterative optimization algorithm for ﬁtting adaptive basis function models.  \n",
    "- Start with $f_0 ≡ 0$.  \n",
    "- After $m−1$ stages, we have  \n",
    "$$\n",
    "f_{m-1}=\\sum_{i=1}^{m-1} v_{i} h_{i}\n",
    "$$  \n",
    "- In $m$’th round, we want to ﬁnd   \n",
    "  - step direction $h_m \\in H$ (i.e. a basis function) and \n",
    "  - step size $v_i > 0$  \n",
    "- such that\n",
    "$$\n",
    "f_{m}=f_{m-1}+v_{i} h_{m}\n",
    "$$  \n",
    "improves objective function value by as much as possible  \n",
    "\n",
    "## Forward Stagewise Additive Modeling for ERM\n",
    "- Initialize $f_0(x) =0$.  \n",
    "-  For $m = 1$ to $M$:  \n",
    "  -  Compute:\n",
    "$$\n",
    "\\left(v_{m}, h_{m}\\right)=\\underset{v \\in \\mathbf{R}, h \\in \\mathcal{H}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n} \\ell(y_{i}, f_{m-1}\\left(x_{i}\\right) \\underbrace{+v h\\left(x_{i}\\right)}_{\\text {new piece }})\n",
    "$$  \n",
    "  - Set $f_m = f_{m−1} +ν_m h$.  \n",
    "-  Return: $f_M$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example $L^2$ Boosting  \n",
    "- Suppose we use the square loss. Then in each step we minimize  \n",
    "$$\n",
    "J(v, h)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-[f_{m-1}\\left(x_{i}\\right) \\underbrace{+v h\\left(x_{i}\\right)}_{\\text {new piece }}]\\right)^{2}\n",
    "$$  \n",
    "- If $H$ is closed under rescaling (i.e. if $h\\in H$, then $vh \\in H$ for all $h \\in R$),  then don’t need $ν$.  \n",
    "- Take $ν = 1$ and minimize   \n",
    "$$\n",
    "J(h)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\left[y_{i}-f_{m-1}\\left(x_{i}\\right)\\right]-h\\left(x_{i}\\right)\\right)^{2}\n",
    "$$  \n",
    "- This is just ﬁtting the residuals with least-squares regression!   \n",
    "- If we can do regression with our base hypothesis space $H$, then we’re set!  \n",
    "\n",
    "## Recall: Regression Stumps\n",
    "- A regression stump is a function of the form $h(x) = a1(x_i \\leq c)+b1(x_i > c)$\n",
    "<div align=\"center\"><img src = \"./regression stump.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## L2 Boosting with Decision Stumps: Demo\n",
    "- Consider FSAM with $L_2$ loss (i.e. $L_2$ Boosting)  \n",
    "- For base hypothesis space of regression stumps  \n",
    "- Data we’ll ﬁt with code:\n",
    "<div align=\"center\"><img src = \"./data.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "  \n",
    "## L2 Boosting with Decision Stumps: Results\n",
    "<div align=\"center\"><img src = \"./result.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "<div align=\"center\"><img src = \"./result2.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "<div align=\"center\"><img src = \"./result3.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: AdaBoost  \n",
    "## The Classiﬁcation Problem\n",
    "- Outcome space $Y = \\{−1,1\\}$  \n",
    "- Action space $A = R$  \n",
    "- Score function $f : X \\to A$.  \n",
    "- Margin for example $(x,y)$ is $m = yf (x)$.  \n",
    "  - $m > 0 \\Leftrightarrow$ classiﬁcation correct   \n",
    "  - Larger $m$ is better.  \n",
    "  \n",
    "## Margin-Based Losses for Classiﬁcation\n",
    "- Introduce the exponential loss: $l(y,f (x)) = e^{−yf (x)}$.\n",
    "<div align=\"center\"><img src = \"./exponential loss.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## FSAM with Exponential Loss\n",
    "- Consider classiﬁcation setting: $Y = \\{−1,1\\}$.  \n",
    "- Take loss function to be the **exponential loss**: $l(y,f (x)) = e^{−yf (x)}$.  \n",
    "- Let $H$ be a base hypothesis space of classiﬁers $h : X \\to \\{−1,1\\}$.  \n",
    "- Then Forward Stagewise Additive Modeling (FSAM) reduces to a **version of AdaBoost**.    \n",
    "**Note that exponential loss puts a very large weight on bad misclassiﬁcations**  \n",
    "\n",
    "## AdaBoost / Exponential Loss: Robustness Issues\n",
    "- When Bayes error rate is high (e.g. $P(f^∗(X)\\ne Y) = 0.25)$  \n",
    "  - e.g. there’s some intrinsic randomness in the label   \n",
    "  - e.g. training examples with same input, but diﬀerent classiﬁcations.   \n",
    "- Best we can do is predict the most likely class for each $x$.   \n",
    "- Some training predictions should be wrong,  \n",
    "  - because example doesn’t have majority class  \n",
    "  - AdaBoost / exponential loss puts a lot of focus on getting those right  \n",
    "- Empirically, AdaBoost has degraded performance in situations with  \n",
    "  - high Bayes error rate, or when there’s high “label noise”   \n",
    "- Logistic loss performs better in settings with high Bayes error\n",
    "\n",
    "## FSAM for Other Loss Functions\n",
    "- We know how to do FSAM for certain loss functions\n",
    "  - e.g square loss, absolute loss, exponential loss,...   \n",
    "- In each case, happens to reduce to another problem we know how to solve.   \n",
    "- However, not clear how to do FSAM in general  \n",
    "- For example, logistic loss / cross-entropy loss?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost/ Any Boost  \n",
    "## FSAM Is Iterative Optimization  \n",
    "- The FSAM step\n",
    "$$\n",
    "\\left(v_{m}, h_{m}\\right)=\\underset{v \\in \\mathbf{R}, h \\in \\mathcal{H}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n} \\ell(y_{i}, f_{m-1}\\left(x_{i}\\right) \\underbrace{+v h\\left(x_{i}\\right)}_{\\text {new piece }})\n",
    "$$  \n",
    "- Hard part: ﬁnding the best step direction $h$.   \n",
    "- What if we looked for the locally best step direction?  \n",
    "  - like in gradient descent\n",
    "\n",
    "## “Functional” Gradient Descent\n",
    "- We want to minimize\n",
    "$$\n",
    "J(f)=\\sum_{i=1}^{n} \\ell\\left(y_{i}, f\\left(x_{i}\\right)\\right)\n",
    "$$  \n",
    "- In some sense, we want to take the gradient w.r.t. $f$, whatever that means.  \n",
    "- $J(f)$ only depends on $f$ at the n training points.  \n",
    "- Define  \n",
    "$$\n",
    "\\mathbf{f}=\\left(f\\left(x_{1}\\right), \\ldots, f\\left(x_{n}\\right)\\right)^{T}\n",
    "$$  \n",
    "- and write the objective function as\n",
    "$$\n",
    "J(\\mathbf{f})=\\sum_{i=1}^{n} \\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)\n",
    "$$  \n",
    "\n",
    "## Functional Gradient Descent: Unconstrained Step Direction\n",
    "- Consider gradient descent on\n",
    "$$\n",
    "J(\\mathbf{f})=\\sum_{i=1}^{n} \\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)\n",
    "$$  \n",
    "- The **negative gradient step direction** at $f$ is  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\mathbf{g} &=-\\nabla_{\\boldsymbol{f}} J(\\mathbf{f}) \\\\\n",
    "&=-\\left(\\partial_{\\mathbf{f}_{1}} \\ell\\left(y_{1}, \\mathbf{f}_{1}\\right), \\ldots, \\partial_{\\mathbf{f}_{n}} \\ell\\left(y_{n}, \\mathbf{f}_{n}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "which we can easily calculate.   \n",
    "- $-g\\in R_n$ is the direction we want to change each of our $n$ predictions on training data.   \n",
    "- Eventually we need more than just $\\mathbf{f}$, which is just predictions on training.\n",
    "\n",
    "## Unconstrained Functional Gradient Stepping\n",
    "\n",
    "<div align=\"center\"><img src = \"./functional boost.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- $R(f)$ is the empirical risk, where $f = (f (x_1),f (x_2))$ are predictions on training set.   \n",
    "- Issue: $\\hat{f}_M$ only deﬁned at training points  \n",
    "\n",
    "## Functional Gradient Descent: Projection Step\n",
    "- Unconstrained step direction is  \n",
    "$$\n",
    "-\\mathbf{g}=-\\nabla_{\\boldsymbol{f}} J(\\mathbf{f})=-\\left(\\partial_{\\mathbf{f}_{1}} \\ell\\left(y_{1}, \\mathbf{f}_{1}\\right), \\ldots, \\partial_{\\mathbf{f}_{n}} \\ell\\left(y_{n}, \\mathbf{f}_{n}\\right)\\right)\n",
    "$$   \n",
    "- Also called the “pseudo-residuals”   \n",
    "  - (for square loss, they’re exactly the residuals)   \n",
    "- Find the closest base hypothesis $h \\in H$ in $l_2$ sense  \n",
    "$$\n",
    "\\min _{h \\in \\mathcal{H}} \\sum_{i=1}^{n}\\left(-\\mathbf{g}_{i}-h\\left(x_{i}\\right)\\right)^{2}\n",
    "$$  \n",
    "- This is a least squares regression problem over hypothesis space $H$.  \n",
    "- **Take the $h\\in H$ that best approximates $−g$ as our step direction.**  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src = \"./projection.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "$T(x;p)\\in H$ is our actual step direction – like the projection of $-g=-\\nabla R(f)$ onto $H$.  \n",
    "\n",
    "## Functional Gradient Descent: Step Size\n",
    "- Finally, we choose a stepsize.  \n",
    "- Option 1 (Line search):\n",
    "$$\n",
    "v_{m}=\\underset{v>0}{\\arg \\min } \\sum_{i=1}^{n} \\ell\\left\\{y_{i}, f_{m-1}\\left(x_{i}\\right)+v h_{m}\\left(x_{i}\\right)\\right\\}\n",
    "$$    \n",
    "- Option 2: (Shrinkage parameter – more common)   \n",
    "  - We consider $ν =1$ to be the full gradient step.   \n",
    "  - Choose a ﬁxed $ν\\in (0,1)$ – called a shrinkage parameter  \n",
    "  - A value of $ν =0.1$ is typical – optimize as a hyperparameter .\n",
    "\n",
    "## The Gradient Boosting Machine Ingredients (Recap)\n",
    "- Take any [sub]diﬀerentiable loss function.   \n",
    "- Choose a base hypothesis space for regression.  \n",
    "- Choose number of steps (or a stopping criterion).   \n",
    "- Choose step size methodology.   \n",
    "- Then you’re good to go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: BinomialBoost  \n",
    "## BinomialBoost: Gradient Boosting with Logistic Loss\n",
    "- Recall the logistic loss for classiﬁcation, with $Y = \\{−1,1\\}$:  \n",
    "$$\n",
    "\\ell(y, f(x))=\\log \\left(1+e^{-y f(x)}\\right)\n",
    "$$  \n",
    "- Pseudoresidual for $i$’th example is negative derivative of loss w.r.t. prediction:   \n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_{i} &=-\\partial_{f\\left(x_{i}\\right)}\\left[\\log \\left(1+e^{-y_{i} f\\left(x_{i}\\right)}\\right)\\right] \\\\\n",
    "&=\\frac{y_{i} e^{-y_{i} f\\left(x_{i}\\right)}}{1+e^{-y_{i} f\\left(x_{i}\\right)}} \\\\\n",
    "&=\\frac{y_{i}}{1+e^{y_{i} f\\left(x_{i}\\right)}}\n",
    "\\end{aligned}\n",
    "$$   \n",
    "- Pseudoresidual for $i$th example:   \n",
    "$$\n",
    "r_{i}=-\\partial_{f\\left(x_{i}\\right)}\\left[\\log \\left(1+e^{-y_{i} f\\left(x_{i}\\right)}\\right)\\right]=\\frac{y_{i}}{1+e^{y_{i} f\\left(x_{i}\\right)}}\n",
    "$$  \n",
    "So if $f_{m−1}(x)$ is prediction after $m−1$ rounds, step direction for $m$’th round is  \n",
    "$$\n",
    "h_{m}=\\underset{h \\in \\mathcal{H}}{\\operatorname{argmin}} \\sum_{i=1}^{n}\\left[\\left(\\frac{y_{i}}{1+e^{y_{i} f_{m-1}\\left(x_{i}\\right)}}\\right)-h\\left(x_{i}\\right)\\right]^{2}\n",
    "$$  \n",
    "- and\n",
    "$$\n",
    "f_{m}(x)=f_{m-1}(x)+v h_{m}(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Tree Boosting  \n",
    "## Gradient Tree Boosting\n",
    "- One common form of gradient boosting machine takes\n",
    "$$\n",
    "H = \\{\\text{regression trees of size } J\\},\n",
    "$$  \n",
    "where $J$ is the number of terminal nodes.  \n",
    "- $J = 2$ gives decision stumps  \n",
    "- HTF recommends $4\\leq J \\leq 8$ (but more recent results use much larger trees)   \n",
    "- Software packages:   \n",
    "  - Gradient tree boosting is implemented by the **gbm package for R**   \n",
    "  - as GradientBoostingClassifier and GradientBoostingRegressor in sklearn  \n",
    "  - **xgboost** and **lightGBM** are state of the art for speed and performance  \n",
    "  \n",
    "# GBM Regression with Stumps  \n",
    "Sinc Function: Our Dataset\n",
    "<div align=\"center\"><img src = \"./sinc.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Minimizing Square Loss with Ensemble of Decision Stumps\n",
    "<div align=\"center\"><img src = \"./ensemble stumps.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Decision stumps with $1,10,50,$ and $100$ steps, step size $\\lambda = 1$.  \n",
    "\n",
    "## Step Size as Regularization\n",
    "<div align=\"center\"><img src = \"./lambda.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Rule of Thumb\n",
    "- The smaller the step size, the more steps you’ll need.  \n",
    "- But never seems to make results worse, and often better.  \n",
    "- So set your step size as small as you have patience for.\n",
    "\n",
    "# Variation on Gradient Boosting  \n",
    "## Stochastic Gradient Boosting\n",
    "- For each stage,   \n",
    "  - choose random subset of data for computing projected gradient step.  \n",
    "  - “Typically, about 50% of the dataset size, can be much smaller for large training set.”   \n",
    "  - Fraction is called the bag fraction.   \n",
    "- Why do this?   \n",
    "  - Subsample percentage is additional regularization parameter – may help overﬁtting.   \n",
    "  - Faster.   \n",
    "- We can view this is a **minibatch method**. \n",
    "  - we’re estimating the “true” step direction (the projected gradient) using a subset of data\n",
    "\n",
    "## Bag as Minibatch\n",
    "- Just as we argued for minibatch SGD,   \n",
    "  - sample size needed for a good estimate of step direction is independent of training set size   \n",
    "- Minibatch size should depend on   \n",
    "  - the complexity of base hypothesis space   \n",
    "  - the complexity of the target function (Bayes decision function)   \n",
    "- Seems like an interesting area for both practical and theoretical pursuit.\n",
    "- Column / Feature Subsampling for Regularization  \n",
    "  - Similar to random forest, randomly choose a subset of features for each round.\n",
    "  - XGBoost paper says: “According to user feedback, using column sub-sampling prevents overﬁtting even more so than the traditional row sub-sampling.”\n",
    "\n",
    "## Newton Step Direction\n",
    "- For GBM, we ﬁnd the closest $h\\in F$ to the negative gradient   \n",
    "$$\n",
    "-\\mathbf{g}=-\\nabla_{\\boldsymbol{f}} J(\\mathbf{f})\n",
    "$$  \n",
    "- This is a “ﬁrst order” method.  \n",
    "- Newton’s method is a “second order method”:  \n",
    "  - Find 2nd order (quadratic) approximation to $J$ at $f$.  \n",
    "  - Requires computing gradient and Hessian of $J$.  \n",
    "  - Newton step direction points towards minimizer of the quadratic  \n",
    "  - Minimizer of quadratic is easy to ﬁnd in closed form  \n",
    "- Boosting methods with projected Newton step direction:   \n",
    "  - LogitBoost (logistic loss function)   \n",
    "  - XGBoost (any loss – uses regression trees for base classiﬁer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Step Direction for GBM\n",
    "- Generically, second order Taylor expansion of $J$ at $f$ in direction $r$  \n",
    "$$\n",
    "J(\\mathbf{f}+\\mathbf{r})=J(\\mathbf{f})+\\left[\\nabla_{\\mathbf{f}} J(\\mathbf{f})\\right]^{T} \\mathbf{r}+\\frac{1}{2} \\mathbf{r}^{T}\\left[\\nabla_{\\mathbf{f}}^{2} J(\\mathbf{f})\\right] \\mathbf{r}\n",
    "$$  \n",
    "- For $J(\\mathbf{f})=\\sum_{i=1}^{n} \\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)$  \n",
    "$$\n",
    "J(\\mathbf{f}+\\mathbf{r})=\\sum_{i=1}^{n}\\left[\\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)+g_{i} \\mathbf{r}_{i}+\\frac{1}{2} h_{i} \\mathbf{r}_{i}^{2}\\right]\n",
    "$$  \n",
    "where $g_{i}=\\partial_{\\mathbf{f}_{i}} \\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)$, and $h_{i}=\\partial_{\\mathbf{f}_{i}}^{2} \\ell\\left(y_{i}, \\mathbf{f}_{i}\\right)$  \n",
    "- Can ﬁnd $r$ that minimizes $J(f+r)$ in closed form  \n",
    "- Can take step direction to be “projection” of $r$ into base hypothesis space $H$.  \n",
    "\n",
    "## XGBoost: Objective Function with Tree Penalty Term\n",
    "- Adds explicit penalty term on tree complexity to the empirical risk:  \n",
    "$$\n",
    "\\Omega(r)=\\gamma T+\\frac{1}{2} \\lambda \\sum_{i=1}^{T} w_{j}^{2}\n",
    "$$  \n",
    "where $r \\in H$ is a regression tree from our base hypothesis space and   \n",
    "  - $T$ is the number of leaf nodes and   \n",
    "  - $w_j$ is the prediction in the $j$’th node   \n",
    "- Objective function at step $m$:  \n",
    "$$\n",
    "J(r)=\\sum_{i=1}^{n}\\left[g_{i} r\\left(x_{i}\\right)+\\frac{1}{2} h_{i} r\\left(x_{i}\\right)^{2}\\right]+\\Omega(r)\n",
    "$$  \n",
    "- In XGBoost, they also use this objective to decide on tree splits  \n",
    "\n",
    "## XGBoost: Rewriting objective function\n",
    "- For a given tree, let $q(x_i)$ be $x_i$’s node assignment and $w_j$ the prediction for node $j$.  \n",
    "- In each step of XGBoost we’re looking for a tree that minimizes  \n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left[g_{i} w_{q\\left(x_{i}\\right)}+\\frac{1}{2} h_{i} w_{q\\left(x_{i}\\right)}^{2}\\right]+\\gamma T+\\frac{1}{2} \\lambda \\sum_{i=1}^{T} w_{j}^{2}\n",
    "$$\n",
    "$$\n",
    "=\\sum_{\\text {leaf node } j=1}^{T}\\left[(\\underbrace{\\sum_{i \\in I_{j}} g_{i}}_{G_{j}}) w_{j}+\\frac{1}{2}(\\underbrace{\\sum_{i \\in I_{j}} h_{i}+\\lambda}_{H_{j}}) w_{j}^{2}\\right]+\\gamma T\n",
    "$$  \n",
    "where $l_{j}=\\left\\{i \\mid q\\left(x_{i}\\right)=j\\right\\}$  is set of training example indices landing in leaf $j$.  \n",
    "- Simpliﬁes to  \n",
    "$$\n",
    "\\sum_{j=1}^{T}\\left[G_{j} w_{j}+\\frac{1}{2}\\left(H_{j}+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T\n",
    "$$  \n",
    "- For ﬁxed $q(x)$ (i.e. ﬁxed tree partitioning), objective minimized when leaf node values are   \n",
    "$$\n",
    "w_{j}^{*}=-G_{j} /\\left(H_{j}+\\lambda\\right)\n",
    "$$  \n",
    "- Plugging $w^∗_j$ back in, this objective reduces to  \n",
    "$$\n",
    "-\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_{j}^{2}}{H_{j}+\\lambda}+\\gamma T\n",
    "$$  \n",
    "- which we can think of as the loss for tree partitioning function $q(x)$.  \n",
    "- If time were no issue, we could search over all trees to mininize this objective.  \n",
    "- Expression to evaluate a tree’s node assignment function $q(x)$:  \n",
    "$$\n",
    "-\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_{j}^{2}}{H_{j}+\\lambda}+\\gamma T\n",
    "$$  \n",
    "where $G_{j}=\\sum_{i \\in I_{j}} g_{i}$  for examples $i$ assigned to leaf node $j$,and $H_{j}=\\sum_{i \\in l_{j}} h_{i}$  \n",
    "- Suppose we’re considering splitting some data into two nodes: $L$ and $R$.  \n",
    "- Loss of tree with this one split is  \n",
    "$$\n",
    "-\\frac{1}{2}\\left[\\frac{G_{L}^{2}}{H_{L}+\\lambda}+\\frac{G_{R}^{2}}{H_{R}+\\lambda}\\right]+2 \\gamma\n",
    "$$  \n",
    "-Without the split – i.e. a tree with a single leaf node, loss is\n",
    "$$\n",
    "-\\frac{1}{2}\\left[\\frac{\\left(G_{L}+G_{R}\\right)^{2}}{H_{L}+H_{R}+\\lambda}\\right]+\\gamma\n",
    "$$  \n",
    "\n",
    "## XGBoost: Node Splitting Criterion\n",
    "- We can deﬁne the gain of a split to be the reduction in objective between tree with and without split:  \n",
    "$$\n",
    "\\text { Gain }=\\frac{1}{2}\\left[\\frac{G_{L}^{2}}{H_{L}+\\lambda}+\\frac{G_{R}^{2}}{H_{R}+\\lambda}-\\frac{\\left(G_{L}+G_{R}\\right)^{2}}{H_{L}+H_{R}+\\lambda}\\right]-\\gamma\n",
    "$$  \n",
    "- Tree building method: recursively choose split that maximizes the gain.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
