{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Information Prediction Function (Classiﬁcation)\n",
    "For classiﬁcation, let $y_{mode}$ be the most frequently occurring class in training\n",
    "  - Prediction function that always predicts $y_{mode}$ is called  \n",
    "    - zero-information prediction function, or \n",
    "    - no-information prediction function   \n",
    "\n",
    "# Zero-Information Prediction Function (Regression)\n",
    "What’s the right zero-Information prediction function for square loss?   \n",
    "- Mean of training data labels   \n",
    "What’s the right zero-Information prediction function for absolute loss?   \n",
    "- Median of training data labels  \n",
    "\n",
    "# Regularized Linear Model\n",
    "Whatever fancy model you are using (gradient boosting, neural networks, etc.)  \n",
    "- always spend some time building a linear baseline model \n",
    "Build a regularized linear model  \n",
    "If your fancier model isn’t beating linear  \n",
    "- perhaps something’s wrong with your fancier model (e.g. hyperparameter settings), or  \n",
    "- you don’t have enough data to beat the simpler model  \n",
    "Prefer simpler models if performance is the same  \n",
    "- usually cheaper to train and easier to deploy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Models\n",
    "- Often helpful to get an upper bound on achievable performance  \n",
    "- What’s the best performance function you can get, looking at your validation/test data?   \n",
    "  - Performance will estimate the Bayes risk (i.e. optimal error rate). \n",
    "  - This won’t always be 0 - why?  \n",
    "- Using same model class as your ML model  \n",
    "  - ﬁt to the validation/test data without regularization  \n",
    "  - Performance will tell us the limit of our model class, even with inﬁnite training data  \n",
    "  - Gives estimate of the approximation error of our hypothesis space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing Classifier Performance  \n",
    "## Confusion Matrix  \n",
    "A **confusion matrix** summarizes results for a binary classiﬁcation problem:  \n",
    "<div align=\"center\"><img src = \"./confusion matrix.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- a is the number of examples of Class 0 that the classiﬁer predicted [correctly] as Class 0.  \n",
    "- b is the number of examples of Class 1 that the classiﬁer predicted [incorrectly] as Class 0.  \n",
    "- c is the number of examples of Class 0 that the classiﬁer predicted [incorrectly] as Class 1.  \n",
    "- d is the number of examples of Class 1 that the classiﬁer predicted [correctly] as Class 1.  \n",
    " \n",
    "- **Accuracy** is the fraction of correct predictions  \n",
    "$$\\frac{a + d}{a + b + c + d}$$\n",
    "- **Error rate** is the fraction of incorrect predictions  \n",
    "$$\\frac{b + c}{a + b + c + d}$$  \n",
    "\n",
    "We can talk about accuracy of diﬀerent subgroups of examples:   \n",
    "- **Accuracy for examples** of class 0:  \n",
    "$$\\frac{a}{a+c}$$  \n",
    "- **Accuracy for examples predicted** to have class 0: a/(a+b).\n",
    "$$\\frac{a}{a + b}$$  \n",
    "\n",
    "## Issue with Accuracy and Error Rate  \n",
    "Consider a no-information classiﬁer that achieves the following   \n",
    "<div align=\"center\"><img src = \"./issue.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- Accuracy is 99.9% and error rate is .09%.  \n",
    "- Two lessons: \n",
    "  - Accuracy numbers meaningless without knowing the **no-information rate** or **base rate**.  \n",
    "  - Accuracy alone doesn’t capture what’s going on (0% success on class 1).\n",
    "\n",
    "## Positive and Negative Classes\n",
    "- In many contexts, it’s very natural to identify a **positive class** and a **negative class**  \n",
    "   - pregnancy test (positive = you’re pregnant)\n",
    "   - radar system (positive = threat detected)\n",
    "   - searching for documents about bitcoin (positive = document is about bitcoin)\n",
    "   - statistical hypothesis testing (positive = reject the null hypothesis)\n",
    "###  FP, FN, TP, TN\n",
    "- Let’s denote the positive class by + and negative class by −:\n",
    "<div align=\"center\"><img src = \"./T.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- **TP** is the number of true positives: predicted correctly as Class +.  \n",
    "- **FP** is the number of false positives: predicted incorrectly as Class + (i.e true class −)  \n",
    "- **TN** is the number of true negatives: predicted correctly as Class −.   \n",
    "- **FN** is the number of false negatives: predicted incorrectly as Class − (i.e. true class +)\n",
    "\n",
    "### Precision and Recall\n",
    "- The **precision** is the accuracy of the positive predictions  \n",
    "$$\\frac{TP}{TP + FP}$$  \n",
    "  - High precision means low “false alarm rate” (if you test positive, you’re probably positive)\n",
    "- The **recall** is the accuracy of the positive class  \n",
    "$$\\frac{TP}{TP + FN}$$\n",
    "  - High recall means you’re not missing many positives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "- Consider a database of 100,000 documents  \n",
    "- Query for bitcoin returns 200 documents  \n",
    "- 100 of them are actually about bitcoin  \n",
    "- 50 documents about bitcoin were not returned  \n",
    "\n",
    "<div align=\"center\"><img src = \"./bitcoin.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- The **precision** is the accuracy of the + predictions: TP / (TP + FP) = 100/200 = 50%.  \n",
    "  - 50% of the documents oﬀered as relevant are actually relevant  \n",
    "- The **recall** is the accuracy of the positive class: TP/(TP+FN) = 100/(100+50) = 67%.  \n",
    "  - 67% of the relevant documents were found (or “recalled”).   \n",
    "\n",
    "## $F_1$ Score  \n",
    "- We really want high precision **and** high recall  \n",
    "- But to choose a “best” model, we need a single number performance summary  \n",
    "- The **F-measure** or $F_1$ score is the harmonic mean of precision and recall:   \n",
    "$$F_{1}=2 \\cdot \\frac{1}{\\frac{1}{\\text { recall }}+\\frac{1}{\\text { precision }}}=2 \\cdot \\frac{\\text { precision } \\cdot \\text { recall }}{\\text { precision }+\\text { recal }}$$  \n",
    "\n",
    "## $F_\\beta$ Score  \n",
    "- $F_\\beta$ score for $\\beta > 0$:\n",
    "$$F_{\\beta}=\\left(1+\\beta^{2}\\right) \\cdot \\frac{\\text { precision } \\cdot \\text { recall }}{\\left(\\beta^{2} \\cdot \\text { precision }\\right)+\\text { recall }}$$  \n",
    "\n",
    "## TPR, FNR, FPR, TNR\n",
    "- **True positive rate** is the accuracy on the positive class: TP / (FN + TP) \n",
    "  - same as recall, also called **sensitivity**\n",
    "- **False negative rate** is the error rate on the positive class: FN / (FN + TP) (“miss rate”) \n",
    "- **False positive rate** is error rate on the negative class: FP / (FP + TN)  \n",
    "  - also called fall-out or false alarm rate   \n",
    "- **True negative rate** is accuracy on the negative class: TN / (FP + TN) (“speciﬁcity”)\n",
    "\n",
    "## Medical Diagnostic Test: Sensitivity and Speciﬁcity\n",
    "- **Sensitivity** is another name for TPR and recall  \n",
    "  - What fraction of people with disease do we identify as having disease?   \n",
    "  - How “sensitive” is our test to indicators of disease?   \n",
    "- **Speciﬁcity** is another name for TNR   \n",
    "  - What fraction of people without disease do we identify as being without disease?   \n",
    "  - High speciﬁcity means few false alarms  \n",
    "- In medical diagnosis, we want both sensitivity and speciﬁcity to be high.\n",
    "\n",
    "# Thresholding Classification Score Function  \n",
    "## The Classiﬁcation Problem\n",
    "- Action space $A =R$ Output space $Y = \\{−1,1\\}$  \n",
    "- **Real-valued** prediction function $f : X\\to  R$, called the score function. \n",
    "- Convention is:  \n",
    "$$\\begin{array}{l}\n",
    "f(x)>0 \\Longrightarrow \\text { Predict } 1 \\\\\n",
    "f(x)<0 \\Longrightarrow \\text { Predict }-1\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Scores, Predictions, and Labels\n",
    "<div align=\"center\"><img src = \"./result.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- Performance measures  \n",
    "  - Error Rate = 4/12≈.33 \n",
    "  - Precision =4/6≈.67 \n",
    "  - Recall =4/6≈.67 \n",
    "  - F1 = 4/6≈.67   \n",
    "- Now predict + iﬀ Score > 2? \n",
    "  - Error Rate = 2/12≈.17 \n",
    "  - Precision =4/4 = 1.0 \n",
    "  - Recall =4/6≈.67 \n",
    "  - F1 = 0.8   \n",
    "- Now predict + iﬀ Score> -1? \n",
    "  - Error Rate = 2/12≈.17 \n",
    "  - Precision =6/8 = .75 \n",
    "  - Recall =6/6 =1.0 \n",
    "  - F1 = 0.86   \n",
    "- Generally, diﬀerent thresholds on the score function lead to  \n",
    "  - diﬀerent confusion matrices \n",
    "  - diﬀerent performance metrics   \n",
    "\n",
    "# The performance Curve  \n",
    "##  Precision-Recall Curve\n",
    "\n",
    "<div align=\"center\"><img src = \"./precision recall curve.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "- Recall FPR and TPR: \n",
    "  - FPR = FP / (Number of Negatives Examples) \n",
    "  - TPR = TP / (Number of Positives Examples)  \n",
    "- As we decrease threshold from $+\\infty$ to $-\\infty$,  \n",
    "  - Number of positives predicted increases - some correct, some incorrect. \n",
    "  - So both FP and TP increase  \n",
    "- ROC Curve charts TPR vs FPR as we vary the threshold  \n",
    "<div align=\"center\"><img src = \"./roc.jpg\" width = '500' height = '100' align = center /></div>    \n",
    "\n",
    "## Comparing ROC Curves\n",
    "<div align=\"center\"><img src = \"./3roc.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Here we have ROC curves for 3 score functions \n",
    "- For diﬀerent FPRs, diﬀerent score functions give better TPRs  \n",
    "- No score function dominates another at every FPR  \n",
    "- Can we come up with an overall performance measure for a score function  \n",
    "\n",
    "## Area Under the ROC Curve\n",
    "- AUC ROC = area under the ROC curve \n",
    "- Often just referred to as “AUC  \n",
    "- A single number commonly used to summarize classiﬁer performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
