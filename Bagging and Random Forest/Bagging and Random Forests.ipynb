{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: Introduction  \n",
    "## Ensembles: Parallel vs Sequential\n",
    "- Ensemble methods combine multiple models  \n",
    "- Parallel ensembles: each model is built independently   \n",
    "  - e.g. bagging and random forests   \n",
    "  - Main Idea: Combine many (high complexity, low bias) models to reduce variance  \n",
    "- Sequential ensembles  \n",
    "  - Models are generated sequentially   \n",
    "  - Try to add new models that do well where previous models lack\n",
    "\n",
    "# The Benefits of Averaging  \n",
    "## A Poor Estimator\n",
    "- Let $Z,Z_1,...,Z_n$ i.i.d. $EZ = \\mu$ and $\\text{Var} Z = \\sigma^2$.  \n",
    "- We could use any single $Z_i$ to estimate $\\mu$.  \n",
    "- Unbiased: $EZ_i = \\mu$.  \n",
    "- Standard error of estimator would be $\\sigma$.  \n",
    "  - The standard error is the standard deviation of the sampling distribution of a statistic  \n",
    "  - $\\mathrm{SD}(Z)=\\sqrt{\\operatorname{Var}(Z)}=\\sqrt{\\sigma^{2}}=\\sigma$  \n",
    "  \n",
    "## Variance of a Mean\n",
    "Let’s consider the average of the $Z_i$’s.   \n",
    "- Average has the same expected value but smaller standard error:   \n",
    "$$\n",
    "\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right]=\\mu \\quad \\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right]=\\frac{\\sigma^{2}}{n}\n",
    "$$  \n",
    "- Clearly the average is preferred to a single $Z_i$ as estimator  \n",
    "- Can we apply this to reduce variance of general prediction functions?\n",
    "\n",
    "## Averaging Independent Prediction Functions\n",
    "- Suppose we have $B$ independent training sets from the same distribution  \n",
    "- Learning algorithm gives $B$ decision functions: $\\hat{f}_{1}(x), \\hat{f}_{2}(x), \\ldots, \\hat{f}_{B}(x)$  \n",
    "- Deﬁne the average prediction function as  \n",
    "$$\n",
    "\\hat{f}_{\\mathrm{avg}}=\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_{b}\n",
    "$$  \n",
    "- What’s random here?\n",
    "- Fix some $x \\in X$.  \n",
    "- Then average prediction on $x$ is  \n",
    "$$\n",
    "\\hat{f}_{\\text {avg }}(x)=\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_{b}(x)\n",
    "$$  \n",
    "- Consider $\\hat{f}_{\\text {avg }}(x)$ and $\\hat{f}_{1}(x), \\ldots, \\hat{f}_{B}(x)$ as random variables (since training data random).  \n",
    "- $\\hat{f}_{1}(x), \\ldots, \\hat{f}_{B}(x)$ are i.i.d.\n",
    "- $\\hat{f}_{\\text {avg }}(x)$ and $\\hat{f}_{b}(x)$ have the same expected value, but  \n",
    "- $\\hat{f}_{\\text {avg }}(x)$ haas smaller variance:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left(\\hat{f}_{\\text {avg }}(x)\\right) &=\\frac{1}{B^{2}} \\operatorname{Var}\\left(\\sum_{b=1}^{B} \\hat{f}_{b}(x)\\right) \\\\\n",
    "&=\\frac{1}{B} \\operatorname{Var}\\left(\\hat{f}_{1}(x)\\right)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- But in practice we don’t have $B$ independent training sets...(Since we have to divide our dataset into $B$ independent seperate sets)   \n",
    "- Instead, we can use the bootstrap....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging  \n",
    "- Draw $B$ bootstrap samples $D_1,...,D_B$ from original data $D$.  \n",
    "- Let $\\hat{f}_{1}, \\hat{f}_{2}, \\ldots, \\hat{f}_{B}$ be the decision function of each set  \n",
    "- The **bagged decision function** is a combination of these:\n",
    "$$\n",
    "\\hat{f}_{\\text {avg }}(x)=\\text { Combine }\\left(\\hat{f}_{1}(x), \\hat{f}_{2}(x), \\ldots, \\hat{f}_{B}(x)\\right)\n",
    "$$  \n",
    "- How might we combine  \n",
    "  - decision functions for regression?   \n",
    "  - binary class predictions? \n",
    "  - binary probability predictions? \n",
    "  - multiclass predictions?   \n",
    "  \n",
    "## Bagging for Regression\n",
    "- Draw B bootstrap samples $D^1,...,D^B$ from original data $D$.  \n",
    "- Let $\\hat{f}_{1}, \\hat{f}_{2}, \\ldots, \\hat{f}_{B}: x \\rightarrow \\mathbf{R}$ be the predict functions for each set  \n",
    "- Bagged prediction function is given as  \n",
    "$$\n",
    "\\hat{f}_{\\mathrm{bag}}(x)=\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_{b}(x)\n",
    "$$  \n",
    "- Empirically, $\\hat{f}_{bag}$ often performs similarly to what we’d get from training on $B$ independent samples: \n",
    "  - Same expectation value  \n",
    "  - Smaller variance  \n",
    "\n",
    "## Out-of-Bag Error Estimation\n",
    "- Each bagged predictor is trained on about 63% of the data.  \n",
    "- Remaining 37% are called out-of-bag (OOB) observations.  \n",
    "- For $i$th training point, let  \n",
    "$$\n",
    "S_{i}=\\left\\{b \\mid D^{b} \\text { does not contain ith point }\\right\\}\n",
    "$$  \n",
    "- For example, we can't find $i$th training point in $D^3, D^5, D^7$, then $S_i = \\{3,5,7\\}$\n",
    "- The OOB prediction on $x_i$ is  \n",
    "$$\n",
    "\\hat{f}_{\\mathrm{OOB}}\\left(x_{i}\\right)=\\frac{1}{\\left|S_{i}\\right|} \\sum_{b \\in S_{i}} \\hat{f}_{b}\\left(x_{i}\\right)\n",
    "$$   \n",
    "\n",
    "**Remark:**\n",
    "- The OOB error is a good estimate of the test error.  \n",
    "- OOB error is similar to cross validation error – both are computed on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classiﬁcation Trees\n",
    "- Input space $X =R^5$ and output space $Y = \\{−1,1\\}$.  \n",
    "- Sample size $N = 30$ (simulated data)  \n",
    "<div align=\"center\"><img src = \"./bagging.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## Comparing Classiﬁcation Combination Methods\n",
    "- Two ways to combine classiﬁcations: consensus class or average probabilities.\n",
    "<div align=\"center\"><img src = \"./number of sample.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## Terms “Bias” and “Variance” in Casual Usage (Warning! Confusion Zone!)\n",
    "- Restricting the hypothesis space $F$ “biases” the ﬁt  \n",
    "  - away from the best possible ﬁt of the training data, \n",
    "  - and towards a [usually] simpler model.  \n",
    "- Full, unpruned decision trees have very little bias.  \n",
    "- Pruning decision trees introduces a bias  \n",
    "- Variance describes how much the ﬁt changes across diﬀerent random training sets  \n",
    "- If diﬀerent random training sets give very similar ﬁts, then algorithm has high stability  \n",
    "- Decision trees are found to be high variance (i.e. not very stable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Wisdom on When Bagging Helps\n",
    "- Hope is that bagging reduces variance without making bias worse.  \n",
    "- General sentiment is that bagging helps most when \n",
    "  - Relatively unbiased base prediction functions \n",
    "  - High variance / low stability \n",
    "- Hard to ﬁnd clear and convincing theoretical results on this   \n",
    "- But following this intuition leads to improved ML methods, e.g. Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest  \n",
    "## Recall the Motivating Principal of Bagging\n",
    "Averaging $\\hat{f}_1,..., \\hat{f}_B$ reduces variance, if they’re based on i.i.d. samples from $P_{X×Y}$   \n",
    "- Bootstrap samples are   \n",
    "  - independent samples from the training set, but   \n",
    "  - are not indepedendent samples from $P_{X×Y}$  \n",
    "- This dependence limits the amount of variance reduction we can get.  \n",
    "- Would be nice to reduce the dependence between $\\hat{f}_i$’s...  \n",
    "\n",
    "## Main idea of random forests  \n",
    "Use **bagged decision trees**, but modify the tree-growing procedure to reduce the correlation between trees.\n",
    "- Key step in random forests  \n",
    "  - When constructing each tree node, restrict choice of splitting variable to a randomly chosen subset of features of size $m$.  \n",
    "  - Typically choose $m \\approx\\sqrt{p}$, where $p$ is the number of features  \n",
    "  - Can choose $m$ using cross validation  \n",
    "<div align=\"center\"><img src = \"./m.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "# Appendix  \n",
    "-  Let $Z,Z_1,...,Z_n$ i.i.d. $EZ = \\mu$ and $\\text{Var} Z = \\sigma^2$.  \n",
    "$$\n",
    "\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right]=\\mu \\quad \\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right]=\\frac{\\sigma^{2}}{n}\n",
    "$$  \n",
    "- What if Z’s are correlated?  \n",
    "- Suppose $\\forall i \\ne j$, $\\text{Corr}(Z_i,Z_j) = \\rho$ . Then   \n",
    "$$\n",
    "\\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\right]=\\rho \\sigma^{2}+\\frac{1-\\rho}{n} \\sigma^{2}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
