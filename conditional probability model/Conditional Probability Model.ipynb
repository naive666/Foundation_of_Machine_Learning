{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probabilistic Models vs GLMs  \n",
    "- Most books and software libraries related to this topic are actually about \n",
    "  - generalized linear models (GLMs). \n",
    "- They’re “special” because \n",
    "  - they’re a restriction of our setting, but more importantly \n",
    "  - we can state theorems for GLMs, and \n",
    "  - all GLMs can be implemented in essentially the same way. \n",
    "\n",
    "# Generalized Regression  \n",
    "- Given x, predict probability distribution $p(y | x) $\n",
    "- How do we represent the probability distribution?   \n",
    "- We’ll consider parametric families of distributions  \n",
    "  - distribution represented by parameter vector  \n",
    "- Examples:   \n",
    "  - Logistic regression (Bernoulli distribution) \n",
    "  - Probit regression (Bernoulli distribution) \n",
    "  - Poisson regression (Poisson distribution) \n",
    "  - Linear regression (Normal distribution, ﬁxed variance) \n",
    "  - Generalized Linear Models (GLM) (encompasses all of the above) \n",
    "  - Generalized Additive Models (GAM) \n",
    "  - Gradient Boosting Machines (GBM) / AnyBoost   \n",
    "  \n",
    "# Bernoulli Regression  \n",
    "## Probabilistic Binary Classiﬁers\n",
    "- Setting: $X =R^d$, $Y = \\{0,1\\}$  \n",
    "- For each $x$, need to predict a distribution on $Y = \\{0,1\\}$  \n",
    "- How can we deﬁne a distribution supported on $\\{0,1\\}$?  \n",
    "- Suﬃcient to specify the Bernoulli parameter $\\theta = p(y = 1)$.  \n",
    "- We can refer to this distribution as Bernoulli($\\theta$).  \n",
    "\n",
    "## Linear Probabilistic Classiﬁers  \n",
    "- Setting: $X =R^d$, $Y = \\{0,1\\}$  \n",
    "- Want prediction function to map each $x \\in R^d$ to the right $\\theta \\in [0,1]$  \n",
    "- We ﬁrst extract information from $x \\in R^d$ and summarize in a single number.  \n",
    "  - That number is analogous to the score in classiﬁcation.  \n",
    "- For a linear method, this extraction is done with a linear function  \n",
    "$$\\underbrace{x}_{\\in \\mathbf{R}^{d}} \\mapsto \\underbrace{w^{T} x}_{\\in \\mathbf{R}}$$\n",
    "- As usual, $x \\mapsto w^Tx$ will include aﬃne functions if we include a constant feature in $x$  \n",
    "- $w^Tx$ is called linear predictor  \n",
    "- Still need to map this to $[0,1]$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transfer Function\n",
    "Need a function to map the linear predictor in $R$ to $[0,1]$:  \n",
    "$$\\underbrace{x}_{\\in \\mathbf{R}^{d}} \\mapsto \\underbrace{w^{T} x}_{\\in \\mathbf{R}} \\mapsto \\underbrace{f\\left(w^{T} x\\right)}_{\\in[0,1]}=\\theta$$  \n",
    "where $f :R\\to [0,1]$. We’ll call $f$ the transfer function  \n",
    "So prediction function is $x \\mapsto f (w^Tx)$, which gives value for $\\theta = p(y = 1 | x)$.   \n",
    "### Terminology Alert  \n",
    "In generalized linear models (GLMs), if θ is the distribution mean, then f is called the response function or inverse link function. Transfer function is not standard terminology, but we’re avoiding the heavy set of deﬁnitions needed for a full development of GLMs, which is actually more restrictive than our current framework.\n",
    "\n",
    "## Transfer Functions for Bernoulli\n",
    "Two commonly used transfer functions to map from $w^Tx to \\theta$:  \n",
    "<div align=\"center\"><img src = \"./transfer.jpg\" width = '500' height = '100' align = center /></div>    \n",
    "- Logistic function $$f(\\eta)=\\frac{1}{1+e^{-\\eta}}$$  \n",
    "- Normal CDF  \n",
    "$$f(\\eta)=\\int_{-\\infty}^{\\eta} \\frac{1}{\\sqrt{2 \\pi}} e^{-x^{2} / 2}$$  \n",
    "\n",
    "## Learning  \n",
    "- $X =R^d$  \n",
    "- $Y = \\{0,1\\}$  \n",
    "- $A = [0,1]$ (Representing Bernoulli($\\theta$) distributions by $\\theta \\in [0,1]$   \n",
    "- $H =\\{x \\mapsto f (w^Tx) | w \\in R^d\\}$\t(Each prediction function represented by $w \\in R^d$.)   \n",
    "- We can choose w using maximum likelihood...\n",
    "\n",
    "## Bernoulli Regression: Likelihood Scoring\n",
    "Suppose we have data $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$.  \n",
    "Compute the model likelihood for $D$:  \n",
    "$$\\begin{aligned}\n",
    "p_{w}(\\mathcal{D}) &=\\prod_{i=1}^{n} p_{w}\\left(y_{i} \\mid x_{i}\\right)[\\text { by independence }] \\\\\n",
    "&=\\prod_{i=1}^{n}\\left[f\\left(w^{T} x_{i}\\right)\\right]^{y_{i}}\\left[1-f\\left(w^{T} x_{i}\\right)\\right]^{1-y_{i}}\n",
    "\\end{aligned}$$  \n",
    "Easier to work with the log-likelihood:\n",
    "$$\\log p_{w}(\\mathcal{D})=\\sum_{i=1}^{n} y_{i} \\log f\\left(w^{T} x_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left[1-f\\left(w^{T} x_{i}\\right)\\right]$$  \n",
    "\n",
    "## Bernoulli Regression: MLE\n",
    "Maximum Likelihood Estimation (MLE) ﬁnds $w$ maximizing $\\text{log}p_w(D)$.   \n",
    "Equivalently, minimize the negative log-likelihood objective function   \n",
    "$$J(w)=-\\left[\\sum_{i=1}^{n} y_{i} \\log f\\left(w^{T} x_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left[1-f\\left(w^{T} x_{i}\\right)\\right]\\right]$$  \n",
    "For diﬀerentiable $f$, $J(w)$ is diﬀerentiable, and we can use our standard tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Regression  \n",
    "## Setup  \n",
    "- Input space $X =R^d$, Output space $Y = \\{0,1,2,3,4,...\\}$  \n",
    "- In Poisson regression, prediction functions produce a **Poisson distribution**  \n",
    "  - Represent $\\text{Poisson}(\\lambda)$ distribution by the mean parameter $\\lambda \\in(0,\\infty)$.   \n",
    "- Action space $A = (0,\\infty)$  \n",
    "- In Poisson Regression, $x$ enters linearly, $$x \\mapsto \\underbrace{w^{T} x}_{R} \\mapsto \\lambda=\\underbrace{f\\left(w^{T} x\\right)}_{(0, \\infty)}$$  \n",
    "- What can we use as transfer function $f$  \n",
    "- Standard approach is to take $$f\\left(w^{T} x\\right)=\\exp \\left(w^{T} x\\right)$$  \n",
    "\n",
    "**Complementary**  \n",
    "Poisson Distribution:  \n",
    "If we want to find the distribution the number of occurrence per unit time, we can use **Poisson Distribution**  \n",
    "eg.  \n",
    "- The number of customers coming in a shopping store in 1 hour \n",
    "- The number of a cross intersection in 10 minutes   \n",
    "\n",
    "Now we want to find out the relationship between $X$ and $Y$, where $Y = \\{0,1,2,3,4,...\\}$ satisfies Poisson Distribution   \n",
    "We cannot simply use linear regression to predict $y$ based on $x$ since $Y$ is discrete while the result of the linear regression is continuous  \n",
    "One good idea is using $X$ to predict $\\lambda$, the parameter of Poission Distribution, after knowing $\\lambda$, we can get the distribution of $Y$  \n",
    "\n",
    "****  \n",
    "For any sample $(x_i,y_i)$  \n",
    "$$P(y_i) = \\frac{\\lambda_i ^{y_i}}{y_i!}e^{-\\lambda_i}$$  \n",
    "Calculating the likelihood  \n",
    "$$L = \\prod_{i = 1}^n \\frac{\\lambda_i^{y_i}}{y_i!}e^{-\\lambda_i}$$  \n",
    "Log-Likelihood  \n",
    "$$\\log L = \\sum_{i = 1}^n k_i\\log(\\lambda_i) - \\log(y_i!) - \\lambda_i$$  \n",
    "Let $\\lambda_i = f(w^Tx_i) = e^{w^Tx_i}$  \n",
    "then we get  \n",
    "$$\\log L(w) = \\sum_{i = 1}^n k_iw^Tx_i - \\log(y_i!) - e^{w^Tx_i}$$  \n",
    "Then we can use optimization method to solve it\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Gaussian Regression  \n",
    "## Gaussian Linear Reegression  \n",
    "- Input space $X =R^d$, Output space $Y =R$  \n",
    "- In Gaussian regression, prediction functions produce a distribution $N(\\mu, \\sigma^2)$  \n",
    "  - Assume $\\sigma^2$ is known  \n",
    "- Represent $N(\\mu,\\sigma^2)$ by the mean parameter $\\mu\\in R$.  \n",
    "- Action space $A = R$  \n",
    "- In Gaussian linear regression, $x$ enters **linearly**  \n",
    "$$x \\mapsto \\underbrace{w^{T} x}_{\\mathbf{R}} \\mapsto \\mu=\\underbrace{f\\left(w^{T} x\\right)}_{R}$$  \n",
    "- since $\\mu \\in R$ we can take the identity link function $f(w^Tx) = w^Tx$  \n",
    "\n",
    "## Gaussian Regression: Likelihood Scoring\n",
    "- Suppose we have data $D = \\{(x_1,y_1),...,(x_n,y_n)\\}$  \n",
    "- Compute the model likelihood for $D$:  \n",
    "$$\\begin{array}{c}\n",
    "\\sum_{i=1}^{n} \\log p_{w}\\left(y_{i} \\mid x_{i}\\right) \\\\\n",
    "=\\sum_{i=1}^{n} \\log \\left[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(y_{i}-w^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}\\right)\\right] \\\\\n",
    "=\\underbrace{\\sum_{i=1}^{n} \\log \\left[\\frac{1}{\\sigma \\sqrt{2 \\pi}}\\right]}_{\\text {independent of } w}+\\sum_{i=1}^{n}\\left(-\\frac{\\left(y_{i}-w^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}\\right)\n",
    "\\end{array}$$  \n",
    "- The MLE is\n",
    "$$w^{*}=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\sum_{i=1}^{n}\\left(y_{i}-w^{T} x_{i}\\right)^{2}$$  \n",
    "- This is exactly the objective function for least squares  \n",
    "- From here, can use usual approaches to solve for w∗ (SGD, linear algebra, calculus, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression  \n",
    "- Setting: $X =R^d$, $Y = \\{1,...,k\\}$  \n",
    "- For each $x$, we want to produce a distribution on $k$ classes.\n",
    "- Such a distribution is called a “multinoulli” or “categorical” distribution.  \n",
    "- Represent categorical distribution by probability vector $\\theta = (\\theta_1,...,\\theta_k) \\in R^k$  \n",
    "  - $\\sum_{i = 1}^k \\theta_i = 1$, and $\\theta_i \\geq 0$  \n",
    "- So $\\forall y \\in \\{1,2,...,k\\}$, $p(y) = \\theta_y$  \n",
    "- From each x, we compute a linear score function for each class\n",
    "$$x \\mapsto\\left(\\left\\langle w_{1}, x\\right\\rangle, \\ldots,\\left\\langle w_{k}, x\\right\\rangle\\right) \\in \\mathbf{R}^{k}$$  \n",
    "for parameter vectors $w_1,...,w_k \\in R^d$  \n",
    "We need to map this $R^k$ vector into probability vector  \n",
    "- Using softmax function  \n",
    "$$\\left(\\left\\langle w_{1}, x\\right\\rangle, \\ldots,\\left\\langle w_{k}, x\\right\\rangle\\right) \\mapsto \\theta=\\left(\\frac{\\exp \\left(w_{1}^{T} x\\right)}{\\sum_{i=1}^{k} \\exp \\left(w_{i}^{T} x\\right)}, \\cdots, \\frac{\\exp \\left(w_{k}^{T} x\\right)}{\\sum_{i=1}^{k} \\exp \\left(w_{i}^{T} x\\right)}\\right)$$  \n",
    "- Note that $\\theta \\in R^k$ and  \n",
    "$$\\begin{aligned}\n",
    "\\theta_{i} &>0 \\quad i=1, \\ldots, k \\\\\n",
    "\\sum_{i=1}^{k} \\theta_{i} &=1\n",
    "\\end{aligned}$$  \n",
    "Putting this together, we write multinomial logistic regression as\n",
    "$$p(y \\mid x)=\\frac{\\exp \\left(w_{y}^{T} x\\right)}{\\sum_{i=1}^{k} \\exp \\left(w_{i}^{T} x\\right)}$$  \n",
    "\n",
    "- Do we still see score functions in here?  \n",
    "- Can view $x \\mapsto w_y^Tx$ as score for class $y$, for $y \\in \\{1,...,k\\}$  \n",
    "- How do we do learning here? What parameters are we estimatimg?   \n",
    "- Our model is speciﬁed once we have $w_1,...,w_k \\in R^d$  \n",
    "- Find parameter settings maximizing the log-likelihood of data $D$.  \n",
    "- This objective function is concave in w’s and straightforward to optimize  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood as ERM  \n",
    "## Conditional Probability Modeling as Statistical Learning\n",
    "- Input space $X$  \n",
    "- Outcome space $Y$  \n",
    "- All pairs $(x,y)$ are independent with distribution $P_{X×Y}$.  \n",
    "- Action space $A = \\{p(y) | p \\qquad \\text{  is a probability density or mass function on } Y\\}$.  \n",
    "- Hypothesis space $F$ contains prediction functions $f : X\\to A$.  \n",
    "  - Given an $x \\in X$, predict a probability distribution $p(y)$ on $Y$.  \n",
    "- Maximum likelihood estimation for dataset $D=((x_1,y_1),...,(x_n,y_n))$  \n",
    "$$\\hat{f}_{\\mathrm{MLE}}=\\underset{f \\in \\mathcal{H}}{\\arg \\max } \\sum_{i=1}^{n} \\log \\left[f\\left(x_{i}\\right)\\left(y_{i}\\right)\\right]$$  \n",
    "## Conditional Probability Modeling as Statistical Learning  \n",
    "- Take loss $l : A×Y\\to R$ for a predicted PDF or PMF $p(y)$ and outcome $y$ to be  \n",
    "$$\\ell(p, y)=-\\log p(y)$$  \n",
    "The risk of decision function $f : X\\to A$ is  \n",
    "$$R(f)=-\\mathbb{E}_{x, y} \\log [f(x)(y)]$$  \n",
    "where $f(x)$ is a PDF or PMF on $Y$, and we’re evaluating it on $y$.\n",
    "\n",
    "The empirical risk of $f$ for a sample $D = \\{y_1,...,y_n\\}\\in Y$ is  \n",
    "$$\\hat{R}(f)=-\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left[f\\left(x_{i}\\right)\\right]\\left(y_{i}\\right)$$  \n",
    "This is called the **negative conditional log-likelihood**.  \n",
    "Thus for the **negative log-likelihood loss**, ERM and MLE are equivalent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
