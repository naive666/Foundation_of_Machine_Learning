{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Dependent Feature\n",
    "- Suppose the features are not mutually independent:  \n",
    "  - for example: if we have two features $\\mathcal{x_1}$ and $\\mathcal{x_2}$ with $\\mathcal{x_1} = 3\\mathcal{x_2}$, and our decision function is $f = \\mathcal{x_1} + \\mathcal{x_2}$, which now becomes $f = 4\\mathcal{x_2}$.\n",
    "  - Question: What if we introduce $l_1$ and $l_2$ regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplest Case\n",
    "- Input features: $x_1, x_2 \\in R$\n",
    "- Outcome $y \\in R$\n",
    "- Linear prediction functions: $f(x) = w_1x_1 = w_2x_2$\n",
    "- Suppose $x_1 = x_2$\n",
    "- Then all functions with $w_1 + w_2 = k$ are the same\n",
    "### Example\n",
    "$l_2$ regularization with $\\lVert w \\rVert_2 \\leq 2$, then the intersection $w_1 + w_2 = 2\\sqrt2$ is the solution, and with $l_1$ constraints $\\lVert w \\rVert_2 \\leq 1$, the solution becomes $w_1 + w_2 = 2$.  \n",
    "<div align=\"center\"><img src = \"./l2.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "<div align=\"center\"><img src = \"./l1.jpg\" width = '500' height = '100' align = center /></div>\n",
    "\n",
    "  \n",
    "## Linear Related Features\n",
    "Suppose $x_2 = 2x_1$, with the same constrains as the above example  \n",
    "<div align=\"center\"><img src = \"./l2_2.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "<div align=\"center\"><img src = \"./l1_2.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Dependent Features\n",
    "- Identical features\n",
    " - $l_1$ regularization spreads the weights arbitrarily\n",
    " - $l_2$ regularization spreads the weights evenly\n",
    "- Linearly related features\n",
    " - $l_1$ regularization chooses variable with larger scale, 0 weight to others\n",
    " - $l_2$ prefers variables with larger scale – spreads weight proportional to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Risk for Square Loss and Linear Predictors\n",
    "- Sets of $w$ giving same empirical risk (i.e. level sets) formed ellipsoids around the ERM.\n",
    "- With $x_1$ and $x_2$ linearly related, we get a degenerate ellipse. \n",
    "<div align=\"center\"><img src = \"./empirical_risk.jpg\" width = '500' height = '100' align = center /></div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Features,  $l_1$ Regularization\n",
    "- Intersection could be anywhere on the top right edge. \n",
    "- Minor perturbations (in data) can drastically change intersection point – very **unstable** solution. \n",
    "- Makes division of weight among highly correlated features (of same scale) seem arbitrary.  \n",
    "<div align=\"center\"><img src = \"./unstable.jpg\" width = '500' height = '100' align = center /></div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlated Features and the Grouping Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with highly correlated features\n",
    "- Suppose $y$ is a linear combination of $z_1$ and $z_2$  \n",
    "- We don't observe $z_1$ and $z_2$ directly, but we have 3 noisy observations\n",
    "- We want to predict $y$ based on the noisy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $x, y$ generated as follow:  \n",
    "$$\\begin{aligned} z_{1}, z_{2} & \\sim \\mathcal{N}(0,1) \\text { (independent) } \\\\ \\varepsilon_{0}, \\varepsilon_{1}, \\ldots, \\varepsilon_{6} & \\sim \\mathcal{N}(0,1) \\text { (independent) } \\\\ y &=3 z_{1}-1.5 z_{2}+2 \\varepsilon_{0} \\\\ x_{j} &=\\left\\{\\begin{array}{ll}z_{1}+\\varepsilon_{j} / 5 & \\text { for } j=1,2,3 \\\\ z_{2}+\\varepsilon_{j} / 5 & \\text { for } j=4,5,6\\end{array}\\right.\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated a sample of $(x,y)$ pairs of size 100  \n",
    "Correlations within the groups of $x$’s were around 0.97  \n",
    "- Lasso regularization path  \n",
    "<div align=\"center\"><img src = \"./lasso path.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "Lines with the same color correspond to features with essentially the same information  \n",
    "As we can see, Distribution of weight among them seems almost arbitrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedge Bets When Variables Highly Correlated  \n",
    "When variables are highly correlated (and same scale, after normalization)  \n",
    "- we want to give them roughly the same weight, because we want their errors cancel out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net\n",
    "The elastic net combines lasso and ridge penalties:  \n",
    "$$\\hat{w}=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left\\{w^{T} x_{i}-y_{i}\\right\\}^{2}+\\lambda_{1}\\|w\\|_{1}+\\lambda_{2}\\|w\\|_{2}^{2}$$  \n",
    "We expect correlated random variables to have similar coefficients  \n",
    "<div align=\"center\"><img src = \"./elastic net.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "Elastic net solution is closer to $w_2 = w_1$ line, despite high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net - “Sparse Regions”\n",
    "<div align=\"center\"><img src = \"./sparse region.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "Suppose design matrix $X$ is orthogonal, so $X^T X = I$, and contours are circles (and features uncorrelated)  \n",
    "\n",
    "Then OLS solution in green or red regions implies elastic-net constrained solution will be\n",
    "at corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Results on Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src = \"./lassoVSElas.jpg\" width = '500' height = '100' align = center /></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for Correlated Features in Elastic Net  \n",
    "Recall the elastic net objective function:  \n",
    "$$J(w)=\\frac{1}{n}\\|X w-y\\|_{2}^{2}+\\lambda_{1}\\|w\\|_{1}+\\lambda_{2}\\|w\\|_{2}^{2}$$  \n",
    "Let's write $x_i$ as the $i$ th column of the design matrix $X$  \n",
    "- here $x_i \\in R^n$ is the $i$th feature, across all training data  \n",
    "- As we often do in practice, let’s assume the data are standardized so that every column $x_i$ has mean 0, and standard deviation 1  \n",
    "- Then we denote the correlation between any pairs of columns $x_i$ and $x_j$ as $\\rho_{i j}=\\frac{1}{n} x_{i}^{T} x_{j}$\n",
    "## Theorem1  \n",
    "Under the conditions described above, if $\\hat{w}_{i} \\hat{w}_{j}>0$, then  \n",
    "$$\\left|\\hat{w}_{i}-\\hat{w}_{j}\\right| \\leq \\frac{\\|y\\|_{2} \\sqrt{2}}{\\sqrt{n} \\lambda_{2}} \\sqrt{1-\\rho_{i j}}$$  \n",
    "**Proof**  \n",
    "By assumption, $\\hat{w}_i$ and $\\hat{w}_j$ are nonzero, and moreover we must have $\\frac{\\partial J}{\\partial w_{i}}(\\hat{w})=\\frac{\\partial J}{\\partial w_{j}}(\\hat{w})=0$, that is  \n",
    "$$\\frac{\\partial J}{\\partial w_{i}}(\\hat{w})=\\frac{2}{n}(X \\hat{w}-y)^{T} x_{i}+\\lambda_{1} \\operatorname{sign}\\left(\\hat{w}_{i}\\right)+2 \\lambda_{2} \\hat{w}_{i}=0$$  \n",
    "and  \n",
    "$$\\frac{\\partial J}{\\partial w_{j}}(\\hat{w})=\\frac{2}{n}(X \\hat{w}-y)^{T} x_{j}+\\lambda_{1} \\operatorname{sign}\\left(\\hat{w}_{j}\\right)+2 \\lambda_{2} \\hat{w}_{j}=0$$  \n",
    "substraction, we get   \n",
    "$$\\begin{aligned}\n",
    "\\frac{2}{n}(X \\hat{w}-y)^{T}\\left(x_{j}-x_{i}\\right)+2 \\lambda_{2}\\left(\\hat{w}_{j}-\\hat{w}_{i}\\right) &=0 \\\\\n",
    "\\Longleftrightarrow\\left(\\hat{w}_{i}-\\hat{w}_{j}\\right) &=\\frac{1}{n \\lambda_{2}}(X \\hat{w}-y)^{T}\\left(x_{j}-x_{i}\\right)\n",
    "\\end{aligned}$$  \n",
    "Since $\\hat{w}$ is a minimizer of $J$, we must have $J(\\hat{w}) \\leq J(0)$, that is  \n",
    "$$\\frac{1}{n}\\|X w-y\\|_{2}^{2}+\\lambda_{1}\\|\\hat{w}\\|_{1}+\\lambda_{2}\\|\\hat{w}\\|_{2}^{2} \\leq \\frac{1}{n}\\|y\\|_{2}^{2}$$  \n",
    "Since the regularization terms are nonnegative, we must have $\\|X w-y\\|_{2}^{2} \\leq\\|y\\|_{2}^{2}$,  \n",
    "Meanwhile,  \n",
    "$$\\left\\|x_{j}-x_{i}\\right\\|_{2}^{2}=x_{j}^{T} x_{j}+x_{i}^{T} x_{i}-2 x_{j}^{T} x_{i}$$  \n",
    "then we have  \n",
    "$$\\left\\|x_{j}-x_{i}\\right\\|_{2}^{2}=2 n-2 n \\rho_{i j}$$  \n",
    "since  \n",
    "$$1^{T} x_{i}=1^{T} x_{j}=0$$  \n",
    "and  \n",
    "$$ \\frac{1}{n}x_i^Tx_i = \\frac{1}{n}x_j^Tx_j = 1$$  \n",
    "and the corelation between $x_i$ and $x_j$ is $\\rho_{i j}=\\frac{1}{n} x_{i}^{T} x_{j}$  \n",
    "$$\\left\\|x_{j}-x_{i}\\right\\|_{2}^{2}=2 n-2 n \\rho_{i j}$$  \n",
    "Putting things together  \n",
    "$$\\begin{aligned}\n",
    "\\left|\\hat{w}_{i}-\\hat{w}_{j}\\right| &=\\frac{1}{n \\lambda_{2}}\\left|(X \\hat{w}-y)^{T}\\left(x_{j}-x_{i}\\right)\\right| \\\\\n",
    "& \\leq \\frac{1}{n \\lambda_{2}}\\|X \\hat{w}-y\\|_{2}\\left\\|x_{j}-x_{i}\\right\\|_{2} \\text { by Cauchy-Schwarz inequality } \\\\\n",
    "& \\leq \\frac{1}{n \\lambda_{2}}\\|y\\|_{2} \\sqrt{2 n\\left(1-\\rho_{i j}\\right)} \\\\\n",
    "&=\\frac{1}{\\sqrt{n}} \\frac{\\sqrt{2}\\|y\\|_{2}}{\\lambda_{2}} \\sqrt{1-\\rho_{i j}}\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
