{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scores to Conditional Probabilities  \n",
    "1.  \n",
    "$$\n",
    "E_y[l(\\hat{y}y) | x] = P(y = 1 | x) l(f(x)) + P(y = -1 | x)l(-f(x)) = \\pi(x)l(f(x)) + (1-\\pi(x))l(-f(x))\n",
    "$$ \n",
    "\n",
    "2.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_y[l(\\hat{y}y) | x] &= \\pi(x)e^{-f(x)} + (1 - \\pi(x))e^{f(x)}  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "differentiate it w.r.t $\\hat{y} = f(x)$  \n",
    "$$\n",
    "\\frac{dE_y[l(\\hat{y}y) | x]}{d\\hat{y}} = -\\pi(x)e^{-f(x)} + (1 - \\pi(x))e^{f(x)} = 0\n",
    "$$  \n",
    "then we get  \n",
    "$$\n",
    "-\\pi(x) + (1- \\pi(x))e^{2f(x)} = 0\n",
    "$$  \n",
    "and  \n",
    "$$\n",
    "f^*(x) = \\frac{1}{2}\\ln\\frac{\\pi(x)}{1 - \\pi(x)}\n",
    "$$\n",
    "\n",
    "3.  \n",
    "$$\n",
    "E_y[l(\\hat{y}y) | x] = \\pi(x)\\ln(1 + e^{-f(x)}) + (1 - \\pi(x))\\ln(1 + e^{f(x)})\n",
    "$$  \n",
    "differentiation  \n",
    "$$\n",
    "\\frac{dE_y[l(\\hat{y}y) | x]}{d\\hat{y}} = -\\pi(x)\\frac{-e^{-f(x)}}{1 + e^{-f(x)}} + (1- \\pi(x))\\frac{e^{f(x)}}{1 + e^{f(x)}}=0\n",
    "$$  \n",
    "then we get  \n",
    "\n",
    "$$\n",
    "f^*(x) = \\ln\\frac{\\pi(x)}{1-\\pi(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic Regression  \n",
    "##  Equivalence of ERM and probabilistic approaches  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{NLL}(w) &=-\\sum_{i=1}^{n} y_{i}^{\\prime} \\log \\phi\\left(w^{T} x_{i}\\right)+\\left(1-y_{i}^{\\prime}\\right) \\log \\left(1-\\phi\\left(w^{T} x_{i}\\right)\\right) \\\\\n",
    "&=\\sum_{i=1}^{n}\\left[-y_{i}^{\\prime} \\log \\phi\\left(w^{T} x_{i}\\right)\\right]+\\left(y_{i}^{\\prime}-1\\right) \\log \\left(1-\\phi\\left(w^{T} x_{i}\\right)\\right) \\\\ \n",
    "&= \\sum_{i = 1}^n -y_i'\\left(-\\log \\left(1 + e^{w^Tx_i}\\right)\\right) + (y_i' -1)\\left(-w^Tx_i -\\log \\left(1 + e^{-w^Tx_i} \\right) \\right) \\\\\n",
    "&= \\sum_{i = 1}^n w^Tx_i(1- y_i') + \\log \\left(1 + e^{-w^Tx_i} \\right)\n",
    "\\end{aligned}\n",
    "$$   \n",
    "\n",
    "if $y_i = 1$, then $y_i' = 1$  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "n\\hat{R}_n(w) &= \\sum_{i = 1}^n\\log \\left(1 + e^{-w^Tx_i} \\right) \\\\\n",
    "&= \\mathrm{NLL}(w)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "else if $y_i = -1$, $y_i' = 0$  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n\\hat{R}_n(w) &= \\sum_{i = 1}^n\\log \\left(1 + e^{w^Tx_i} \\right) \\\\\n",
    "&= \\sum_{i = 1}^n \\log e^{w^Tx_i} + \\log \\left(1 + e^{-w^Tx_i} \\right) \\\\ \n",
    "&= \\sum_{i = 1}^n \\log \\left(1 + e^{w^Tx_i} \\right) = \\mathrm{NLL}(w)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regularized Logistic Regression   \n",
    "1. Log-Sum-Exp is convex, norm is convex, the sum of convex is convex  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:47:25.784712Z",
     "start_time": "2020-10-20T05:47:25.081068Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.optimize import minimize\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:47:26.321730Z",
     "start_time": "2020-10-20T05:47:26.313758Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_objective(theta, X, y, l2_param=1):\n",
    "    '''\n",
    "    Args:\n",
    "        theta: 1D numpy array of size num_features\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        l2_param: regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        objective: scalar value of objective function\n",
    "    '''\n",
    "    num_instance, num_feature = X.shape\n",
    "    J = 0\n",
    "    for i in range(num_instance):\n",
    "        J += 1/(num_instance) * (np.logaddexp(0, -y[i] * np.dot(theta, X[i])))\n",
    "    J_logistic = J + l2_param *  np.linalg.norm(theta)\n",
    "    return J_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:47:39.678150Z",
     "start_time": "2020-10-20T05:47:39.672172Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_logistic_reg(X, y, objective_function, l2_param = 1):\n",
    "    '''\n",
    "    Args:\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        objective_function: function returning the value of the objective\n",
    "        l2_param: regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        optimal_theta: 1D numpy array of size num_features\n",
    "    '''\n",
    "    partial_objective = partial(objective_function, X = X, y = y, l2_param = l2_param)\n",
    "    num_instance, num_feature = X.shape\n",
    "    initial_theta = np.random.randn(num_feature)\n",
    "    optimal_theta = minimize(partial_objective, initial_theta, method = 'Nelder-Mead')\n",
    "    return optimal_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:48:37.945445Z",
     "start_time": "2020-10-20T05:48:37.871622Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_init = np.loadtxt('X_train.txt', delimiter = ',')\n",
    "X_val_init = np.loadtxt('X_val.txt', delimiter = ',')\n",
    "y_train_init = np.loadtxt('y_train.txt', delimiter = ',')\n",
    "y_val_int = np.loadtxt('y_val.txt', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:48:51.584516Z",
     "start_time": "2020-10-20T05:48:49.951577Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train = MinMaxScaler().fit_transform(X_train_init)\n",
    "X_val = MinMaxScaler().fit_transform(X_val_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:49:02.666538Z",
     "start_time": "2020-10-20T05:49:02.655611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41716799, 0.60399767, 0.36840068, ..., 0.63191707, 0.23994726,\n",
       "        0.42968297],\n",
       "       [0.56673526, 0.87792192, 0.57637651, ..., 0.60527623, 0.53393452,\n",
       "        0.43512694],\n",
       "       [0.27095087, 0.49513585, 0.65234197, ..., 0.41306849, 0.60742099,\n",
       "        0.40874664],\n",
       "       ...,\n",
       "       [0.46845955, 0.60300687, 0.57895224, ..., 0.5880624 , 0.50323627,\n",
       "        0.42324938],\n",
       "       [0.56176742, 0.74867496, 0.6004432 , ..., 0.62166153, 0.35907216,\n",
       "        0.33692692],\n",
       "       [0.18960615, 0.66065241, 0.85321182, ..., 0.62622847, 0.51578704,\n",
       "        0.48375912]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:49:34.288840Z",
     "start_time": "2020-10-20T05:49:13.246069Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_theta = fit_logistic_reg(X_train, y_train_init, f_objective).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:49:44.337060Z",
     "start_time": "2020-10-20T05:49:44.331079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03362034,  0.373663  ,  0.11982482, -0.03353394, -0.23651557,\n",
       "       -0.06450096,  0.04983136,  0.04466461,  0.10927934,  0.07377231,\n",
       "        0.06958334, -0.37405481,  0.28128228,  0.1423054 ,  0.13799391,\n",
       "       -0.33843753,  0.04633243,  0.12222491,  0.05834767,  0.11109797])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:49:56.252309Z",
     "start_time": "2020-10-20T05:49:56.248265Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = X_val.dot(opt_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:50:05.441646Z",
     "start_time": "2020-10-20T05:50:05.437696Z"
    }
   },
   "outputs": [],
   "source": [
    "new_prediction = prediction - 1\n",
    "new_prediction[new_prediction >= 0.751] = 1\n",
    "new_prediction[new_prediction < 0.751] =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression - Implementation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [problem.py](./problem.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Coin Flipping: Maximum Likelihood  \n",
    "1.  \n",
    "$$\n",
    "P(D|\\theta) = \\theta^2(1-\\theta)\n",
    "$$  \n",
    "\n",
    "2. we have\n",
    "$$\n",
    "(T,H,H),(H,T,H),(H,H,T)\n",
    "$$  \n",
    "$$\n",
    "C_3^2\\theta^2(1-\\theta)\n",
    "$$  \n",
    "\n",
    "3.  \n",
    "$$\n",
    "P(D|\\theta) = \\theta^{n_h}(1-\\theta)^{n_t}\n",
    "$$  \n",
    "\n",
    "4.Differentiate it w.r.t $\\theta$,  \n",
    "$$\n",
    "\\frac{dP}{d\\theta} = n_h\\theta^{n_h - 1}(1-\\theta)^{n_t}-n_t\\theta^{n_h}(1-\\theta)^{n_t-1} = 0\n",
    "$$  \n",
    "thus  \n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\frac{n_h}{n_h+n_t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Coin Flipping: Bayesian Approach with Beta Prior   \n",
    "1.  \n",
    "$$\n",
    "P(\\theta \\mid \\mathcal{D}) \\propto P(\\theta)P(x_1,...,x_n|\\theta) = p(\\theta) \\prod_{i=1}^n p\\left(x_{i} \\mid \\theta\\right) = \\theta^{h-1+n_{h}}(1-\\theta)^{t-1+n_{t}}\n",
    "$$  \n",
    "\n",
    "2.  \n",
    "$$\n",
    "\\theta_{\\mathrm{MLE}}=\\frac{n_{h}}{n_{h}+n_{t}}\n",
    "$$  \n",
    "The MAP estimation $\\theta_{MAP}$ should be the mode of posterior distribution:  \n",
    "$$\n",
    "\\hat{\\theta}_{\\mathrm{MAP}}=\\frac{n_{h}+h-1}{n_{h}+h+n_{t}+t-2}\n",
    "$$  \n",
    "The posterior mean of $\\theta$ is:  \n",
    "$$\n",
    "\\hat{\\theta}_{\\text {POSTERIOR MEAN }}=\\frac{n_{h}+h}{n_{h}+h+n_{t}+t}\n",
    "$$  \n",
    "\n",
    "3.  \n",
    "When $n$ approaches inﬁnity, the eﬀect of prior on posterior is negligible. Therefore we expect $\\theta_{MAP}$, and $\\hat{\\theta}_{\\text {POSTERIOR MEAN }}$ to converges to $\\theta$  \n",
    "\n",
    "4.  \n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{MLE}}\\right]=\\mathbb{E}\\left[\\frac{n_{h}}{n}\\right]=\\frac{1}{n} \\mathbb{E}\\left[n_{h}\\right]=\\frac{n \\theta}{n}=\\theta\n",
    "$$  \n",
    "so MLE is unbiased estimator  \n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{MAP}}\\right]=\\mathbb{E}\\left[\\frac{n_{h}+h-1}{n+h+t-2}\\right]\n",
    "$$  \n",
    "MAP unbiased if $h = t = 1$,   \n",
    "$$\n",
    "\\mathbb{E}\\left[\\hat{\\theta}_{\\text {POSTERIOR MEAN }}\\right]=\\mathbb{E}\\left[\\frac{n_{h}+h}{n+h+t}\\right]\n",
    "$$\n",
    "posterior mean is unbiased if $h = t = 0$  \n",
    "\n",
    "5.  \n",
    "Posterior Mean, since 3 times is very unstable if we choose MLE. I will choose $\\text{Beta}(2,2)$ as my prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hierarchical Bayes for Click-Through Rate Estimation  \n",
    "##  \n",
    "1.  \n",
    "$$\n",
    "P(D_i | \\theta_i) = \\theta_i^{x_i}(1-\\theta_i)^{n_i - x_i}\n",
    "$$  \n",
    "2. Since if $\\theta_i$ follows $\\text{Beta}(a,b)$, then $p(\\theta_i) \\propto \\theta_i^{a-1}(1-\\theta_i)^{b-1}$, then by the definition, $$\n",
    "\\int \\theta_{i}^{a-1}\\left(1-\\theta_{i}\\right)^{b-1} d \\theta_{i}=B(a, b)\n",
    "$$  \n",
    "3.  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(\\theta_{i} \\mid \\mathcal{D}_{i}\\right) & \\propto p\\left(\\mathcal{D} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right) \\\\\n",
    "&=\\frac{\\theta_{i}^{x_{i}+a-1}\\left(1-\\theta_{i}\\right)^{n-x_{i}+b-1}}{B(a, b)} \\\\\n",
    "& \\propto \\theta_{i}^{x_{i}+a-1}\\left(1-\\theta_{i}\\right)^{n-x_{i}+b-1}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "and we know that $\\int p(\\theta_i|D_i)d\\theta_i = 1$, combine with the conclusion before, we must have  \n",
    "$$\n",
    "p\\left(\\theta_{i} \\mid \\mathcal{D}_{i}\\right)=\\frac{\\theta_{i}^{x_{i}+a-1}\\left(1-\\theta_{i}\\right)^{n-x_{i}+b-1}}{B\\left(x_{i}+a, n-x_ i+b\\right)}\n",
    "$$  \n",
    "4. \n",
    "  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(\\mathcal{D}_{i}\\right) &=\\int_{0}^{1} p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right) d \\theta_{i} \\\\\n",
    "&=\\frac{1}{B(a, b)} \\int_{0}^{1} \\theta^{x_{i}+a-1}(1-\\theta)^{n_{i}-x_{i}+b-1} \\\\\n",
    "&=\\frac{B\\left(x_{i}+a, n_{i}-x_{i}+b\\right)}{B(a, b)}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "5. It may help to think about the integral $p\\left(\\mathcal{D}_{i}\\right)=\\int p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right) d \\theta_{i}$ as a weighted average of $p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right)$, where the weights are $p(\\theta_i)$. By definition of MLE, we must have $p\\left(\\mathcal{D}_{i} \\mid \\theta\\right) \\leq p\\left(\\mathcal{D}_{i} \\mid \\theta_{\\mathrm{MLE}}\\right)$, then  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(\\mathcal{D}_{i}\\right) &=\\int p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right) d \\theta \\\\\n",
    "& \\leq \\int p\\left(\\mathcal{D}_{i} \\mid \\theta_{\\mathrm{MLE}}\\right) p\\left(\\theta_{i}\\right) d \\theta \\\\\n",
    "&=p\\left(\\mathcal{D}_{i} \\mid \\theta_{\\mathrm{MLE}}\\right) \\int p\\left(\\theta_{i}\\right) d \\theta \\\\\n",
    "&=p\\left(\\mathcal{D}_{i} \\mid \\theta_{\\mathrm{MLE}}\\right)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "6.  \n",
    "If we keep increase the likelihood, the eﬀect of prior on posterior will increase and ﬁnally dominate the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Bayes Using All App Data  \n",
    "\n",
    "1. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathcal{D} \\mid a, b) &=\\prod_{i=1}^{d} p\\left(\\mathcal{D}_{i} \\mid a, b\\right) \\\\\n",
    "&=\\prod_{i=1}^{d} \\frac{B\\left(x_{i}+a, n_{i}-x_{i}+b\\right)}{B(a, b)}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "2.  \n",
    "\n",
    "$$\n",
    "p\\left(\\theta_{i} \\mid \\mathcal{D}\\right)=\\frac{p\\left(\\mathcal{D} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right)}{p(\\mathcal{D})}=\\frac{p\\left(\\mathcal{D} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right)}{\\prod_{k=1}^{p} p\\left(\\mathcal{D}_{k}\\right)}\n",
    "$$  \n",
    "we learn that only $p(D_i|\\theta_i)$ is influenced by $\\theta_i$, thus  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(\\theta_{i} \\mid \\mathcal{D}\\right) &=\\frac{p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right) \\prod_{k \\neq i} p\\left(\\mathcal{D}_{k}\\right)}{\\prod_{k \\neq i} p\\left(\\mathcal{D}_{k}\\right) p\\left(\\mathcal{D}_{i}\\right)} \\\\\n",
    "&=\\frac{p\\left(\\mathcal{D}_{i} \\mid \\theta_{i}\\right) p\\left(\\theta_{i}\\right)}{p\\left(\\mathcal{D}_{i}\\right)} \\\\\n",
    "&=p\\left(\\theta_{i} \\mid \\mathcal{D}_{i}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
