{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convexity Concept Check Problems  \n",
    "3. First we prove: Suppose $f : R^n \\to R$ is twice differentiable over an open domain, then $f$ is convex $\\Rightarrow$ $f(x) \\geqslant f(y) + \\nabla f(y)^T(x - y)$  \n",
    "**proof**  \n",
    "$$\\begin{align}  \n",
    "f(\\theta + (1-\\theta)y) &\\leqslant \\theta f(x) + (1 - \\theta)f(y) \\\\   \n",
    "f(y + \\theta (x - y)) &\\leqslant \\theta (f(x) - f(y)) + f(y) \\\\  \n",
    "\\frac{f(y + \\theta (x - y)) - f(y)}{\\theta} &\\leqslant f(x) - f(y) \\\\  \n",
    "\\frac{f(y + \\theta (x - y)) - f(y)}{\\theta (x - y)}(x - y) &\\leqslant f(x) - f(y)\\\\  \n",
    "\\nabla f(y)^T(x-y) &\\leqslant f(x) - f(y)  \n",
    "\\end{align}$$  \n",
    "if we take $\\theta \\to 0$  \n",
    "\n",
    "Then we can prove question (3)  \n",
    "**proof**  \n",
    "if $\\nabla f(x) = 0 $, then $f(y) - f(x) \\geqslant 0$ for all $y \\in R^n$, which means $x$ is global minimum  \n",
    "  \n",
    "5. let $f(x) = a^Tx + b$, where $a \\in R^n, b \\in R$  \n",
    "$$\\begin{aligned}\n",
    "\\forall \\lambda \\in[0,1], f(\\lambda x+(1-\\lambda) y) &=a^{T}(\\lambda x+(1-\\lambda) y)+b \\\\\n",
    "&=\\lambda a^{T} x+(1-\\lambda) a^{T} y+\\lambda b+(1-\\lambda) b \\\\\n",
    "&=\\lambda f(x)+(1-\\lambda) f(y)\n",
    "\\end{aligned}$$  \n",
    "and indeed $-f$ is convex as well, which indicates $f$ is concave \n",
    "  \n",
    "7. (**)  \n",
    "(a) We first prove: if $f$ is convex, then $\\frac{f\\left(x_{0}+\\epsilon\\right)-f\\left(x_{0}\\right)}{\\epsilon}$ is nondecresing.  \n",
    "we can write: $f\\left(x_{0}+\\epsilon\\right)=f\\left(\\frac{\\epsilon}{t}\\left(x_{0}+t\\right)+\\left(1-\\frac{\\epsilon}{t}\\right) x_{0}\\right)$, then we have for $t > \\epsilon >0$ \n",
    "$$\n",
    "\\frac{f\\left(x_{0}+\\epsilon\\right)-f\\left(x_{0}\\right)}{\\epsilon} \\leq \\frac{\\frac{\\epsilon}{t} f(x_0 + t) + (1 - \\frac{\\epsilon}{t})f(x_0) - f(x_0)}{\\epsilon}\\\\  \n",
    "= \\frac{f\\left(x_{0}+t\\right)-f\\left(x_{0}\\right)}{t}\n",
    "$$  \n",
    "then we have   \n",
    "$$f_{+}^{\\prime}(a)=\\lim _{h \\rightarrow 0^{+}} \\frac{f(a+h)-f(a)}{h}=\\inf \\left\\{\\frac{f(a+h)-f(a)}{h}: h>0\\right\\}$$  \n",
    "this inf exists because each quotient $\\frac{f(a+h)-f(a)}{h}$ is greater than any one such quotient for $h' < 0$ (which means it has lower bound, then the inf exists). Now we prove  \n",
    "$$\\frac{f\\left(a+h^{\\prime}\\right)-f(a)}{h^{\\prime}} \\leqslant \\frac{f(a+h)-f(a)}{h}$$,  \n",
    "for $h > 0$ and $h' < 0$ (Each $\\frac{f(a+h)-f(a)}{h}$ is greater than any one such quotient for $h' < 0$ )  \n",
    "$$\\begin{aligned}\n",
    "\\frac{f(b)-f(a)}{b-a} &=\\frac{(f(b)-f(x))+(f(x)-f(a))}{b-a} \\\\\n",
    "&=\\frac{b-x}{b-a} \\cdot \\frac{f(b)-f(x)}{b-x}+\\frac{x-a}{b-a} \\cdot \\frac{f(x)-f(a)}{x-a} \\\\\n",
    "&=\\lambda \\cdot \\frac{f(b)-f(x)}{b-x}+(1-\\lambda) \\cdot \\frac{f(x)-f(a)}{x-a}\n",
    "\\end{aligned}$$  \n",
    "where $0<\\lambda=\\frac{b-x}{b-a}<1$  \n",
    "By the convexity condition  \n",
    "$$\\frac{f(x)-f(a)}{x-a} \\leqslant \\frac{f(b)-f(a)}{b-a}$$  \n",
    "it then follows that   \n",
    "$$\\frac{f(b)-f(a)}{b-a} \\leqslant \\frac{f(b)-f(x)}{b-x}$$   \n",
    "by substituting, we have  \n",
    "$$\\frac{f(x)-f(a)}{x-a} \\leq \\frac{f(b)-f(x)}{b-x}$$  \n",
    "then we have  \n",
    "$$\\frac{f\\left(a+h^{\\prime}\\right)-f(a)}{h^{\\prime}} \\leqslant \\frac{f(a+h)-f(a)}{h}$$  \n",
    "\n",
    "(b) Given a convex function $f: R^n \\to R$, we define a funcion:  \n",
    "$$\\phi(t) = \\frac{f(x + td) - f(x)}{t}, t > 0$$  \n",
    "then the function is nondecreasing on $(0,\\infty)$  \n",
    "choose arbitary $0 < \\epsilon < t $, we get:  \n",
    "$$\\bar{x}+\\epsilon d=\\frac{\\epsilon}{t}\\left(\\bar{x}+t d\\right)+\\left(1-\\frac{\\epsilon}{t}\\right)\\bar{x}$$  \n",
    "then the proof is quite similar to that above.  \n",
    "After proving the nondecreasing, we have  \n",
    "$$f^{\\prime}(\\bar{x} ; d)=\\lim _{t \\rightarrow 0+} \\frac{f(\\bar{x}+t d)-f(\\bar{x})}{t}=\\lim _{t \\rightarrow 0^{+}} \\phi(t)=\\inf _{t>0}\\phi(t)$$  \n",
    "thus $f'(\\bar{x};d)$ is either a real number or $\\pm \\infty$,  \n",
    "now let $\\bar{x} \\in int(\\text{dom} f)$, then it implies $f$ is locally Lipschitz around $\\bar{x}$, thus there exists $\\ell > 0$ and $\\delta > 0$, such that  \n",
    "$$|f(x)-f(y)| \\leq \\ell\\|x-y\\|$$  \n",
    "whenever $x,y \\in \\mathbb{B}(\\bar{x}, \\delta)$, then for $t$ sufficient small, we have  \n",
    "$$|f(\\bar{x}+t d)-f(\\bar{x})| \\leq \\ell t\\|d\\|$$  \n",
    "$$\\left|\\frac{f(\\bar{x}+t d)-f(\\bar{x})}{t}\\right| \\leq \\ell\\|d\\|$$  \n",
    "thus we take the limit as $t \\to 0^+$, we get  \n",
    "$$f^{\\prime}(\\bar{x} ; d) \\leq \\ell \\| d \\mid$$\n",
    "Thus the directional derivative is a real number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization Problems  \n",
    "(1)  $$a=\\max _{j} \\min _{i} H_{i j} \\leq \\min _{i} \\max _{j} H_{i j}=b$$  \n",
    "(2)  $$\\begin{aligned}\\text{ minimize } R \\\\  \n",
    "s.t\\\\  \n",
    "d_2(x_i, x_0) \\leqslant R  \n",
    "\\end{aligned}$$  \n",
    "where $d_2$ is $l_2$ norm and $i = 1...n$  \n",
    "(3) **Hard SVM**  \n",
    "one can solve the following convex optimization problem:  \n",
    "$$\\begin{array}{ll}\n",
    "\\operatorname{minimize}_{w, b} & \\|w\\|_{2}^{2} \\\\\n",
    "\\text { subject to } & y_{i}\\left(w^{T} x_{i}+b\\right) \\geq 1 \\quad \\text { for all } i=1, \\ldots, n\n",
    "\\end{array}$$  \n",
    "If the resulting problem is feasible, then the data is linearly separable.  \n",
    "Since $w^Tx+b$ represents a hyperplane in $R^n$, and if we plug $x_i$ in, the sign of result $w^Tx_i+b$ represents the point $x_i$ lies above of below the hyperplane.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4)  \n",
    "Lagrangian:  \n",
    "$$L(x, \\lambda)=\\|A x-y\\|_{2}^{2}+\\lambda\\left(\\|x\\|_{2}^{2}-r^{2}\\right)$$  \n",
    "(b)  \n",
    "$$\\sup _{\\lambda \\succeq 0} L(x, \\lambda)=\\left\\{\\begin{array}{ll}\n",
    "+\\infty & \\text { if }\\|x\\|_{2}^{2}>r^{2} \\\\\n",
    "\\|A x-y\\|_{2}^{2} & \\text { otherwise }\n",
    "\\end{array}\\right.$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Suppose $g \\in \\partial f(x)$, this means for all $v \\in R^n$ and $h \\in R$, we have  \n",
    "$$f(x + hv) - f(x) \\geq hg^Tv$$  \n",
    "$$\\frac{f(x + hv) - f(x)}{h} \\geq g^Tv$$  \n",
    "Taking limit,\n",
    "$$\\nabla f(x)^Tv \\geq g^Tv$$  \n",
    "Using $-h$ in place of $h$ gives  \n",
    "$$f(x-h v) \\geq f(x)-h g^{T} v \\Longrightarrow g^{T} v \\geq \\frac{f(x-h v)-f(x)}{-h}$$  \n",
    "then  \n",
    "$$\\nabla f(x)^{T} v \\geq g^{T} v \\geq \\nabla f(x)^{T} v$$  \n",
    "and we thus have  \n",
    "$$(\\nabla f(x)-g)^{T} v=0$$  \n",
    "letting  \n",
    "$$v=\\nabla f(x)-g$$  \n",
    "we have  \n",
    "$$\\|\\nabla f(x)-g\\|_{2}^{2}=0$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   \n",
    "\n",
    "If $\\partial f(x) = \\emptyset$, then it is convex, if $\\partial f(x) = \\{\\nabla f(x)\\}$, which contains only one elements, then it is convex.  \n",
    "let $g_1, g_2 \\in \\partial f(x)$, then we have $f(z) \\geq f(x) + g_1^T(z - x)$, and $f(z) \\geq f(x) + g_2^T(z-x)$, for any fixed $x$, and for all $z$, then let $g_3 = \\theta g_1 + (1 - \\theta)g_2$   \n",
    "we have  \n",
    "$$\\theta f(z) + ( 1- \\theta) f(z) \\geq \\theta f(x) + (1 - \\theta)f(x) + (\\theta g_1^T + (1 - \\theta) g_2^T)(z-x))$$  \n",
    "$$f(z) \\geq f(x) + g_3^T (z - x)$$  \n",
    "the $g_3 \\in \\partial f(x)$  \n",
    "\n",
    "(3)  \n",
    "\n",
    "- False: Dimension error. The underestimation hyperplane is a subset of $R^{n + 1}$, but the subgradient is a subset of $R^n$. For example $f : R^2 \\to R, f(x,y) \\mapsto z$, the hyperplane is of 3 dimension, while the subgradient is of 2 dimension.  \n",
    "- False  \n",
    "- True  If 0 is a subgradient of $f$ at $x$, then $x$ is a global minimizer  \n",
    "- False: Suppose $|\\partial f(x)| = 1$  \n",
    "- False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
