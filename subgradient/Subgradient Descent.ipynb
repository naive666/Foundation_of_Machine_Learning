{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Optimization Problem(no intercept)  \n",
    "- SVM objective function  \n",
    "$$J(w)=\\frac{1}{n} \\sum_{i=1}^{n} \\max \\left(0,1-y_{i}\\left[w^{T} x_{i}\\right]\\right)+\\lambda\\|w\\|^{2}$$  \n",
    "- Not differentiable,  but what about we consider subgradient?  \n",
    "- Derivative of hinge loss, $l(m) = max(0, 1 - m)$  \n",
    "$$\\ell^{\\prime}(m)=\\left\\{\\begin{array}{ll}\n",
    "0 & m>1 \\\\\n",
    "-1 & m<1 \\\\\n",
    "\\text { undefined } & m=1\n",
    "\\end{array}\\right.$$  \n",
    "- We need gradient with respect to parameter vector $w \\in R^d$  \n",
    "$$\\begin{aligned}\n",
    "\\nabla_{w} \\ell\\left(y_{i} w^{T} x_{i}\\right) &=\\ell^{\\prime}\\left(y_{i} w^{T} x_{i}\\right) y_{i} x_{i}(\\text { chain rule }) \\\\\n",
    "&=\\left(\\begin{array}{ll}\n",
    "0 & y_{i} w^{T} x_{i}>1 \\\\\n",
    "-1 & y_{i} w^{T} x_{i}<1 \\\\\n",
    "\\text { undefined } & y_{i} w^{T} x_{i}=1\n",
    "\\end{array}\\right) y_{i} x_{i}\\left(\\text { expanded } m \\text { in } \\ell^{\\prime}(m)\\right) \\\\\n",
    "&=\\left\\{\\begin{array}{ll}\n",
    "0 & y_{i} w^{T} x_{i}>1 \\\\\n",
    "-y_{i} x_{i} & y_{i} w^{T} x_{i}<1 \\\\\n",
    "\\text { undefined } & y_{i} w^{T} x_{i}=1\n",
    "\\end{array}\\right.\n",
    "\\end{aligned}$$  \n",
    "thus  \n",
    "$$\\nabla_{w} \\ell\\left(y_{i} w^{T} x_{i}\\right)=\\left\\{\\begin{array}{ll}\n",
    "0 & y_{i} w^{T} x_{i}>1 \\\\\n",
    "-y_{i} x_{i} & y_{i} w^{T} x_{i}<1 \\\\\n",
    "\\text { undefined } & y_{i} w^{T} x_{i}=1\n",
    "\\end{array}\\right.$$  \n",
    "so  \n",
    "$$\\begin{aligned}\n",
    "\\nabla_{w} J(w) &=\\nabla_{w}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(y_{i} w^{T} x_{i}\\right)+\\lambda\\|w\\|^{2}\\right) \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{w} \\ell\\left(y_{i} w^{T} x_{i}\\right)+2 \\lambda w \\\\\n",
    "&=\\left\\{\\begin{array}{ll}\n",
    "\\frac{1}{n} \\sum_{i: y_{i} w^{T} x_{i}<1}\\left(-y_{i} x_{i}\\right)+2 \\lambda w & \\text { all } y_{i} w^{T} x_{i} \\neq 1 \\\\\n",
    "\\text { undefined } & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "\\end{aligned}$$  \n",
    "- The subgradient of the SVM objective is  \n",
    "$$\\nabla_{w} J(w)=\\frac{1}{n} \\sum_{i: y_{i} w^{T} x_{i}<1}\\left(-y_{i} x_{i}\\right)+2 \\lambda w$$  \n",
    "when all $y_iw^Tx_i \\ne 1$ for all $i$, and otherwise is undefined  \n",
    "- Suppose we want to use gradient descent on $J(w)$:  \n",
    "   - If we start at a random $w$, will we ever hit $y_iw^Tx_i = 1$?  \n",
    "   - If we did, could we perturb the step size by $\\epsilon$ to miss such a point\n",
    "   - If we blindly apply gradient descent from a random starting point \n",
    "     - seems unlikely that we’ll hit a point where the gradient is undeﬁned\n",
    "   - Still, doesn’t mean that gradient descent will work if objective not diﬀerentiable  \n",
    "   - Theory of subgradients and subgradient descent will clear up any uncertainty  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convexity and sublevel sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Sets and Sublevel Sets\n",
    "Let $f: R^d \\to R$ be a function  \n",
    "- Definition  \n",
    "A **level set** or **contour line** for the value c is the set of points $f(x) = c$  \n",
    "- Definition  \n",
    "A **sublevel set** for the value c is the set of points $x \\in R^d$ for which $f(x) \\leqslant c$  \n",
    "- Theorem  \n",
    "If $f: R^d \\to R$ is convex, then the sublevel sets are convex  \n",
    "## Convex Optimization Problem: Implicit Form\n",
    "- Convex Optimization Problem: Implicit Form  \n",
    "$$\\begin{array}{ll}\n",
    "\\text { minimize } & f(x) \\\\\n",
    "\\text { subject to } & x \\in C\n",
    "\\end{array}$$  \n",
    "where f is a convex function and C is a convex set  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex and Differentiable Function  \n",
    "First-Order Approximation  \n",
    "- Suppose $f:R^d \\to R$ is differentiable  \n",
    "- Predict $f(y)$ given $f(x)$ and $\\nabla f(x)$  \n",
    "- Linear Approximation  \n",
    "$$f(y) \\approx f(x)+\\nabla f(x)^{T}(y-x)$$  \n",
    "<div align=\"center\"><img src = \"./linearApprox.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "- Suppose $f:R^d \\to R$ is convex and differentiable, then for any $x,y$  \n",
    "- The linear approximation to $f$ at $x$ is a **global underestimator** of $f$  \n",
    "- Corollary  \n",
    "$f$ is convex and differentiable, if $\\nabla f(x) = 0$, then $x$ is a global minimizer of f  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgradients \n",
    "- Definition  \n",
    "A vector $g \\in R^d$ is a subgradient of $f:R^d \\to R$ at $x$ if for all $z$,  \n",
    "$$f(z) \\geq f(x) + g^T(z - x)$$  \n",
    "<div align=\"center\"><img src = \"./subgradients.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "- Blue is the graph of $f(x)$, each red line is a global lower bound of $f(x)$  \n",
    "\n",
    "## Subdiﬀerential  \n",
    "- Definition  \n",
    "   - $f$ is subdifferentiable at $x$ if there exists at least one subgradient at $x$.  \n",
    "   - The set of all subgradients at $x$ is called the subdifferential: $\\partial f(x)$  \n",
    "- Basic Facts  \n",
    "   -$f$ is convex and differentiable $ \\Rightarrow$ $\\partial f(x) = \\{\\nabla f(x) \\}$  \n",
    "   - At any point, there can be 0, 1, or infinitely many subgradients  \n",
    "   - if $\\partial f(x) = \\emptyset$ then $f$ is not convex  \n",
    "\n",
    "## Globla Optimality Condition\n",
    "- Corollary\n",
    "if $0 \\in \\partial f(x)$, then $x$ is a gloabal minimizer of $f$  \n",
    "\n",
    "## Subdiﬀerential of Absolute Value\n",
    "- Consider $f(x) = |x|$  \n",
    "<div align=\"center\"><img src = \"./abs.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## $f(x_1, x_2) = |x_1| + 2|x_2|$  \n",
    "<div align=\"center\"><img src = \"./f(x1,x2).jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Let’s find the subdifferential of $f(x_1, x_2) = |x_1| + 2|x_2| at (3,0)$   \n",
    "- First coordinate of subgradient must be 1,  from $|x_1|$ part  \n",
    "- Second coordinate of subgradient can be anything in $[-2,2]$    \n",
    "- So graph of $h(x_1, x_2) = f(3, 0) + g^T (x_1 - 3 , x_2)$ is a global underestimation of $f(x_1, x_2)$, for any $g = (g_1,g_2)$, where $g_1 = 1, g_2 \\in [-2,2]$  \n",
    "\n",
    "### Underestimation Hyperplane  \n",
    "<div align=\"center\"><img src = \"./hyperplane.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## Subdiﬀerential on Contour Plot  \n",
    "<div align=\"center\"><img src = \"./contour.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Contour Lines and Gradients  \n",
    "- For function $f: R \\to R^d$,  \n",
    "  - Graphs of the function lives in $R^{d + 1}$  \n",
    "  - Gradient and subgradient oof $f$ lives in $R^d$\n",
    "  - Contours and level sets lives in $R^d$  \n",
    "- $f: R^d \\to R$ continuously differentiable, $\\nabla f(x_0) \\ne 0$, then $\\nabla f(x_0)$ normal to level set  \n",
    "\n",
    "## Contour Lines and Subgradients\n",
    "- Let $f : R \\to R^d$ has subgradient $g$ at $x_0$  \n",
    "   - Hyperplane $H$ orthogonal to $g$ at $x_0$ must support the level set $S=\\left\\{x \\in \\mathbf{R}^{d} \\mid f(x)=f\\left(x_{0}\\right)\\right\\}$  \n",
    "   - i.e. $H$ contains $x_0$ and all of $S$ lies one side of $H$  \n",
    "<div align=\"center\"><img src = \"./subgradient_contour.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Points on $g$ side of $H$ have larger f-values than $f(x_0)$  \n",
    "- But points on $-g$ side may not have smaller f-values  \n",
    "- So $-g$ may not be a descent direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgradient Descent  \n",
    "- Suppose $f$ is convex, and we start optimizing at $x_0$  \n",
    "- Repeat  \n",
    "  - Step in a negative subgradient direction  \n",
    "  - $x = x_0 - tg$, where $t > 0$ is the step size and $g \\in \\partial f(x_0)$  \n",
    "- $−g$ not a descent direction – can this work?\n",
    "\n",
    "## Subgradient Gets Us Closer To Minimizer\n",
    "- Theorem  \n",
    "Suppose $f$ is convex:  \n",
    "  - Let $x = x_0 - tg$\n",
    "  - Let $f(z)$ be any point for which $f(z) < f(x_0)$  \n",
    "  - Then for small enough $t > 0$  \n",
    "$$\\|x-z\\|_{2}<\\left\\|x_{0}-z\\right\\|_{2}$$\n",
    "  - Apply this with $z=x^{*} \\in \\arg \\min _{x} f(x)$  \n",
    "  \n",
    "Negative subgradient step gets us closer to minimizer  \n",
    "**proof**  \n",
    "$$\\begin{aligned}\n",
    "\\|x-z\\|_{2}^{2} &=\\left\\|x_{0}-\\operatorname{tg}-z\\right\\|_{2}^{2} \\\\\n",
    "&=\\left\\|x_{0}-z\\right\\|_{2}^{2}-2 \\operatorname{tg}^{T}\\left(x_{0}-z\\right)+t^{2}\\|g\\|_{2}^{2} \\\\\n",
    "& \\leqslant\\left\\|x_{0}-z\\right\\|_{2}^{2}-2 t\\left[f\\left(x_{0}\\right)-f(z)\\right]+t^{2}\\|g\\|_{2}^{2}\n",
    "\\end{aligned}$$  \n",
    "consider $-2 t\\left[f\\left(x_{0}\\right)-f(z)\\right]+t^{2}\\|g\\|_{2}^{2}$, it is convex and quadratic  \n",
    "has zero at $t = 0$ and $t=2\\left(f\\left(x_{0}\\right)-f(z)\\right) /\\|g\\|_{2}^{2}>0$  \n",
    "Therefor it is negative for any  \n",
    "$$t \\in\\left(0, \\frac{2\\left(f\\left(x_{0}\\right)-f(z)\\right)}{\\|g\\|_{2}^{2}}\\right)$$  \n",
    "\n",
    "## Convergence Theorem for Fixed Step Size\n",
    "- Assume $f:R \\to R^d$ is convex and \n",
    "  - f is Lipschitz continuous with constant $G > 0$:  \n",
    "  $$|f(x)-f(y)| \\leqslant G\\|x-y\\| \\text { for all } x, y$$  \n",
    "- Theorem  \n",
    "For fixed step size t, subgradient method satisfies  \n",
    "$$\\lim _{k \\rightarrow \\infty} f\\left(x_{b e s t}^{(k)}\\right) \\leqslant f\\left(x^{*}\\right)+G^{2} t / 2$$  \n",
    "\n",
    "## Convergence Theorems for Decreasing Step Sizes\n",
    "- Assume $f:R \\to R^d$ is convex and \n",
    "  - f is Lipschitz continuous with constant $G > 0$:  \n",
    "  $$|f(x)-f(y)| \\leqslant G\\|x-y\\| \\text { for all } x, y$$  \n",
    "- Theorem  \n",
    "For step size respecting Robbins-Monro conditions  \n",
    "$$\\lim _{k \\rightarrow \\infty} f\\left(x_{\\text {best}}^{(k)}\\right)=f\\left(x^{*}\\right)$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
