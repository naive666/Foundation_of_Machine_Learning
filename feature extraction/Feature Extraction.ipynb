{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Input Space $X$  \n",
    "- Our general learning theory setup: no assumptions about $X$  \n",
    "- But $X = R^d$ for the specific methods we’ve developed:  \n",
    "   - Ridge regression\n",
    "   - Lasso regression\n",
    "   - Linear SVM  \n",
    "- However, often the input space is not $R^d$  \n",
    "\n",
    "# Feature Extraction  \n",
    "- Definition  \n",
    "Mapping an input from $X$ to a vector in $R^d$ is called **feature extraction** or **featurization**  \n",
    "<div align=\"center\"><img src = \"./feature extraction.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## Example: Detecting Email Addresses  \n",
    "- Task: Predict whether a string is an email address\n",
    "- Could use domain knowledge and write down:\n",
    "<div align=\"center\"><img src = \"./email.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- But this was ad-hoc, and maybe we missed something  \n",
    "- Could be more systematic?  \n",
    "\n",
    "# Feature Templates\n",
    "- Deﬁnition (informal)   \n",
    "A feature template is a group of features all computed in a similar way  \n",
    "<div align=\"center\"><img src = \"./feature template.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Feature Template: Last Three Characters Equal ___\n",
    "   - Don’t think about which 3-letter suffixes are meaningful... \n",
    "   - Just include them all  \n",
    "<div align=\"center\"><img src = \"./end.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Feature Template: One-Hot Encoding\n",
    "A one-hot encoding is a set of features (e.g. a feature template) that always has exactly one non-zero value.\n",
    "<div align=\"center\"><img src = \"./onehot.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Feature Vector Representations\n",
    "<div align=\"center\"><img src = \"./onehot2.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "- Array  \n",
    "    - assumed ﬁxed ordering of the features \n",
    "    - appropriate when signiﬁcant number of nonzero elements (“dense feature vectors”)\n",
    "    - very eﬃcient in space and speed \n",
    "- Map\n",
    "    - best for sparse feature vectors (i.e. few nonzero features) \n",
    "    - features not in the map have default value of zero \n",
    "    - Python code for “ends with last 3 characters”: {\"endsWith_\"+x[-3:]: 1}. \n",
    "\n",
    "# Handling nonlinearity with Linear Method  \n",
    "- Example: Predict Health  \n",
    "    - Height\n",
    "    - Weight\n",
    "    - Body Temperature\n",
    "    - blood pressure\n",
    "- Issues for nonlinear problems  \n",
    "  - For linear predictors, it’s important how features are added  \n",
    "  - Three types of nonlinearities can cause problems\n",
    "      - Non-monotonicity \n",
    "      - Saturation\n",
    "      - Interactions between features\n",
    "\n",
    "## Non-monotonicity: The Issue\n",
    "- Feature Map: $\\phi(x) = [1, temperature(x)]$  \n",
    "- Action: predict health score $y \\in R$(positive is good)  \n",
    "- Hypothesis Space $F = \\{\\text{affine functions of temperature}\\}$ \n",
    "- Issue: Health is not an affine function of temperature  \n",
    "- Aﬃne function can either say \n",
    "     - Very high is bad and very low is good, or \n",
    "     - Very low is bad and very high is good\n",
    "     - But here, both extremes are bad.\n",
    "- Solution1:  \n",
    "Transform the input  \n",
    "$$\\phi(x)=\\left[1,\\{\\text { temperature }(x)-37\\}^{2}\\right]$$  \n",
    "where 37 is “normal” temperature in Celsius  \n",
    "    -  but this requires domain knowledge \n",
    "- Solution2  \n",
    "Think less, put in more  \n",
    "$$\\phi(x)=\\left[1, \\text { temperature }(x),\\{\\text { temperature }(x)\\}^{2}\\right]$$  \n",
    "More expressive than solution 1  \n",
    "\n",
    "- General Rule   \n",
    "Features should be simple building blocks that can be pieced together  \n",
    "\n",
    "## Saturation: The Issue\n",
    "- Setting: Find products relevant to user’s query  \n",
    "- Input: Product x \n",
    "- Action: Score the relevance of x to user’s query \n",
    "- Feature Map  \n",
    "$$\\phi(x) = [1,N(x)]$$  \n",
    "where $N(x)$ =number of people who bought x. \n",
    "- We expect a monotonic relationship between $N(x)$ and relevance, but... Is relevance linear in $N(x)$?   \n",
    "    - Relevance score reﬂects conﬁdence in relevance prediction\n",
    "    - Are we 10 times more conﬁdent if N(x) =1000 vs N(x) =100\n",
    "    - Bigger is better... but not that much better.\n",
    "\n",
    "### Saturation: Solve with nonlinear transform\n",
    "- Smooth nonlinear transformation  \n",
    "$$\\phi(x)=[1, \\log \\{1+N(x)\\}]$$  \n",
    "log(·) good for values with large dynamic ranges   \n",
    "\n",
    "### Saturation: Solve by discretization\n",
    "- Discretization (a discontinuous transformation):   \n",
    "$$\\phi(x)=(1(5 \\leqslant N(x)<10), 1(10 \\leqslant N(x)<100), 1(100 \\leqslant N(x)))$$  \n",
    "\n",
    "- Sometimes we might prefer one-sided buckets   \n",
    "$$\\phi(x)=(1(5 \\leqslant N(x)), 1(10 \\leqslant N(x)), 1(100 \\leqslant N(x)))$$  \n",
    "    - Why? Hint: What’s the eﬀect of regularization the parameters for rare buckets?   \n",
    "\n",
    "- Small buckets allow quite ﬂexible relationship  \n",
    "\n",
    "## Interactions: The Issue\n",
    "- Input: Patient information x \n",
    "- Action: Health score $y \\in R$ (higher is better)  \n",
    "- Feature Map  \n",
    "$$\\phi(x)=[\\text { height }(x), \\text { weight }(x)]$$  \n",
    "- Issue: It’s the weight relative to the height that’s important  \n",
    "- Impossible to get with these features and a linear classiﬁer  \n",
    "- Need some interaction between height and weight.\n",
    "\n",
    "### Interactions: Approach 1\n",
    "- Google “ideal weight from height”   \n",
    "- J. D. Robinson’s “ideal weight” formula (for a male):  \n",
    "weight $(\\mathrm{kg})=52+1.9[\\text { height }(\\text { in })-60]$  \n",
    "- Make score square deviation between height(h) and ideal weight(w)   \n",
    "$$f(x)=(52+1.9[h(x)-60]-w(x))^{2}$$  \n",
    "$$f(x)=3.61 h(x)^{2}-3.8 h(x) w(x)-235.6 h(x)+w(x)^{2}+124 w(x)+3844$$  \n",
    "\n",
    "### Interactions: Approach 2\n",
    "Just include all second order features  \n",
    "$$\\phi(x)=[1, h(x), w(x), h(x)^{2}, w(x)^{2}, \\underbrace{h(x) w(x)}_{\\text {cross term }}]$$  \n",
    "- General Principle  \n",
    "Simpler building blocks replace a single “smart” feature  \n",
    "\n",
    "# Predicate Features and Interaction Terms\n",
    "- Definition  \n",
    "A predicate on the input space $X$ is a function $P : X \\to \\{True,False\\}$  \n",
    "- Many features take this form  \n",
    "    - $x \\to s(x) = 1$(subject is sleeping)  \n",
    "    - $x \\to d(x) = 1$(subject is driving)  \n",
    "- For predicates, interaction terms correspond to AND conjunctions  \n",
    "    - $x \\to s(x)d(x) = 1$(subject is sleeping AND subject is driving)  \n",
    "\n",
    "# What is linear?  \n",
    "- Non-linear feature map $\\phi : X \\to R^d$  \n",
    "- Hypothesis space  \n",
    "$$\\mathcal{F}=\\left\\{f(x)=w^{T} \\phi(x) \\mid w \\in \\mathbf{R}^{d}\\right\\}$$  \n",
    "- Linear in w? Yes  \n",
    "- Linear in $\\phi(x)$? Yes  \n",
    "- Linear in $x$? No  \n",
    "\n",
    "# Geometric Example: Two class problem, nonlinear boundary\n",
    "<div align=\"center\"><img src = \"./nonlinear.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "- With linear feature map $\\phi(x) = (x1,x2)$ and linear models, no hope  \n",
    "- With appropriate nonlinearity $\\phi(x)=\\left(x_{1}, x_{2}, x_{1}^{2}+x_{2}^{2}\\right)$, a piece of cake   \n",
    "\n",
    "Consider a linear hypothesis space with a feature map $\\phi : X \\to R^d$  \n",
    "$$\\mathcal{F}=\\left\\{f(x)=w^{T} \\phi(x)\\right\\}$$  \n",
    "<div align=\"center\"><img src = \"./last.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "We can grow the linear hypothesis space F by adding more features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
