{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we need regularization?\n",
    "Suppose we have a decision function $f \\in \\mathcal{F}$, which fits the training data very well, but when we generalize it to test data, its performance becomes terrible. One way to solve this problem is reducing the complexity of the functions.  \n",
    "For linear functions: $x \\to w_1x_1 + ... + w_dx_d$\n",
    "- $\\ell_0$ complexity: the number of all non-zero coefficients.  \n",
    "- $\\ell_1$ complexity (Lasso): $\\sum_{i = 1}^{d}|w_i|$, for coefficient $w_i$  \n",
    "- $\\ell_2$ complexity (Ridge): $\\sum_{i = 1}^{d}w_i^2$, for coefficient $w_i$  \n",
    "<br/>\n",
    "\n",
    "Complexity Measure:  \n",
    "$\\Omega : \\mathcal{F} \\to [0,\\infty)$  \n",
    "Let's consider all function in $\\mathcal{F}$ with complexity at most $r$:  \n",
    "$\\mathcal{F}_r = \\{f \\in \\mathcal{F}| \\Omega(F) \\leq r \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Empirical Risk Minimization\n",
    "## Constrained ERM (Ivanov Regularization)\n",
    "\n",
    "For complexity measure $\\Omega : \\mathcal{F} \\to [0,\\infty)$, and a fixed $r\\ge 0$,\n",
    "$$ \\min_{f \\in \\mathcal{F}} \\frac{1}{n}\\sum_{i = 1}^{n}\\ell(f(x_i), y_i)$$\n",
    "$$s.t.\\quad \\Omega(f) \\le r$$  \n",
    "we can also write in a concise form:  \n",
    "$$ \\min_{f \\in \\mathcal{F}_r} \\frac{1}{n}\\sum_{i = 1}^{n}\\ell(f(x_i), y_i)$$  \n",
    "Choose $r$ using validation data or cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Empirical Risk Minimization\n",
    "## Penalized ERM (Tikhonov regularization) \n",
    "For complexity measure $\\Omega : \\mathcal{F} \\to [0,\\infty)$, and a fixed $\\lambda\\ge 0$,  \n",
    "$$\\min_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i = 1}^{n}\\ell(f(x_i), y_i) + \\lambda \\Omega(f)$$  \n",
    "Choose $\\lambda$ using validation data or cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ivanov vs Tikhonov Regularization \n",
    "Indeed,in most cases they are equivalent.  \n",
    "\n",
    "Ivanov and Tikhonov are equivalent if:  \n",
    "- For any choice of $r > 0$, any Ivanov solution  \n",
    "$$f_r^{*} \\in \\arg\\min_{f \\in \\mathcal{F_r}}L(f) $$  \n",
    "is also a Tikhonov solution for some $\\lambda > 0$. That is $\\exists \\lambda > 0$, such that:  \n",
    "$$f_r^{*} \\in \\arg\\min_{f \\in \\mathcal{F}}L(f) + \\lambda\\Omega(f) $$  \n",
    "- Conversely, for any choice of $\\lambda > 0$, any Tikhonov solution is Ivanov solution for some $r > 0$  \n",
    "We will discuss details in homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\ell_{l}$ and $\\ell_2$ Regularization  \n",
    "- Consider Linear models:  \n",
    "$$\\mathcal{F} = \\{f : \\mathbb R^d \\to \\mathbb R|f(x) = w^Tx, w \\in \\mathbb{R}^d\\}$$  \n",
    "- Loss: $\\ell(\\hat{y}, y) = (\\hat y - y)^2$  \n",
    " - Ridge Regression with Ivanov Form  \n",
    "$\\hat{w}=\\underset{\\|w\\|_{2}^{2} \\leqslant r^{2}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left\\{w^{T} x_{i}-y_{i}\\right\\}^{2}$\n",
    " <br/>\n",
    " - Ridge Regression with Tikhonov Form  \n",
    "$\\hat{w}=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left\\{w^{T} x_{i}-y_{i}\\right\\}^{2}+\\lambda\\|w\\|_{2}^{2}$  \n",
    " <br/>\n",
    " - How does $\\ell_2$ regularization induce “regularity”?\n",
    "For $\\hat{f}(x) = \\hat{w}^Tx$, $\\hat{f}$ is **Lipschitz continuous** with Lipschitz constant $\\lVert \\hat w \\rVert_2$  \n",
    "Proof:  \n",
    "$$|f(x + h) - f(x)| = w^T(x + h) - w^Tx = |w^Th|$$  \n",
    "$$\\le \\lVert w^T \\rVert \\lVert h \\rVert $$  \n",
    " <br/>\n",
    " - Lasso Regression with Ivanov Form  \n",
    "$\\hat{w}=\\underset{\\|w\\|_{1} \\leqslant r}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left\\{w^{T} x_{i}-y_{i}\\right\\}^{2}$  \n",
    " <br/>\n",
    " - Lasso Regression with Tikhonov Form  \n",
    " $\\hat{w}=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left\\{w^{T} x_{i}-y_{i}\\right\\}^{2}+\\lambda\\|w\\|_{1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Gives Feature Sparsity\n",
    "In the optimal solution $\\hat{w}$ from Lasso, many entries are 0, which means the corresponding features we do not need.  \n",
    "(For a data point $x \\in R^d$, it has $d$ features, and for each feature, we assign a weight, which is the corresponding entry of $w$)\n",
    "<br/>  \n",
    "## What are the benefits of Sparsity?\n",
    "- Time to compute the result is reduced.\n",
    "- We need less memory to store the features\n",
    "- Identify the important features.  \n",
    "<br/>\n",
    "\n",
    "## Why does Lasso give Sparsity?\n",
    "Consider $\\ell_1$ and $\\ell_2$ norm constraints in two dimension  \n",
    "Linear Hypothesis space $\\mathcal{F} = \\{f(x) = w_1x_1 + w_2x_2\\}$\n",
    "<div align=\"center\"><img src = \"./norm constraints.jpg\" width = '500' height = '100' align = center /></div>\n",
    "Blue region: Area satisfying complexity constraint $|w_1| + |w_2| \\le r$  \n",
    "\n",
    "Red lines: contours of $\\hat{R}_n(w) = \\sum_{i = 1}^{n}(w^Tx_i - y_i)^2$\n",
    "\n",
    "<div align=\"center\"><img src = \"./famous l1.jpg\" width = '500' height = '100' align = center /></div>\n",
    "\n",
    "\n",
    "As the figure demonstrates, the optimal points lie on the axis, which gives sparsity.  \n",
    "Suppose design matrix X is orthogonal, so $X^TX = I$, and contours are circles, then OLS solution in green or red regions implies $\\ell^1$ constrained solution will be at corner  \n",
    "<div align=\"center\"><img src = \"./l1.jpg\" width = '500' height = '100' align = center /></div>\n",
    "\n",
    "\n",
    "## The Empirical Risk for Square Loss\n",
    "Denote the empirical risk of $f(x) = w^Tx$ by:  \n",
    "$$\\hat{R}_n(w) = \\frac{1}{n}\\lVert Xw - y\\rVert^2$$\n",
    "where $X$ is the **design matrix**  \n",
    "$\\hat{R}_n(w)$ is minimized by $\\hat{w} = (X^TX)^{-1}X^Ty$  \n",
    "- What does $\\hat{R}_n$ look like around $\\hat{w}$?  \n",
    "**Complement**  \n",
    "- **Proposition1** For any vectors $x,b \\in R^d$ and symmetric invertible matrix $M \\in R^{d \\times d}$, we have \n",
    "$$\n",
    "\\begin{aligned}  \n",
    "x^{T} M x-2 b^{T} x=\\left(x-M^{-1} b\\right)^{T} M\\left(x-M^{-1} b\\right)-b^{T} M^{-1} b\n",
    "\\end{aligned}\n",
    "$$\n",
    "- **Proposition2** (Sum of two quadratic forms in x) Suppose $f(x)$ is the sum of two quadratic forms in $x$:  \n",
    "$$f(x)=(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)+(x-\\theta)^{T} V^{-1}(x-\\theta)$$  \n",
    "Then we can write $f$ as a single quadratic form plus a constant term, independent of $x$   \n",
    "$$f(x)=\\left(x-M^{-1} b\\right)^{T} M\\left(x-M^{-1} b\\right)-b^{T} M^{-1} b+R$$  \n",
    "where $M = \\Sigma^{-1} + V^{-1}$, $b = \\Sigma^{-1}\\mu + V^{-1}\\theta$, and $R = \\theta^{T}V^{-1}\\theta + \\mu^{T}\\Sigma^{-1}\\mu$  \n",
    "By tedious calculation and applying the above conclusion, we can get:  \n",
    "$$\\hat{R}_{n}(w)=\\frac{1}{n}(w-\\hat{w})^{T} X^{T} X(w-\\hat{w})+\\hat{R}_{n}(\\hat{w})$$  \n",
    "Since $\\hat{R}_n(\\hat{w})$ is independent on $w$, if we set $\\frac{1}{n}(w-\\hat{w})^{T} X^{T} X(w-\\hat{w}) = c$, the set of $\\left\\{w |(w-\\hat{w})^{T} X^{T} X(w-\\hat{w})=n c\\right\\}$ is an  **ellipsoid centered at $\\hat{w}$** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Famous Picture for $\\ell^2$ Regularization\n",
    "<div align=\"center\"><img src = \"./famous l2.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## $(\\ell_q)^q$ Constraints\n",
    "$\\ell_{q}:\\left(\\|w\\|_{q}\\right)^{q}=\\left|w_{1}\\right|^{q}+\\left|w_{2}\\right|^{q}$  \n",
    "<div align=\"center\"><img src = \"./lq.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "for $0 < q < 1$, $(\\ell_q)^q$ even sparser  \n",
    "<div align=\"center\"><img src = \"./lq_graph.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Find Lasso Solution?\n",
    "- $\\lVert w\\rVert_1 = |w_1| + |w_2|$ is not differentiable  \n",
    "\n",
    "## Splitting a Number into Positive and Negative Parts\n",
    "For any $a \\in R$, let $a_+ = max\\{0,a\\}, a_- = max\\{0,-a\\}$, the followings are some examples:  \n",
    "$7^+ = 7, 7^- = 0$  \n",
    "$-3^+ = 0, -3^- = 3$,  \n",
    "then $a = a^+ - a^-$, $|a| = a^+ + a^-$  \n",
    "<br/>\n",
    "In Lasso, we can divide $w_i$ by $w_i = w_i^+ - w_i^-$, and we denote $w^+ = (w_1^+,...,w_d^+)$ and $w^- = (w_1^-,...,w_d^-)$  \n",
    "\n",
    "## Lasso as an Quadratic Program  \n",
    "Substituting $w = w^+ - w^-$, and $|w| = w^+ + w^-$ gives an equivalent problem:  \n",
    "$$\\min _{w^{+}, w^{-}} \\sum_{i=1}^{n}\\left(\\left(w^{+}-w^{-}\\right)^{T} x_{i}-y_{i}\\right)^{2}+\\lambda 1^{T}\\left(w^{+}+w^{-}\\right)$$  $$s.t. w_i^+,w_i^- \\geq 0, \\forall i$$  \n",
    "Thus, the new objective is differentiable, and more precisely, it is convex and quadratic.  \n",
    "\n",
    "### A Possible Confusion\n",
    "If we want to optimize this program, we have 2d variables and 2d constraints. But finally, we need $w_i^+$ and its corresponding $w_i^-$ to get $w = w_i^+ - w_i^-$. Here, we only have $2d$ variables, which may in any order.  \n",
    "\n",
    "### Solve the Confusion  \n",
    "Lasso problem is trivially equivalent to the following:\n",
    "$$\n",
    "\\min _{w} \\min _{a, b} \\sum_{i=1}^{n}\\left((a-b)^{T} x_{i}-y_{i}\\right)^{2}+\\lambda 1^{T}(a+b)\n",
    "$$\n",
    "subject to $a_{i} \\geqslant 0$ for all $i, b_{i} \\geqslant 0$ for all $i$\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "a-b=w \\\\\n",
    "a+b=|w|\n",
    "\\end{array}\n",
    "$$  \n",
    "Claim: We don't need constraint $a + b = |w|$  \n",
    "Reason: Since $a - b = w$, we must have $a = w^+, b = w^-$, and $a + b = |w|$  \n",
    "<br/> \n",
    "\n",
    "Claim: We don't need constraint $a - b = w$  \n",
    "Reason: For any $a,b \\geq 0$, there exists $w = a - b$, and we do not require $w \\geq 0$  \n",
    "<br/>\n",
    "\n",
    "So Lasso optimization problem becomes  \n",
    "$$\n",
    "\\min _{a, b} \\sum_{i=1}^{n}\\left((a-b)^{T} x_{i}-y_{i}\\right)^{2}+\\lambda 1^{T}(a+b)\n",
    "$$\n",
    "subject to $\\quad a_{i} \\geqslant 0$ for all $i \\quad b_{i} \\geqslant 0$ for all $i$  \n",
    "\n",
    "## Projected SGD  \n",
    "$$\n",
    "\\min _{w^{+}, w^{-} \\in \\mathbf{R}^{d}} \\sum_{i=1}^{n}\\left(\\left(w^{+}-w^{-}\\right)^{T} x_{i}-y_{i}\\right)^{2}+\\lambda 1^{T}\\left(w^{+}+w^{-}\\right)\n",
    "$$\n",
    "subject to $w_{i}^{+} \\geqslant 0$ for all $i$\n",
    "$$\n",
    "w_{i}^{-} \\geqslant 0 \\text { for all } i\n",
    "$$  \n",
    "just like SGD, but after each step, we set it back to $0$,if any component of $w^+$ or $w^-$ is negative  \n",
    "\n",
    "## Coordinate Descent Method\n",
    "Goal: Minimize $L(w) = L(w_1, w_2,..., w_d)$ over $w = (w_1,..., w_d) \\in R^d$, in each step, we solve \n",
    "$$w_{i}^{\\text {new }}=\\underset{w_{i}}{\\arg \\min } L\\left(w_{1}, \\ldots, w_{i-1}, \\mathbf{w}_{\\mathbf{i}}, w_{i+1}, \\ldots, w_{d}\\right)$$  \n",
    "Example:  \n",
    "Suppose we have $w \\in R^3$, we fix $w_2$ and $w_3$ first, and search on $w_1$ to find a $w_1 = w_1^*$ such that $L(w_1^*, w_2, w_3) \\leq L(w_1, w_2, w_3)$ for any $w_1$, then we fix $w_1^*$ and $w_3$, search for the optimal $w_2 = w_2^*$, finally search for $w_3^*$.<br/>\n",
    "\n",
    "**Algorithm**  \n",
    "Goal: Minimize $L(w) = L(w_1, w_2,..., w_d)$ over $w = (w_1,..., w_d) \\in R^d$\n",
    "- Initialize $w^{(0)} = 0$  \n",
    "- While not converge:\n",
    "  - Choose a coordinate $j \\in \\{1,...,d\\}$\n",
    "  - $w_{j}^{\\text {new }} \\leftarrow \\arg \\min _{w_{j}} L\\left(w_{1}^{(t)}, \\ldots, w_{j-1}^{(t)}, \\mathbf{w}_{\\mathbf{j}}, w_{j+1}^{(t)}, \\ldots, w_{d}^{(t)}\\right)$\n",
    "  - $w_{j}^{(t+1)} \\leftarrow w_{j}^{\\text {new }}$ and $w^{(t+1)} \\leftarrow w^{(t)}$\n",
    "  - $t \\leftarrow t+1$\n",
    "- Coordinate Descent is good if it is easy to minimize w.r.t one coordinate at one time.\n",
    "- Random coordinate choice: Stochastic Coordinate Descent\n",
    "- Cyclic coordinate choice: Cyclic Coordinate Descent  \n",
    "<br/>  \n",
    "**Sufficient Conditions:**  \n",
    "Suppose we want to minimize $f : R^d \\to R$  \n",
    "- f is continuous differentiable and   \n",
    "- f is strict convex in each coordinate  \n",
    "**Weak Condition:**  \n",
    "Theorem:  \n",
    "If the objective $f$ has the following structure:  \n",
    "$$f\\left(w_{1}, \\ldots, w_{d}\\right)=g\\left(w_{1}, \\ldots, w_{d}\\right)+\\sum_{j=1}^{d} h_{j}\\left(w_{j}\\right)$$  \n",
    "where $g: R^d \\to R$ is differentiable and convex and each $h_j: R \\to R$ is convex but not necessarily differentiable,  \n",
    "then the coordinate descent algorithm converges to the global minimum\n",
    "\n",
    "## Coordinate Descent Method for Lasso\n",
    "- Why mention coordinate descent for Lasso? \n",
    "- In Lasso, the coordinate minimization has a closed form solution!\n",
    "<br/> \n",
    "Closed Form Coordinate Minimization for Lasso(Details See Homework 1)\n",
    "$$\\hat{w}_{j}=\\underset{w_{j} \\in \\mathbf{R}}{\\arg \\min } \\sum_{i=1}^{n}\\left(w^{T} x_{i}-y_{i}\\right)^{2}+\\lambda|w|_{1}$$  \n",
    "then  \n",
    "$$\\hat{w}_{j}=\\left\\{\\begin{array}{ll}\n",
    "\\left(c_{j}+\\lambda\\right) / a_{j} & \\text { if } c_{j}<-\\lambda \\\\\n",
    "0 & \\text { if } c_{j} \\in[-\\lambda, \\lambda] \\\\\n",
    "\\left(c_{j}-\\lambda\\right) / a_{j} & \\text { if } c_{j}>\\lambda\n",
    "\\end{array}\\right.$$  \n",
    "$$a_{j}=2 \\sum_{i=1}^{n} x_{i, j}^{2}$$,  \n",
    "$$c_{j}=2 \\sum_{i=1}^{n} x_{i, j}\\left(y_{i}-w_{-j}^{T} x_{i,-j}\\right)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
