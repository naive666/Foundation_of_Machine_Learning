{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression  \n",
    "- Objective function  \n",
    "$$\\text{min}_{w \\in R^{n}} \\frac{\\lambda}{N} \\|w\\|^2 + \\frac{1}{N}\\sum_{i = 1}^N (w^T \\psi(x_i) - y_i)^2 $$  \n",
    "using representer theorem:  \n",
    "If $J(w)$ has a minimizer, then it has the form $w^* = \\sum_{i = 1}^n \\alpha_i\\psi(x_i)$  \n",
    "then the objective function becomes  \n",
    "$$\\text{min}_{\\alpha \\in R} \\frac{\\lambda}{N} \\sum_{i,j} \\alpha_i\\alpha_j K_{ij} + \\frac{1}{N} \\sum_{i = 1}^N (\\sum_{j = 1}^N \\alpha_j K_{ij} - y_i)^2   $$  \n",
    "write in matrix form  \n",
    "$$\\text{min}_{\\alpha \\in R} \\frac{\\lambda}{N} \\alpha^T K \\alpha + \\frac{1}{N} ((K\\alpha)^T(K\\alpha) - 2y^TK\\alpha + y^Ty) $$  \n",
    "We can solve this by setting its gradient to 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\partial J(\\alpha) = \\frac{2 \\lambda}{N}(K^T \\alpha) + \\frac{2}{N}(K^T K \\alpha - K^T y)$$  \n",
    "set this result to 0  \n",
    "$$\\alpha = (\\lambda I + K)^{-1}y$$  \n",
    "Notice: since K is psd and add a positive term $\\lambda I$, which means the inverse of $\\lambda I + K$ exists  \n",
    "time complexity to calculate the inverse matrix is $O(N^3)$\n",
    "- Compare this with linear ridge regression  \n",
    "linear ridge regression:  \n",
    "$$\\mathbf{w}=\\left(\\lambda \\mathbf{I}+\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{T} \\mathbf{y}$$  \n",
    "   - More restricted  \n",
    "   - $O(d^3 + d^2 N)$ training  \n",
    "   - $O(d)$ prediction\n",
    "\n",
    "Kernel ridge regression:  \n",
    "$$\\boldsymbol{\\alpha}=(\\lambda \\mathrm{I}+\\mathrm{K})^{-1} \\mathbf{y}$$  \n",
    "   - More **flexible** with $K$  \n",
    "   - $O(N^3)$ training  \n",
    "   - $O(N)$ prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel ridge regression for classification  \n",
    "- least-squares SVM(LSSVM)  \n",
    "<div align=\"center\"><img src = \"./LSSVM.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "The boundaries are similar, but more support vectors in the right graphs, which means $\\alpha$ is more dense and the prediction becomes slower.    \n",
    "\n",
    "- **Tube Regression**  \n",
    "<div align=\"center\"><img src = \"./tube regression.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "We consider to set a neutral zone, and we treat the point inside such zone as errorless, and if the points lie outside, we consider its distance to the zone.  \n",
    "  - The error measure($\\epsilon$-insensitive error)  \n",
    "    $$l(y,\\hat{y}) = \\text{max}(0, |y - \\hat{y}| - \\epsilon)$$  \n",
    "- **Tube error vs Squared error**  \n",
    "<div align=\"center\"><img src = \"./Tube and Square.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "As observed from graphs, we learn that when $|y - \\hat{y}|$ is small, the two graphs are similar, while when $|y - \\hat{y}|$ is larger, tube error is less influenced by outliers.  \n",
    "\n",
    "- L2 regularization with tube regression  \n",
    "$$\\min \\frac{1}{2}w^Tw + C \\sum_{i = 1}^N \\max (0, |y_i - w^Tx_i - b| - \\epsilon)$$  \n",
    "try to solve it using the similar way of SVM  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming it to constrained optimization problem  \n",
    "$$\\begin{array}{l}\n",
    "\\frac{1}{2} \\mathbf{w}^{T} \\mathbf{w}+C \\sum_{n=1}^{N}\\left(\\xi_{i}^{\\vee}+\\xi_{i}^{\\wedge}\\right) \\\\\n",
    "-\\epsilon-\\xi_{i}^{\\vee} \\leq y_{i}-\\mathbf{w}^{T} \\mathbf{x}_{i}-b \\leq \\epsilon+\\xi_{i}^{\\wedge} \\\\\n",
    "\\xi_{i}^{\\vee} \\geq 0, \\xi_{i}^{\\wedge} \\geq 0\n",
    "\\end{array}$$  \n",
    "Just think what we have done in SVM, here we should contrive a method to eliminate the $|\\cdot|$, therefore use $\\xi_{i}^{\\wedge}$ to illustrate the upper error measurement and $\\xi_{i}^{\\vee}$ the lower error measurement   \n",
    "and this is a formal **Support Vector Regression(SVR) primal problem**  \n",
    "\n",
    "- Parameters  \n",
    "C: A trade-off of regularization and tube violation  \n",
    "$\\epsilon$：how much we can bear for the error  \n",
    "\n",
    "\n",
    "- SVR Dual  \n",
    "we first write it in Lagrangian form:  \n",
    "$$L(w,b,\\xi^{\\wedge}, \\xi^{\\vee}, \\alpha_{\\wedge}, \\alpha_{\\vee}, \\lambda^{\\wedge}, \\lambda^{\\vee}) = \\frac{1}{2} w^Tw + C\\sum_{i =1}^N (\\xi_{i}^{\\wedge}+ \\xi_{i}^{\\vee}) + \\sum_{i =1}^N \\alpha_{i}^{\\wedge} (y_i -w^T x_i -b -\\epsilon - \\xi_{i}^{\\wedge}) + \\sum_{i = 1}^N \\alpha_{i}^{\\vee} (-y_i + w^Tx_i +b -\\epsilon -\\xi_{i}^{\\vee}) + \\sum_{i = 1}^N \\lambda_i^{\\wedge} (-\\xi^{\\wedge}_{i}) + \\lambda_i^{\\vee} (-\\xi^{\\vee}_{i})$$  \n",
    "\n",
    "Then we can write the primal and dual form  \n",
    "$$\\begin{aligned}\n",
    "p^{*} &=\\inf _{w, \\xi^{\\wedge}, \\xi_{\\vee}, b} \\sup _{\\alpha^{\\wedge}, \\alpha^{\\vee}, \\lambda \\succeq 0} L(w,b,\\xi^{\\wedge}, \\xi^{\\vee}, \\alpha_{\\wedge}, \\alpha_{\\vee}, \\lambda) \\\\\n",
    "& \\geqslant \\sup _{\\alpha^{\\wedge}, \\alpha^{\\vee}, \\lambda \\succeq 0} \\inf _{w, \\xi^{\\wedge}, \\xi_{\\vee}, b} L(w,b,\\xi^{\\wedge}, \\xi^{\\vee}, \\alpha_{\\wedge}, \\alpha_{\\vee}, \\lambda)=d^{*}\n",
    "\\end{aligned}$$  \n",
    "\n",
    "we let  \n",
    "$$\n",
    "g(w, \\xi^{\\wedge}, \\xi_{\\vee}, b) = \\inf _{w, \\xi^{\\wedge}, \\xi_{\\vee}, b} L(w,b,\\xi^{\\wedge}, \\xi^{\\vee}, \\alpha_{\\wedge}, \\alpha_{\\vee}, \\lambda)\n",
    "$$\n",
    "calculating the deravatives:  \n",
    "$$\\begin{array}{l}\n",
    "\\partial_{w} L=0 \\quad \\Longleftrightarrow \\quad w-\\sum_{i=1}^{n} (\\alpha_{i}^{\\wedge} - \\alpha_{i}^{\\vee}) x_{i}=0 \\\\\n",
    "\\partial_{b} L=0 \\quad \\Longleftrightarrow \\quad-\\sum_{i=1}^{n} (\\alpha_{i}^{\\wedge} - \\alpha_i^{\\vee}) =0 \\quad \\\\\n",
    "\\partial_{\\xi_{i}^{\\wedge}} L=0 \\quad \\Longleftrightarrow \\quad C - \\alpha_{i}^{\\wedge} -\\lambda_{i}^{\\wedge}=0  \\\\\n",
    "\\partial_{\\xi_{i}^{\\vee}} L=0 \\quad \\Longleftrightarrow \\quad C - \\alpha_{i}^{\\vee} -\\lambda_{i}^{\\vee}=0\n",
    "\\end{array}$$  \n",
    "we can find the solution is quite similar as that of SVM  \n",
    "we also have slack complementary for the **optimal solution**  \n",
    "$$\\begin{array}{l}\n",
    "\\alpha_{i}^{\\wedge}\\left(\\epsilon+\\xi_{i}^{\\wedge}-y_{i}+\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)=0 \\\\\n",
    "\\alpha_{i}^{\\vee}\\left(\\epsilon+\\xi_{i}^{\\vee}+y_{i}-\\mathbf{w}^{T} \\mathbf{x}_{i}-b\\right)=0 \\\\  \n",
    "\\lambda_{i}^{\\wedge}\\xi_{i}^{\\wedge} = 0 \\\\  \n",
    "\\lambda_{i}^{\\vee}\\xi_{i}^{\\vee} = 0\n",
    "\\end{array}$$  \n",
    "for all $i \\in [1,N]$  \n",
    "\n",
    "We then have:  \n",
    "$$\\begin{array}{l}\n",
    "g(w, \\xi^{\\wedge}, \\xi_{\\vee}, b) = -\\frac{1}{2} \\sum_{i, j=1}^{N}\\left(\\alpha_{i}^{\\wedge}-\\alpha_{i}^{\\vee}\\right)\\left(\\alpha_{j}^{\\wedge}-\\alpha_{j}^{\\vee}\\right) \\underbrace{\\Phi\\left(\\mathbf{x}_{i}\\right)^{T} \\Phi\\left(\\mathbf{x}_{j}\\right)}_{k\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)} \\\\\n",
    "-\\epsilon \\sum_{i=1}^{N}\\left(\\alpha_{i}^{\\wedge}+\\alpha_{i}^{\\vee}\\right)+\\sum_{i=1}^{N} y_{i}\\left(\\alpha_{i}^{\\wedge}-\\alpha_{i}^{\\vee}\\right)\n",
    "\\end{array}\n",
    "$$  \n",
    "Now the problem becomes:  \n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha^{\\wedge}, \\alpha^{\\vee}, \\lambda} g(w, \\xi^{\\wedge}, \\xi_{\\vee}, b) \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{N}\\left(\\alpha_{i}^{\\wedge}-\\alpha_{i}^{\\vee}\\right)=0 \\quad \\alpha_{i}^{\\vee}, \\alpha_{i}^{\\wedge} \\in[0, C] \\quad \\forall i \\in[1, N]\n",
    "\\end{array}$$  \n",
    "\n",
    "Therefore, if the result is inside the tube $\\xi^{\\wedge}, \\xi^{\\vee}$ are 0, which indicates that $\\alpha^{\\wedge}, \\alpha^{\\vee}$ are 0 from complementary slackness, which means $w$ is determinded by those sample outside the tube   \n",
    "\n",
    "- Using kernel trick, the prediction function becomes:  \n",
    "$$f(x) = \\sum_{i=1}^{n} (\\alpha_{i}^{\\wedge} - \\alpha_{i}^{\\vee}) \\Phi(x_{i})^T \\Phi(x) + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas of Gaussian Kernel  \n",
    "$$k_{\\sigma}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\|^{2}}{2 \\sigma^{2}}\\right)=\\exp \\left(-\\frac{\\mathbf{x}^{T} \\mathbf{x}-2 \\mathbf{x}^{T} \\mathbf{x}^{\\prime}+\\mathbf{x}^{\\prime T} \\mathbf{x}^{\\prime}}{2 \\sigma^{2}}\\right)$$  \n",
    "- Depends on a width parameter $\\sigma$  \n",
    "- The smaller the width, the more prediction on a point only depends on its nearest neighbours   \n",
    "-  Example of *Universal* kernel: they can uniformly approximate any arbitrary continuous target function (pb of number of training examples and choice of $\\sigma$)\n",
    "\n",
    "## Kernels on structured data  \n",
    "- Kernels are generalization of dot products to arbitrary domains   \n",
    "- It is possible to design kernels over structured objects like sequences, trees or graphs   \n",
    "- The idea is designing a pairwise function measuring the similarity of two objects   \n",
    "- This measure has to sastisfy the p.d. conditions to be a valid kernel\n",
    "\n",
    "### E.g. string kernel: 3-gram spectrum kernel  \n",
    "<div align=\"center\"><img src = \"./3-gram.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Kernels  \n",
    "- decomposition kernels deﬁning a kernel as the convolution of its parts:   \n",
    "$$\\left(k_{1} \\star \\cdots \\star k_{D}\\right)\\left(x, x^{\\prime}\\right)=\\sum_{\\left(x_{1}, \\ldots, x_{D}\\right) \\in R(x)\\left(x_{1}^{\\prime}, \\ldots, x_{D}^{\\prime}\\right) \\in R\\left(x^{\\prime}\\right)} \\prod_{d=1}^{D} k_{d}\\left(x_{d}, x_{d}^{\\prime}\\right)$$  \n",
    "- where the sums run over all possible decompositions of $x$ and $x'$.\n",
    " \n",
    "### Set Kernels  \n",
    "- Let $R(x)$ be the set membership relationship (written as $\\in$)  \n",
    "- Let $k_{member}(\\xi,\\xi_{0})$ be a kernel deﬁned over set elements \n",
    "- The set kernel is deﬁned as:\n",
    "$$k_{s e t}\\left(X, X^{\\prime}\\right)=\\sum_{\\xi \\in X} \\sum_{\\xi^{\\prime} \\in X^{\\prime}} k_{m e m b e r}\\left(\\xi, \\xi^{\\prime}\\right)$$  \n",
    "\n",
    "## Kernel normalization  \n",
    "-  Kernel values can often be inﬂuenced by the dimension of objects   \n",
    "- E.g. a longer string has more substrings → higher kernel value  \n",
    "- This effect can be reduced *normalizing* the kernel\n",
    "\n",
    "### Cosine normalization\n",
    "Cosine normalization computes the cosine of the dot product in feature space:\n",
    "$$\\hat{k}\\left(x, x^{\\prime}\\right)=\\frac{k\\left(x, x^{\\prime}\\right)}{\\sqrt{k(x, x) k\\left(x^{\\prime}, x^{\\prime}\\right)}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
