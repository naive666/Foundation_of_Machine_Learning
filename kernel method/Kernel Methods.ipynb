{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation  \n",
    "## The Input Space $X$  \n",
    "- Generally, no assumptions on $X$  \n",
    "- But $X = R^d$ is for the specific methods we have developed:  \n",
    "  - Ridge Regression\n",
    "  - Lasso Regression\n",
    "  - Linear SVM  \n",
    "- Our hypothesis space is $H = \\{x \\mapsto w^tx + b | w \\in R^d, b\\in R\\}$  \n",
    "- What if the input space is not $R^d$?  \n",
    "\n",
    "# Feature Extraction  \n",
    "- Definition  \n",
    "Mapping an input space $X$ to $R^d$ is called feature extraction or featurization.  \n",
    "\n",
    "- Geometric Example: Two class problem, nonlinear boundary\n",
    "<div align=\"center\"><img src = \"./nolinear_boundary.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "  - With linear feature map $\\psi(x) = (x_1,x_2)$ and linear models, can’t separate regions  \n",
    "  - With appropriate nonlinearity $\\psi(x) = (x_1, x_2, x_1^2 + x_2 ^2)$  \n",
    "  - Consider a linear hypothesis space with a feature map $\\phi: X \\to R^d$  \n",
    "  $$F = \\{f(x) = w^T\\phi(x) \\}$$  \n",
    "<div align=\"center\"><img src = \"./predictor.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Linear Models Need Big Feature Spaces\n",
    "- To get expressive hypothesis spaces using linear models, need high-dimensional feature spaces   \n",
    "- Suppose we start with $x = (1,x_1,...,x_d)\\in R^{d+1} = X$.  \n",
    "- We want to add all monomials of degree $M: x_{1}^{p_{1}} \\cdots x_{d}^{p_{d}},$ with $p_{1}+\\cdots+p_{d}=M$  \n",
    "- How many features will we end up with?   \n",
    "- $\\left(\\begin{array}{c}M+d-1 \\\\ M\\end{array}\\right)$\n",
    "- For $d = 4$, $M = 8$, we get 314457495 features  \n",
    "\n",
    "## Big Feature Spaces\n",
    "Very large feature spaces have two problems:  \n",
    "- Overfitting  \n",
    "- Memory and computational cost  \n",
    "We may use \"Kernel Method\" to handle memory and computational cost problem  \n",
    "\n",
    "# Kernel Methods  \n",
    "\n",
    "## Motivation  \n",
    "The featurized SVM prediction function is the solution to\n",
    "$$\\min _{w \\in \\mathbf{R}^{d}, b \\in \\mathbf{R}} \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n}\\left(1-y_{i}\\left[w^{T} \\psi\\left(x_{i}\\right)+b\\right]\\right)_{+}$$  \n",
    "\n",
    "Found it is equivalent to solve the dual problem to get $\\alpha^*$:\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha} \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\psi\\left(x_{j}\\right)^{T} \\psi\\left(x_{i}\\right) \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\\\\n",
    " \\alpha_{i} \\in\\left[0, \\frac{c}{n}\\right] i=1, \\ldots, n\n",
    "\\end{array}$$  \n",
    "\n",
    "## Some Methods Can Be “Kernelized”\n",
    "- Definition  \n",
    "A method is **kernelized** if inputs only appear inside inner products: $\\left\\langle\\psi(x), \\psi\\left(x^{\\prime}\\right)\\right\\rangle$ for $x, x^{\\prime} \\in X$  \n",
    "- The kernel function corresponding to $\\psi$ and inner product is  \n",
    "$$k\\left(x, x^{\\prime}\\right)=\\left\\langle\\psi(x), \\psi\\left(x^{\\prime}\\right)\\right\\rangle$$  \n",
    "- Why introduce this new notation $k(x,x_0)$?  \n",
    "- We can often evaluate $k(x,x_0)$ directly, without calculating $\\psi(x) $ and $\\psi(x')$\n",
    "- For large feature spaces, can be much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Evaluation Can Be Fast\n",
    "- Example:  \n",
    "Quadratic feature map for $x = (x_1, ... , x_d) \\in R^d$  \n",
    "$$\\phi(x)=\\left(x_{1}, \\ldots, x_{d}, x_{1}^{2}, \\ldots, x_{d}^{2}, \\sqrt{2} x_{1} x_{2}, \\ldots, \\sqrt{2} x_{i} x_{j}, \\ldots \\sqrt{2} x_{d-1} x_{d}\\right)^{T}$$  \n",
    "has dimention $O(d^2)$, but for any $x, x' \\in R^d$,  \n",
    "$$k\\left(x, x^{\\prime}\\right)=\\left\\langle\\phi(x), \\phi\\left(x^{\\prime}\\right)\\right\\rangle=\\left\\langle x, x^{\\prime}\\right\\rangle+\\left\\langle x, x^{\\prime}\\right\\rangle^{2}$$  \n",
    "Naively explicit computation of  $k\\left(x, x^{\\prime}\\right): O\\left(d^{2}\\right)$   \n",
    "\n",
    "Implicit computation of $k\\left(x, x^{\\prime}\\right): O\\left(d\\right)$   \n",
    "\n",
    "## Kernels as Similarity Scores\n",
    "- Often useful to think of the kernel function as a **similarity score**  \n",
    "- But this is not a mathematically precise statement  \n",
    "- There are many ways to design a similarity score.  \n",
    "  - We will use Mercer kernels, which correspond to inner products in some feature space. \n",
    "  - Has many mathematical beneﬁts.\n",
    "\n",
    "## What are the Beneﬁts of Kernelization?\n",
    "- Computational (e.g. when feature space dimension $d$ larger than sample size $n$).  \n",
    "- Access to infinite-dimensional feature spaces  \n",
    "- Allows thinking in terms of “similarity” rather than features.\n",
    "\n",
    "## Example: SVM  \n",
    "Recall the SVM dual optimization problem for training set $(x_1,y_1),...,(x_n,y_n)$:  \n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha} \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i} \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\\\\n",
    " \\alpha_{i} \\in\\left[0, \\frac{c}{n}\\right] i=1, \\ldots, n\n",
    "\\end{array}$$    \n",
    "we can replace $x_j^Tx_i$ by $k(x_j, x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel  \n",
    "- Input Space: $X = R^d$  \n",
    "- Feature Space: $H = R^d$  \n",
    "- Feature map: $\\psi(x) = x$  \n",
    "- kernel: $k(x,x') = x^Tx'$  \n",
    "\n",
    "### The kernel matrix (Gram Matrix)  \n",
    "- Definition\n",
    "For points of $x_1, ... ,x_n \\in X$, and an inner product on $X$, the kernel matrix is defined as:  \n",
    "$$K=\\left(\\left\\langle x_{i}, x_{j}\\right\\rangle\\right)_{i, j}=\\left(\\begin{array}{ccc}\n",
    "\\left\\langle x_{1}, x_{1}\\right\\rangle & \\cdots & \\left\\langle x_{1}, x_{n}\\right\\rangle \\\\\n",
    "\\vdots & \\ddots & \\ldots \\\\\n",
    "\\left\\langle x_{n}, x_{1}\\right\\rangle & \\cdots & \\left\\langle x_{n}, x_{n}\\right\\rangle\n",
    "\\end{array}\\right)$$  \n",
    "Then for the standard Euclidean inner product $\\left\\langle x_{i}, x_{j}\\right\\rangle=x_{i}^{T} x_{j}$, we have  \n",
    "$$K = XX^T$$  \n",
    "\n",
    "### SVM Dual with Kernel Matrix\n",
    "$$\\begin{array}{ll}\n",
    "\\sup _{\\alpha} \\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K_{j i} \\\\\n",
    "\\text { s.t. }  \\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\\\\n",
    " \\alpha_{i} \\in\\left[0, \\frac{c}{n}\\right] i=1, \\ldots, n\n",
    "\\end{array}$$  \n",
    "\n",
    "- Once our algorithm works with kernel matrices, we can change kernel just by changing the matrix  \n",
    "- Size of matrix: $n×n$, where $n$ is the number of data points  \n",
    "- Recall with ridge regression, we worked with $X^TX$, which is $d×d$, where $d$ is feature space dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Nonlinear Kernels  \n",
    "\n",
    "### Quadratic Kernel in $R^d$  \n",
    "- Input space: $X = R^d$\n",
    "- Feature space: $H = R^D$, where $D = d + \\left(\\begin{array}{l}\n",
    "d \\\\\n",
    "2\n",
    "\\end{array}\\right) \\approx d^{2} / 2$  \n",
    "- Feature map:  \n",
    "$$\\phi(x)=\\left(x_{1}, \\ldots, x_{d}, x_{1}^{2}, \\ldots, x_{d}^{2}, \\sqrt{2} x_{1} x_{2}, \\ldots, \\sqrt{2} x_{i} x_{j}, \\ldots \\sqrt{2} x_{d-1} x_{d}\\right)^{T}$$  \n",
    "\n",
    "- Then for all $x, x' \\in R^d$, \n",
    "$$\\begin{aligned}\n",
    "k\\left(x, x^{\\prime}\\right) &=\\left\\langle\\phi(x), \\phi\\left(x^{\\prime}\\right)\\right\\rangle \\\\\n",
    "&=\\left\\langle x, x^{\\prime}\\right\\rangle+\\left\\langle x, x^{\\prime}\\right\\rangle^{2}\n",
    "\\end{aligned}$$  \n",
    "\n",
    "### Polynomial Kernel in $R^d$  \n",
    "- Kernel function  \n",
    "$$k\\left(x, x^{\\prime}\\right)=\\left(1+\\left\\langle x, x^{\\prime}\\right\\rangle\\right)^{M}$$\n",
    "- For any $M$, computing the kernel has same computational cost   \n",
    "\n",
    "## Radial Basis Function(RBF) Kernel /  Gaussian Kernel\n",
    "  \n",
    "- Input space $X= R^d$, for all $x,x' \\in R^d$,  \n",
    "$$k(x, x')=\\exp \\left(-\\frac{\\left\\|x-x^{\\prime}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$$  \n",
    "where $\\sigma^2$ is known as the bandwidth parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Tricks  \n",
    "\n",
    "## The “Kernel Trick”\n",
    "-  Given a kernelized ML algorithm  \n",
    "- Can swap out the inner product for a new kernel function  \n",
    "- New kernel may correspond to a high dimensional feature space.  \n",
    "- Once kernel matrix is computed, computational cost depends on number of data points, rather than the dimension of feature space.  \n",
    "Swapping out a linear kernel for a new kernel is called the **kernel trick**  \n",
    "\n",
    "# Inner Product Spaces and Projections (Hilbert Space)  \n",
    "\n",
    "## Inner Product Space (or “Pre-Hilbert” Spaces)\n",
    "- An inner product space (over reals) is a vector space $V$ and an inner product, which is a mapping  \n",
    "$$\\langle\\cdot, \\cdot\\rangle: \\mathcal{V} \\times \\mathcal{V} \\rightarrow \\mathbf{R}$$  \n",
    "that has the following properties $\\forall x,y,z \\in V$ and $a,b \\in R$  \n",
    "- Symmetry: $<x,y> = <y,x>$  \n",
    "- Linearity: $<ax+by, z> = a<x,z> + b<y,z>$  \n",
    "- Positive-definiteness: $<x,x> \\geqslant 0 $, and $<x,x> = 0 \\Leftrightarrow x = 0$  \n",
    "\n",
    "## Norm from Inner Product\n",
    "For an inner product space, we deﬁne a norm as   \n",
    "$$\\|x\\|=\\sqrt{\\langle x, x\\rangle}$$  \n",
    "\n",
    "- Theorem (Parallelogram Law)   \n",
    "A norm $\\|\\cdot\\|$ can be written in terms of an inner product on $V$ iff $\\forall x,x' \\in V$,  \n",
    "$$2\\|x\\|^{2}+2\\left\\|x^{\\prime}\\right\\|^{2}=\\left\\|x+x^{\\prime}\\right\\|^{2}+\\left\\|x-x^{\\prime}\\right\\|^{2}$$  \n",
    "and if it can, the inner product is given by the **polarization identity**  \n",
    "$$\\left\\langle x, x^{\\prime}\\right\\rangle=\\frac{\\|x\\|^{2}+\\left\\|x^{\\prime}\\right\\|^{2}-\\left\\|x-x^{\\prime}\\right\\|^{2}}{2}$$  \n",
    "\n",
    "## Pythagorean Theorem\n",
    "- Definition  \n",
    "Two vectors are orthogonal if $<x,x'>= 0$. We denote this by $x ⊥x'$  \n",
    "- Deﬁnition\n",
    "$x$ is orthogonal to a set $S$, i.e. $x ⊥ S$, if $x ⊥ s$ for all $x \\in S$  \n",
    "- Theorem (Pythagorean Theorem)   \n",
    "If $x ⊥ s$, then $\\left\\|x+x^{\\prime}\\right\\|^{2}=\\|x\\|^{2}+\\left\\|x^{\\prime}\\right\\|^{2}$  \n",
    "\n",
    "## Projection onto a Plane  \n",
    "- Choose some $x \\in V$.  \n",
    "- Let $M$ be a subspace of inner product space $V$. \n",
    "- Then $m_0$ is the projection of $x$ onto $M$\n",
    "\n",
    "## Hilbert Space\n",
    "- Projections exist for all ﬁnite-dimensional inner product spaces. \n",
    "- We want to allow inﬁnite-dimensional spaces. \n",
    "- Need an extra condition called completeness. \n",
    "- A space is complete if all Cauchy sequences in the space converge  \n",
    "- Definition  \n",
    "A Hilbert space is a complete inner product space  \n",
    "Any ﬁnite dimensional inner product space is a Hilbert space  \n",
    "\n",
    "## The Projection Theorem\n",
    "- Theorem (Classical Projection Theorem) \n",
    "   - $H$ a Hilbert space \n",
    "   - $M$ a closed subspace of $H$ (picture a hyperplane through the origin) \n",
    "   - For any $x \\in H$, there exists a unique $m_0 \\in M$ for which   \n",
    "   $$\\left\\|x-m_{0}\\right\\| \\leqslant\\|x-m\\| \\forall m \\in M$$  \n",
    "   - Then $m_0$ **orthogonal** projection of $x$ onto $M$.  \n",
    "   - Furthermore, $m_0 \\in M$ is the projection of $x$ onto $M$ iff   \n",
    "   $$x-m_{0} \\perp M$$  \n",
    "   \n",
    "## Projection Reduces Norm\n",
    "- Theorem  \n",
    "Let $M$ be a closed subspace of $H$. For any $x \\in H$, let $m_0 =Proj_{M}x$ be the projection of $x$ onto $M$. Then   \n",
    "$$\\left\\|m_{0}\\right\\| \\leqslant\\|x\\|$$  \n",
    "with equality only when $m_0 = x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representer Theorem  \n",
    "## Generalize from SVM Objective  \n",
    "- Featurized SVM objective:\n",
    "$$\\min _{w \\in \\mathbf{R}^{d}} \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n} \\max \\left(0,1-y_{i}\\left[\\left\\langle w, \\psi\\left(x_{i}\\right)\\right\\rangle\\right]\\right)$$\n",
    "- **Generalized objective**  \n",
    "$$\\min _{w \\in \\mathcal{H}} R(\\|w\\|)+L\\left(\\left\\langle w, \\psi\\left(x_{1}\\right)\\right\\rangle, \\ldots,\\left\\langle w, \\psi\\left(x_{n}\\right)\\right\\rangle\\right)$$  \n",
    "where  \n",
    "  - $R : R_{\\geq 0} \\to R$ is nondecreasing (Regularization term)  \n",
    "  - $L : R^n \\to R$ is arbitrary (Loss term)  \n",
    "  - $w, \\psi\\left(x_{1}\\right), \\ldots, \\psi\\left(x_{n}\\right) \\in \\mathcal{H}$ for some HIlbert space $H$, we typically have $(H = R^d)$\n",
    "  - $\\|w\\|=\\sqrt{\\langle w, w\\rangle}$\n",
    "- What is linear?  \n",
    "  - The prediction/score function $x \\to <w, \\psi(x_i)>$ in parameter vector $w$ and feature vector $\\psi(x_i) $  \n",
    "  - The important part is the **linearity in the parameter w**.\n",
    "  - When we discuss neural networks, we’ll mention a “linear network” in which prediction functions are linear in the feature vector $\\psi(x)$, but nonlinear in the parameter vector $w$. In other words, we have something like \n",
    " $$\\min _{w \\in \\mathcal{H}} R(\\|w\\|)+L\\left(\\left\\langle f(w), \\psi\\left(x_{1}\\right)\\right\\rangle, \\ldots,\\left\\langle f(w), \\psi\\left(x_{n}\\right)\\right\\rangle\\right)$$  \n",
    "  - What if we penalize with $\\lambda \\|w\\|_2$ instead of $\\lambda \\|w\\|_2^2 $? Yes!  \n",
    "  - What if we use lasso regression? No! $l_1$ norm does not correspond to an inner product\n",
    "\n",
    "## The Representer Theorem\n",
    "Let  \n",
    "$$J(w)=R(\\|w\\|)+L\\left(\\left\\langle w, \\psi\\left(x_{1}\\right)\\right\\rangle, \\ldots,\\left\\langle w, \\psi\\left(x_{n}\\right)\\right\\rangle\\right)$$  \n",
    "If $J(w)$ has a minimizer, then it has the form $w^* = \\sum_{i = 1}^n \\alpha_i\\psi(x_i)$  \n",
    "If $R$ is strictly increasing, then all minimizers have this form  \n",
    "\n",
    "**proof**  \n",
    "We consider if $w^*$ does not lie in the span space of $\\{\\psi(x_i)\\}$, then its projection on that span space is also a minimizer, and we therefore get contradiction\n",
    "Let $w^*$ be the minimizer, and $M = span(\\psi(x_1),..,\\psi(x_n))$ --- the span of data. Let $w = Proj_Mw^*$, so $\\exists w = \\sum_{i = 1}^n \\alpha_i \\psi(x_i)$, then $$w^{\\perp}:=w^{*}-w$$ is orthogonal to $M$.  \n",
    "Projections decrease norms, then $\\|w\\| \\leqslant \\|w^*\\|$, and since $R$ is nondecreasing, we have $R(\\|w\\|) \\leqslant R(\\|w^*\\|)$  \n",
    "and we have $\\left\\langle w^{*}, \\psi\\left(x_{i}\\right)\\right\\rangle=\\left\\langle w+w^{\\perp}, \\psi\\left(x_{i}\\right)\\right\\rangle=\\left\\langle w, \\psi\\left(x_{i}\\right)\\right\\rangle$  \n",
    "we also have  \n",
    "$$L\\left(\\left\\langle w^{*}, \\psi\\left(x_{1}\\right)\\right\\rangle, \\ldots,\\left\\langle w^{*}, \\psi\\left(x_{n}\\right)\\right\\rangle\\right)=L\\left(\\left\\langle w, \\psi\\left(x_{1}\\right)\\right\\rangle, \\ldots,\\left\\langle w, \\psi\\left(x_{n}\\right)\\right\\rangle\\right)$$  \n",
    "thus, $J(w) \\leqslant J(w^*)$, and $w$ is also a minimizer. Hence, $w^*$ lies in the span space of $\\{\\psi(x_i)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Prediction  \n",
    "- Consider $w = \\sum_{i = 1}^n \\alpha_i \\psi(x_i)$  \n",
    "- How do we make prediction of a given $x \\in X$?  \n",
    "    \n",
    "$$\\begin{aligned}\n",
    "f(x) = <w, \\psi(x)>  &=  <\\sum_{i = 1}^n \\alpha_i \\psi(x_i), \\psi(x)>\\\\\n",
    "&= \\sum_{i = 1}^n \\alpha_i <\\psi(x_i), \\psi(x)>  \\\\  \n",
    "&= \\sum_{i = 1}^n \\alpha_i k(x_i, x)\n",
    "\\end{aligned}$$   \n",
    "\n",
    "Note: $f(x)$ is a combination of $k(x_1,x),.., k(x_n, x)$, all considered as functions of $x$.   \n",
    "\n",
    "- Write $f_{\\alpha} = \\sum_{i = 1}^n \\alpha_i k(x_i, x)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Regularization  \n",
    "- What does $R(\\|w\\|)$ look like?  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|w\\|^2 &= <w,w> \\\\  \n",
    "&= <\\sum_{i = 1}^n \\alpha_i \\psi(x_i), \\sum_{i = 1}^n \\alpha_i \\psi(x_i)> \\\\  \n",
    "&= \\sum_{i,j = 1}^n \\alpha_i \\alpha_j <\\psi(x_i), \\psi(x_j)>  \\\\  \n",
    "&= \\sum_{i,j = 1}^n \\alpha_i \\alpha_j k(x_i,x_j) \\\\  \n",
    "&= \\alpha^T K \\alpha\n",
    "\\end{aligned}\n",
    "$$  \n",
    "where $K$ is the Kernel Matrix  \n",
    "- So $R(\\|w\\|) = \\sqrt{\\alpha^T K \\alpha}$  \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left(\\begin{array}{c}\n",
    "f_{\\alpha}\\left(x_{1}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "f_{\\alpha}\\left(x_{n}\\right)\n",
    "\\end{array}\\right) &=\\left(\\begin{array}{ccc}\n",
    "\\alpha_{1} k\\left(x_{1}, x_{1}\\right)+\\cdots+\\alpha_{n} k\\left(x_{1}, x_{n}\\right) \\\\\n",
    "& \\vdots \\\\\n",
    "\\alpha_{1} k\\left(x_{n}, x_{1}\\right)+\\cdots+\\alpha_{n} k\\left(x_{n}, x_{n}\\right)\n",
    "\\end{array}\\right) \\\\\n",
    "&=\\left(\\begin{array}{ccc}\n",
    "k\\left(x_{1}, x_{1}\\right) & \\cdots & k\\left(x_{1}, x_{n}\\right) \\\\\n",
    "\\vdots & \\ddots & \\ldots \\\\\n",
    "k\\left(x_{n}, x_{1}\\right) & \\cdots & k\\left(x_{n}, x_{n}\\right)\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\alpha_{1} \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_{n}\n",
    "\\end{array}\\right) \\\\\n",
    "&=K \\alpha\n",
    "\\end{aligned}$$  \n",
    "Now, our goal is to find the best coefficient $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Objective  \n",
    "- Substituting $w = \\sum_{i = 1}^n \\alpha_i \\psi(x_i)$ into generalized objective, we got:  \n",
    "$$\\min _{\\alpha \\in \\mathbf{R}^{n}} R(\\sqrt{\\alpha^{T} K \\alpha})+L(K \\alpha)$$  \n",
    "- All references are via kernel matrix $K$  \n",
    "- This is called kernelized objective function.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized SVM  \n",
    "- SVM objectives\n",
    "$$\\min _{w \\in \\mathcal{H}} \\frac{1}{2}\\|w\\|^{2}+\\frac{c}{n} \\sum_{i=1}^{n}\\left(1-y_{i}\\left[\\left\\langle w, \\psi\\left(x_{i}\\right)\\right\\rangle\\right]\\right)_{+}$$  \n",
    "\n",
    "- Kenelization yields  \n",
    "$$\\min _{\\alpha \\in \\mathrm{R}^{n}} \\frac{1}{2} \\alpha^{T} K \\alpha+\\frac{c}{n} \\sum_{i=1}^{n}\\left(1-y_{i}(K \\alpha)_{i}\\right)_{+}$$  \n",
    "### Kernelized Ridge Regression\n",
    "- Ridge Regression:  \n",
    "$$\\min _{w \\in \\mathbf{R}^{d}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(w^{T} x_{i}-y_{i}\\right)^{2}+\\lambda\\|w\\|^{2}$$  \n",
    "- Featurized Ridge Regression  \n",
    "$$\\min _{w \\in \\mathbf{R}^{d}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(w^{T} \\psi(x_{i})-y_{i}\\right)^{2}+\\lambda\\|w\\|^{2}$$  \n",
    "- Kernelized Ridge Regression  \n",
    "$$\\min _{\\alpha \\in \\mathbf{R}^{n}} \\frac{1}{n}\\|K \\alpha-y\\|^{2}+\\lambda \\alpha^{T} K \\alpha$$  \n",
    "where $y = (y_1, ... , y_n)^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Functions with RBF Kernel  \n",
    "\n",
    "## RBF Basis  \n",
    "- Input Space: X = R  \n",
    "- Output Space y = R  \n",
    "- RBF kernel: $k(w,x) = e^{-(w - x)^2}$  \n",
    "- Suppose we have 6 training examples: $x_i \\in \\{-6,-4,-3,0,2,4\\}$  \n",
    "- If Representer Theorem applies, then  \n",
    "$$f(x) = \\sum_{i=1}^6 \\alpha_i k(x,x_i)$$  \n",
    "- $f$ is a linear combination of 6 basis functions of form $k(x_i, \\cdot)$  \n",
    "<div align=\"center\"><img src = \"./RBF.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "- Predictions of the form $f(x) = \\sum_{i=1}^6 \\alpha_i k(x,x_i)$  \n",
    "<div align=\"center\"><img src = \"./rbf_sum.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "When kernelizing with RBF kernel, prediction functions always look this way  \n",
    "\n",
    "## RBF Feature Space: The Sequence Space $l_2$  \n",
    "- To work with inﬁnite dimensional feature vectors, we need a space with certain properties  \n",
    "  - An inner product  \n",
    "  - a norm related to the inner product  \n",
    "  - projection theorem: $x=x_{\\perp}+x_{\\|}$, where $x_{\\|} \\in S = \\text{span}(w_1,...,w_n)$ and $<x_{\\perp}, s> = 0, \\forall s \\in S$\n",
    "Basically, we need a Hilbert Space  \n",
    "$l_2$ is the space of all real-valued sequences $\\{A_n : \\sum_n A_n^2 < \\infty\\}$   \n",
    "\n",
    "Theorem  \n",
    "With the the inner product $\\left\\langle x, x^{\\prime}\\right\\rangle=\\sum_{i=0}^{\\infty} x_{i} x_{i}^{\\prime}$, $l_2$ space is a Hilbert space  \n",
    "\n",
    "## The Inﬁnite Dimensional Feature Vector for RBF   \n",
    "- Consider RBF kernel (1-dim): $k(x,x') = e^{-\\frac{(x - x')^2}{2}}$  \n",
    "- We claim that $\\psi: R \\to l_2$  defined by  \n",
    "$$[\\psi(x)]_{n}=\\frac{1}{\\sqrt{n !}} e^{-x^{2} / 2} x^{n}$$  \n",
    "gives the **“infnite-dimensional feature vector” corresponding to RBF kernel**.  \n",
    "- Is $\\psi(x)$ even an element of $l_2$?   \n",
    "yes  \n",
    "$$\\sum_{n=0}^{\\infty} \\frac{1}{n !} e^{-x^{2}} x^{2 n}=e^{-x^{2}} \\sum_{n=0}^{\\infty} \\frac{\\left(x^{2}\\right)^{n}}{n !}=1<\\infty$$  \n",
    "\n",
    "- Does feature vector $[\\psi(x)]_{n}=\\frac{1}{\\sqrt{n !}} e^{-x^{2} / 2} x^{n}$ actually corresponding to the RBF kernel?  \n",
    "Yes  \n",
    "$$\\begin{aligned}\n",
    "\\left\\langle\\psi(x), \\psi\\left(x^{\\prime}\\right)\\right\\rangle &=\\sum_{n=0}^{\\infty} \\frac{1}{n !} e^{-\\left(x^{2}+\\left(x^{\\prime}\\right)^{2}\\right) / 2} x^{n}\\left(x^{\\prime}\\right)^{n} \\\\\n",
    "&=e^{-\\left(x^{2}+\\left(x^{\\prime}\\right)^{2}\\right) / 2} \\sum_{n=0}^{\\infty} \\frac{\\left(x x^{\\prime}\\right)^{n}}{n !} \\\\\n",
    "&=\\exp \\left(-\\left[x^{2}+\\left(x^{\\prime}\\right)^{2}\\right] / 2\\right) \\exp \\left(x x^{\\prime}\\right) \\\\\n",
    "&=\\exp \\left(-\\left[\\left(x-x^{\\prime}\\right)^{2} / 2\\right]\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When is $k(x,x')$ a kernel function? (Mercer's Theorem)  \n",
    "## How to get kernel?  \n",
    "- Explicitly construct $psi(x): X \\to R^d$, and define $k(x,x') = \\psi(x)^T\\psi(x')$  \n",
    "- Directly define kernel function $k(x,x')$, and verify it corresponding to $<\\psi(x), \\psi(x')>$ for some $\\psi$   \n",
    "\n",
    "## Positive Semideﬁnite Matrices\n",
    "- Definition  \n",
    "A real, symmetric matrix $M \\in R^{n \\times n}$ is positive semideﬁnite (psd) if for any $x \\in R^n$  \n",
    "$$x^TMx \\geq 0$$  \n",
    "- Theorem \n",
    "The following conditions are each necessary and suﬃcient for $M$ to be positive semideﬁnite  \n",
    "   - $M$ has a “square root”, i.e. there exists $R$ s.t. $M = R^TR$  \n",
    "   - All eigenvalues of $M$ are greater than or equal to 0.\n",
    "\n",
    "A symmetric kernel function $k : X\\times X \\to R$ is positive semideﬁnite (psd) if for any ﬁnite set $\\{x_1,...,x_n\\}\\in X$, the kernel matrix on this set  \n",
    "$$K=\\left(k\\left(x_{i}, x_{j}\\right)\\right)_{i, j}=\\left(\\begin{array}{ccc}\n",
    "k\\left(x_{1}, x_{1}\\right) & \\cdots & k\\left(x_{1}, x_{n}\\right) \\\\\n",
    "\\vdots & \\ddots & \\ldots \\\\\n",
    "k\\left(x_{n}, x_{1}\\right) & \\cdots & k\\left(x_{n}, x_{n}\\right)\n",
    "\\end{array}\\right)$$  \n",
    "is positive semidefinite  \n",
    "\n",
    "## Mercer’s Theorem\n",
    "A symmetric function $k(x,x_0)$ can be expressed as an inner product  \n",
    "$$k(x,x') = <\\psi(x), \\psi(x')>$$  \n",
    "for some $\\psi$ if and only if $k(x,x')$ is positive semidefinte  \n",
    "In other words, if we want a matrix to be a kernel matrix, such matrix has to be positive semidefinte  \n",
    "\n",
    "## Generating New Kernels from Old  \n",
    "Suppose $k,k_1,k_2 : X\\times X \\to R$ are psd kernels. Then so are the following  \n",
    "$k_{\\text {new }}\\left(x, x^{\\prime}\\right)=k_{1}\\left(x, x^{\\prime}\\right)+k_{2}\\left(x, x^{\\prime}\\right)$  \n",
    "\n",
    "$k_{\\text {new }}\\left(x, x^{\\prime}\\right)=\\alpha k\\left(x, x^{\\prime}\\right)$  \n",
    "\n",
    "$k_{\\text {new }}\\left(x, x^{\\prime}\\right)=f(x) f\\left(x^{\\prime}\\right)$ for any function $f(\\cdot)$  \n",
    "\n",
    "$k_{\\text {new }}\\left(x, x^{\\prime}\\right)=k_{1}\\left(x, x^{\\prime}\\right) k_{2}\\left(x, x^{\\prime}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details on New Kernels from Old  \n",
    "\n",
    "## Additive Closure  \n",
    "Suppose $k_1$ and $k_2$ are psd kernels with feature maps $\\psi_1$ and $\\psi_2$, respectively, then  \n",
    "$$k_{1}\\left(x, x^{\\prime}\\right)+k_{2}\\left(x, x^{\\prime}\\right)$$  \n",
    "is psd kernel  \n",
    "\n",
    "## Closure under Positive Scaling\n",
    "- Suppose $k$ is a psd kernel with feature maps $\\psi$.  \n",
    "Then for any $\\alpha > 0$,  \n",
    "$$\\alpha k $$  \n",
    "is a psd kernel  \n",
    "\n",
    "## Scalar Function Gives a Kernel\n",
    "For any function $f (x)$,\n",
    "$$k\\left(x, x^{\\prime}\\right)=f(x) f\\left(x^{\\prime}\\right)$$  \n",
    "is a kernel  \n",
    "proof:  \n",
    "Let $f (x)$ be the feature mapping. (It maps into a 1-dimensional feature space.)  \n",
    "$$\\left\\langle f(x), f\\left(x^{\\prime}\\right)\\right\\rangle=f(x) f\\left(x^{\\prime}\\right)=k\\left(x, x^{\\prime}\\right)$$  \n",
    "\n",
    "## Closure under Hadamard Products\n",
    "- Suppose $k_1$ and $k_2$ are psd kernels with feature maps $\\psi_1$ and $\\psi_2$, respectively, then  \n",
    "$$k_{1}\\left(x, x^{\\prime}\\right) k_{2}\\left(x, x^{\\prime}\\right)$$  \n",
    "is a psd kernel  \n",
    "Proof: Take the outer product of the feature vectors  \n",
    "$$\\phi(x)=\\phi_{1}(x)\\left[\\phi_{2}(x)\\right]^{T}$$  \n",
    "Note that $\\phi(x)$ is a matrix.   \n",
    "then  \n",
    "$$\\begin{aligned}\n",
    "\\left\\langle\\phi(x), \\phi\\left(x^{\\prime}\\right)\\right\\rangle &=\\sum_{i j} \\phi(x) \\phi\\left(x^{\\prime}\\right) \\\\\n",
    "&=\\sum_{i, j}\\left[\\phi_{1}(x)\\left[\\phi_{2}(x)\\right]^{T}\\right]_{i j}\\left[\\phi_{1}\\left(x^{\\prime}\\right)\\left[\\phi_{2}\\left(x^{\\prime}\\right)\\right]^{T}\\right]_{i j} \\\\\n",
    "&=\\sum_{i, j}\\left[\\phi_{1}(x)\\right]_{i}\\left[\\phi_{2}(x)\\right]_{j}\\left[\\phi_{1}\\left(x^{\\prime}\\right)\\right]_{i}\\left[\\phi_{2}\\left(x^{\\prime}\\right)\\right]_{j} \\\\\n",
    "&=\\left(\\sum_{i}\\left[\\phi_{1}(x)\\right]_{i}\\left[\\phi_{1}\\left(x^{\\prime}\\right)\\right]_{i}\\right)\\left(\\sum_{j}\\left[\\phi_{2}(x)\\right]_{j}\\left[\\phi_{2}\\left(x^{\\prime}\\right)\\right]_{j}\\right) \\\\\n",
    "&=k_{1}\\left(x, x^{\\prime}\\right) k_{2}\\left(x, x^{\\prime}\\right)\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
