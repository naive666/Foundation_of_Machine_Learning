{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Subgradient  \n",
    "1. Since $g \\in \\partial f_k(x)$, we have $f_k(z) \\geq f_k(x) + g^T(z - x)$, and $f(x)=\\max _{i=1, \\ldots, m} f_{i}(x)$, we have $f(z) \\geq f_k(z) \\geq f_k(x) + g^T(z - x) = f_k(z) \\geq f(x) + g^T(z - x)$, thus $g \\in \\partial f(x)$  \n",
    "Here $x$ should be treated as an arbitrary fixed point  \n",
    "\n",
    "2. Base on (1), let $f_1 = 0,f_2 = 1 - yw^Tx$, and the subgradient of $f_1=0$ is 0, of $f_2 = 1 - yw^Tx$ is $-yx$, and therefore  \n",
    "$$\\partial J(w)=\n",
    "\\begin{cases}\n",
    "0 & 1-yw^Tx<0\\\\\n",
    "-yx & else\n",
    "\\end{cases}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron  \n",
    "The perceptron algorithm is often the ﬁrst classiﬁcation algorithm taught in machine learning classes. Suppose we have a labeled training set $(x_1,y_1),...(x_n,y_n) \\in R^d \\times \\{1,-1\\}$. In the perceptron algorithm, we are looking for a hyperplane that perfectly separates the classes. That is we are looking for a hyperplane $w \\in R^n$, such that  \n",
    "$$y_iw^Tx_i \\geq 0$$  \n",
    "for all $i$   \n",
    "When such a hyperplane exists,we say, that the data are linearly separable. The perceptron algorithm is given in Algorithm 1.  \n",
    "<div align=\"center\"><img src = \"./perceptron.jpg\" width = '500' height = '100' align = center /></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:  \n",
    "if \n",
    "$$y_i xi^T w^{(k)} \\leqslant 0$$  \n",
    "then  \n",
    "$$w^{(k+1)} = w^{(k)} + y_ix_i$$,  \n",
    "this indicates that the mispredicted samples contribute the fianl result of $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perceptron Loss  \n",
    "There is also something called the perceptron loss, given by  \n",
    "$$l(\\hat{y},y) = max \\{0,-y\\hat{y} \\}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If it is a separating hyperplane, then   \n",
    "$$y_iw^Tx_i \\geqslant 0$$  \n",
    "and we know $\\hat{y}_i = w^Tx_i$, then  \n",
    "$$-y_i\\hat{y}_i \\leqslant 0$$  \n",
    "for all $i$,  \n",
    "then  \n",
    "$$l(\\hat{y}_i,y_i) = max \\{0,-y_i\\hat{y}_i \\} = 0$$   \n",
    "then the average loss is 0  \n",
    "\n",
    "\n",
    "2. Subgradient of Perceptron Loss  \n",
    "$$\\partial J(w)=\n",
    "\\begin{cases}\n",
    "0 & -yw^Tx<0\\\\\n",
    "-yx & else\n",
    "\\end{cases}\n",
    "$$  \n",
    "Then using SSGD  \n",
    "<div align=\"center\"><img src = \"./SSGD_perceptron.jpg\" width = '500' height = '100' align = center /></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initial $w^{(0)} = (0,0,...0)\\in R^d$,  \n",
    "for those $(x_i,y_i)$ such that $y_ix_i^Tw^{(k)} \\leqslant 0$  \n",
    "$$w^{(k+1)} = w^{(k)} + y_ix_i$$\n",
    "$$w^{(k)} = w^{(k-1)} + y_jx_j + y_ix_i$$\n",
    "\n",
    "where $y_i \\in \\{-1,1\\}$, then  \n",
    "we can write $w = \\sum_{i=1}^{n}\\alpha_i x_i$, which is a linear combination of $x$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "'''\n",
    "Note:  This code is just a hint for people who are not familiar with text processing in python. There is no obligation to use this code, though you may if you like. \n",
    "'''\n",
    "\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings. \n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    return words\n",
    "\n",
    "###############################################\n",
    "######## YOUR CODE STARTS FROM HERE. ##########\n",
    "###############################################\n",
    "\n",
    "def shuffle_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = \"data/pos\"\n",
    "    neg_path = \"data/neg\"\n",
    "\n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "    return review\n",
    "\n",
    "'''\n",
    "Now you have read all the files into list 'review' and it has been shuffled.\n",
    "Save your shuffled result by pickle.\n",
    "*Pickle is a useful module to serialize a python object structure. \n",
    "*Check it out. https://wiki.python.org/moin/UsingPickle\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine via Pegasos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM objective function:  \n",
    "\n",
    "$$\\min _{w \\in \\mathbf{R}^{d}} \\frac{\\lambda}{2}\\|w\\|^{2}+\\frac{1}{m} \\sum_{i=1}^{m} \\max \\left\\{0,1-y_{i} w^{T} x_{i}\\right\\}$$  \n",
    "- for simplicity, we are leaving off the unregularized bias term $b$.  \n",
    "- Pegasos is stochastic subgradient descent using a step size rule $\\eta_t = \\frac{1}{\\lambda t}$  \n",
    "<div align=\"center\"><img src = \"./pegasos.jpg\" width = '500' height = '100' align = center /></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)  \n",
    "$$J_{i}(w)=\\frac{\\lambda}{2}\\|w\\|^{2}+\\max \\left\\{0,1-y_{i} w^{T} x_{i}\\right\\}$$  \n",
    "is not differentiable when $y_iw^Tx_i = 1$  \n",
    "(2)  \n",
    "A subgradient of $J_i(w)$ is given by  \n",
    "$$g=\\left\\{\\begin{array}{ll}\n",
    "\\lambda w-y_{i} x_{i} & \\text { for } y_{i} w^{T} x_{i}<1 \\\\\n",
    "\\lambda w & \\text { for } y_{i} w^{T} x_{i} \\geq 1\n",
    "\\end{array}\\right.$$  \n",
    "(3) \n",
    "Using subgradient descent with step size $\\eta_t = \\frac{1}{\\lambda t} $\n",
    "\n",
    "(4)  \n",
    "Implement the Pegasos algorithm to run on a sparse data representation. The output should be a sparse weight vector $w$.   \n",
    "Also: If you normalize your data in some way, be sure not to destroy the sparsity of your data. Anything that starts as 0 should stay at 0.\n",
    "\n",
    "(5)  \n",
    "Note that in every step of the Pegasos algorithm, we rescale every entry of wt by the factor $(1 - \\eta_t\\lambda)$. Implementing this directly with dictionaries is very slow. We can make things signiﬁcantly faster by representing $w$ as $w = sW$, where $s \\in R$ and $W \\in R^d$. You can start with $s = 1$ and $W$ all zeros (i.e. an empty dictionary). Note that both updates (i.e. whether or not we have a margin error) start with rescaling $w_t$, which we can do simply by setting $s_{t+1} = (1 - \\eta_t\\lambda)s_t$, If the update is $w_{t+1}=\\left(1-\\eta_{t} \\lambda\\right) w_{t}+\\eta_{t} y_{j} x_{j}$, then verify that the Pegasos update step is equivalent to  \n",
    "$$\\begin{aligned}\n",
    "s_{t+1} &=\\left(1-\\eta_{t} \\lambda\\right) s_{t} \\\\\n",
    "W_{t+1} &=W_{t}+\\frac{1}{s_{t+1}} \\eta_{t} y_{j} x_{j}\n",
    "\\end{aligned}$$  \n",
    "\n",
    "There is one subtle issue with the approach described above: if we ever have $1 - \\eta_t \\lambda = 0$, then $s_{t+1} = 0$, and we’ll have a divide by 0 in the calculation for $W_{t+1}$. This only happens when $\\eta_t = \\frac{1}{\\lambda}$. With our step-size rule of $\\eta_t = \\frac{1}{\\lambda t}$, it happens exactly when $t = 1$. So one approach is to just start at $t = 2$. More generically, note that if $s_{t+1} = 0$, then $w_{t+1} = 0$. Thus an equivalent representation is $s_{t+1} = 1$ and $W = 0$. Thus if we ever get $s_{t+1} = 0$, simply set it back to 1 and reset $W_{t+1}$ to zero, which is an empty dictionary in a sparse representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_error(X, y, w):\n",
    "    \"\"\"\n",
    "    X: Array_like, (n_samples, n_features)\n",
    "       traing points\n",
    "    y: Array_like  (n_samples, )\n",
    "       labels\n",
    "    w: Array_like  (n_features, )\n",
    "       sparse weight vector \n",
    "    \n",
    "    return\n",
    "    error: float\n",
    "    \"\"\"\n",
    "    result = np.dot(X, w) * y\n",
    "    n_sample = y.shape[0]\n",
    "    return np.sum(np.where(result >= 0, 0, 1)) / n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10)\n",
    "If the data such that $y_iw^Tx_i$ takes up a large proportion, ther is no doubt that we can't ignore this part data. If it is only of small amount,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "This method only investigate the relative importence, what if all features are terrible or the difference of the performance of all features is not significant? We need further research.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
