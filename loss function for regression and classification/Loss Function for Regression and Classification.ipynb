{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function for Regression\n",
    "- In general, the loss function may take the form $(\\hat{y}, y) \\mapsto l(\\hat{y}, y) \\in R$  \n",
    "- Regression losses usually only depend on the residual $r = y - \\hat{y}$.  \n",
    "- Loss $l(\\hat{y},y)$ is called **distance-based** if it only depends on the residual  \n",
    "## Distance-Based Losses are Translation Invariant  \n",
    "- Translation Invariant\n",
    "$$\\ell(\\hat{y}+a, y+a)=\\ell(\\hat{y}, y)$$  \n",
    "- Sometimes relative error $\\frac{y - \\hat{y}}{y}$ is a more natural loss (but not translation-invariant)  \n",
    "## Some Losses for Regression  \n",
    "- residual: $r = y - \\hat{y}$\n",
    "- $l_2$ loss: $l(r) = r^2$ (not robust)  \n",
    "- $l_1$ loss: $l(r) = |r|$ (robust but not differentiable)\n",
    "- Huber loss: : Quadratic for $|r| \\leq \\delta$ and linear for $|r| \\ge \\delta$ (Robust and differentiable)  \n",
    "Square loss much more affected by outliers than absolute loss.  \n",
    "<div align=\"center\"><img src = \"./losses.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "\n",
    "## Loss Function Robustness\n",
    "Robustness refers to how affected a learning algorithm is by outliers  \n",
    "<div align=\"center\"><img src = \"./robustness.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Loss\n",
    "## Notations  \n",
    "- 0 - 1 Loss for $f : X \\to R$  \n",
    "$$\\begin{array}{l}\n",
    "f(x)>0 \\Longrightarrow \\text { Predict } 1 \\\\\n",
    "f(x)<0 \\Longrightarrow \\text { Predict }-1\n",
    "\\end{array}$$  \n",
    "## The Score Function\n",
    "- Notation  \n",
    "  - Action Space $\\mathcal{A} = R$  -\n",
    "  - Output Space $\\mathcal{y} = \\{-1, 1 \\}$\n",
    "  - Real-valued prediction function $f : X \\to R$  \n",
    "- Definition  \n",
    "The value $f (x)$ is called the **score** for the input x\n",
    "- In this context, $f$ may be called a **score function**  \n",
    "- Intuitively, magnitude of the score represents the **confidence of our prediction.**  \n",
    "## Margin  \n",
    "- Definition  \n",
    "The **margin (or functional margin)** for predicted score $\\hat{y}$ and true class $y \\in \\{-1,1\\}$ is $y\\hat{y}$  \n",
    "- The margin often looks like $yf (x)$, where $f (x)$ is our score function  \n",
    "- The margin is a measure of how correct we are  \n",
    "   - If $y$ and $\\hat{y}$ are the same sign, prediction is correct and margin is positive.\n",
    "   - If $y$ and $\\hat{y}$ have different sign, prediction is incorrect and margin is negative   \n",
    "   \n",
    "We want to maximize the **margin**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin-Based Losses  \n",
    "- Most classification losses depend only on the margin  \n",
    "- Such a loss is called a margin-based loss.\n",
    "- There is a related concept, the **geometric margin**, in the notes on hard-margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Losses: 0−1 Loss  \n",
    "Empirical risk for 0-1 loss: \n",
    "$$\\hat{R}_{n}(f)=\\frac{1}{n} \\sum_{i=1}^{n} 1\\left(y_{i} f\\left(x_{i}\\right) \\leqslant 0\\right)$$  \n",
    "- Minimizing empirical 0−1 risk not computationally feasible  \n",
    "- $\\hat{R}_n(f)$ is non-convex, not differentiable (in fact, discontinuous!)  \n",
    "- Optimization is **NP-Hard**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-One loss:  $l_{0-1} = 1$  \n",
    "<div align=\"center\"><img src = \"./01_figure.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss:  $l_{hinge} = max(0, 1 - m)$  \n",
    "Hinge is a convex, upper bound on 0−1 loss. Not differentiable at $m = 1$  \n",
    "<div align=\"center\"><img src = \"./hinge.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Soft Margin) Linear Support Vector Machine  \n",
    "- Hypothesis Space $\\mathcal{F}=\\left\\{f(x)=w^{T} x \\mid w \\in \\mathbf{R}^{d}\\right\\}$  \n",
    "- Loss $l(m) = (1 - m)_+$  \n",
    "- $l_2$ regularization  \n",
    "$$\\min _{w \\in \\mathbf{R}^{d}} \\sum_{i=1}^{n}\\left(1-y_{i} f_{w}\\left(x_{i}\\right)\\right)_{+}+\\lambda\\|w\\|_{2}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss $l_{logistic} = log(1 + e^{-m})$  \n",
    "- Logistic loss is differentiable. Logistic loss always wants more margin (loss never 0)  \n",
    "<div align=\"center\"><img src = \"./logistics.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "What if we substitute residual for margin?   \n",
    "It is not possible, as the residual becomes larger, loss declines, which is unacceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About Square Loss for Classification?  \n",
    "- Loss $\\ell(f(x), y)=(f(x)-y)^{2}$  \n",
    "- Turns out, can write this in terms of margin $m = f (x)y$:  \n",
    "$$\\ell(f(x), y)=(f(x)-y)^{2}=(1-f(x) y)^{2}=(1-m)^{2}$$  \n",
    "- Heavily penalty the outliers  \n",
    "<div align=\"center\"><img src = \"./square.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
