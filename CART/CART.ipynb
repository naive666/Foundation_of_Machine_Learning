{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree \n",
    "\n",
    "## Tree terminology  \n",
    "<div align=\"center\"><img src = \"./tree.jpg\" width = '500' height = '100' align = center /></div> \n",
    "\n",
    "## A Binary Decision Tree\n",
    "- **binary tree**: each node has either 2 children or 0 children  \n",
    "\n",
    "## Binary Decision Tree on $R^2$  \n",
    "- Consider a binary tree on $\\{(X_1,X_2) | X_1,X_2 \\in R\\}$  \n",
    "\n",
    "## Types of Decision Trees\n",
    "- We’ll only consider  \n",
    "  - **binary trees** (vs multiway trees where nodes can have more than 2 children)  \n",
    "  - decisions at each node involve only a single feature (i.e. input coordinate)   \n",
    "  - for continuous variables, splits always of the form  \n",
    "  $$x_i \\leq t$$\n",
    "  - for discrete variables, partitions values into two groups  \n",
    "- Other types of splitting rules   \n",
    "  - **oblique decision trees** or **binary space partition trees** (BSP trees) have a linear split at each node   \n",
    "  - **sphere trees** – space is partitioned by a sphere of a certain radius around a ﬁxed point\n",
    "\n",
    "# Regression Tree  \n",
    "Binary Regression Tree on $R^2$  \n",
    "<div align=\"center\"><img src = \"./binaryR2.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Fitting a Regression Tree\n",
    "- The decision tree gives the partition of $X$ into regions:  \n",
    "$$\\{R_1, ... , R_M\\}$$  \n",
    "- Recall that a partition is a **disjoint union**, that is:  \n",
    "$$\n",
    "x=R_{1} \\cup R_{2} \\cup \\cdots \\cup R_{M}\n",
    "$$  \n",
    "and  \n",
    "$$\n",
    "R_{i} \\cap R_{j}=\\emptyset \\quad \\forall i \\neq j\n",
    "$$  \n",
    "- Given the partition $\\{R_1,...,R_M\\}$, ﬁnal prediction is  \n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M} c_{m} 1\\left(x \\in R_{m}\\right)\n",
    "$$  \n",
    "The linear combination of the sample data lying in the same partition  \n",
    "- How to choose $c_1,...,c_M$?  \n",
    "- For loss function $l(\\hat{y},y) = (\\hat{y}−y)^2$, best is  \n",
    "$$\n",
    "\\hat{c}_{m}=\\operatorname{ave}\\left(y_{i} \\mid x_{i} \\in R_{m}\\right)\n",
    "$$  \n",
    "\n",
    "## Trees and Overﬁtting\n",
    "- If we do enough splitting, every unique $x$ value will be in its own partition  \n",
    "- This very likely overﬁts.  \n",
    "- As usual, we need to control the complexity of our hypothesis space.  \n",
    "- CART (Breiman et al. 1984) uses number of terminal nodes.  \n",
    "- Tree depth is also common.\n",
    "\n",
    "## Complexity of a Tree\n",
    "- Let $|T| = M$ denote the number of terminal nodes in $T$.  \n",
    "- We will use $|T|$ to measure the complexity of a tree  \n",
    "- For any given complexity,  \n",
    "  - we want the tree minimizing square error on training set.   \n",
    "- Finding the optimal binary tree of a given complexity is computationally intractable  \n",
    "- We proceed with a **greedy algorithm**  \n",
    "  - Means build the tree one node at a time, without any planning ahead  \n",
    "  \n",
    "## Root Node, Continuous Variables\n",
    "- Let $x = (x_1,...,x_d)\\in R^d$. ($d$ features)   \n",
    "- Splitting variable $j \\in \\{1,...,d\\}$.  \n",
    "- **Split point** $s \\in R$.  \n",
    "- Partition based on $j$ and $s$:  \n",
    "$$\n",
    "\\begin{array}{l}\n",
    "R_{1}(j, s)=\\left\\{x \\mid x_{j} \\leqslant s\\right\\} \\\\\n",
    "R_{2}(j, s)=\\left\\{x \\mid x_{j}>s\\right\\}\n",
    "\\end{array}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Node, Continuous Variables\n",
    "- For each splitting variable $j$ and split point $s$,  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{c}_{1}(j, s) &=\\operatorname{ave}\\left(y_{i} \\mid x_{i} \\in R_{1}(j, s)\\right) \\\\\n",
    "\\hat{c}_{2}(j, s) &=\\operatorname{ave}\\left(y_{i} \\mid x_{i} \\in R_{2}(j, s)\\right)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- Find $j,s$ minimizing loss   \n",
    "$$\n",
    "L(j, s)=\\sum_{i: x_{i} \\in R_{1}(j, s)}\\left(y_{i}-\\hat{c}_{1}(j, s)\\right)^{2}+\\sum_{i: x_{i} \\in R_{2}(j, s)}\\left(y_{i}-\\hat{c}_{2}(j, s)\\right)^{2}\n",
    "$$ \n",
    "\n",
    "## Finding the Split Point\n",
    "- Consider splitting on the $j$’th feature $x_j$  \n",
    "If $x_{j(1)},...,x_{j(n)}$ are the sorted values of the $j$’th feature  \n",
    "  - we only need to check split points between adjacent values \n",
    "  - traditionally take split points halfway between adjacent values:   \n",
    "$$\n",
    "s_{j} \\in\\left\\{\\frac{1}{2}\\left(x_{j(r)}+x_{j(r+1)}\\right) \\mid r=1, \\ldots, n-1\\right\\}\n",
    "$$  \n",
    "- So only need to check performance of $n−1$ splits  \n",
    "\n",
    "## Then Proceed Recursively  \n",
    "-  We have determined $R_1$ and $R_2$  \n",
    "- Find best split for points in $R_1$  \n",
    "- Find best split for points in $R_2$  \n",
    "- Continue...  \n",
    "\n",
    "When do we stop?\n",
    "\n",
    "## Complexity Control Strategy\n",
    "- If the tree is too big, we may overﬁt.   \n",
    "- If too small, we may miss patterns in the data (underﬁt).  \n",
    "- Can limit max depth of tree.  \n",
    "- Can require all leaf nodes contain a minimum number of points.   \n",
    "- Can require a node have at least a certain number of data points to split  \n",
    "- Can do backward pruning (the approach of CART (Breiman et al 1984):   \n",
    "  -  Build a really big tree (e.g. until all regions have $\\leq$ 5 points).   \n",
    "  - “Prune” the tree back greedily all the way to the root, assessing performance on validation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree  \n",
    "- Consider classiﬁcation case $y=\\{1,2, \\ldots, K\\}$  \n",
    "- We need to modify  \n",
    "  - criteria for splitting nodes\n",
    "- Let node $m$ represent region $R_m$, with $N_m$ observations  \n",
    "- Denote proportion of observations in $R_m$ with class $k$ by  \n",
    "$$\n",
    "\\hat{p}_{m k}=\\frac{1}{N_{m}} \\sum_{\\left\\{i: x_{i} \\in R_{m}\\right\\}} 1\\left(y_{i}=k\\right)\n",
    "$$  \n",
    "- Predicted classiﬁcation for node $m$ is  \n",
    "$$\n",
    "k(m)=\\underset{k}{\\arg \\max } \\hat{p}_{m k}\n",
    "$$  \n",
    "- Predicted class probability distribution is $(\\hat{p}_{m1},..., \\hat{p}_{mK})$.  \n",
    "\n",
    "## Misclassiﬁcation Error\n",
    "- What is the misclassiﬁcation rate on the training data?   \n",
    "- It’s just\n",
    "$$\n",
    "1-\\hat{p}_{m k(m)}\n",
    "$$  \n",
    "\n",
    "## What loss function to use for node splitting?\n",
    "- Natural loss function for classiﬁcation is 0/1 loss.  \n",
    "- Is this tractable for ﬁnding the best split? Yes!  \n",
    "- Should we use it? Maybe not!   \n",
    "- If we’re only splitting once, then make sense to split using ultimate loss function (say 0/1).   \n",
    "- But we can split nodes repeatedly – don’t have to get it right all at once.\n",
    "\n",
    "## Splitting Example\n",
    "- Two class problem: 4 observations in each class.  \n",
    "- Split 1: (3,1) and (1,3) [each region has 3 of one class and 1 of other]\n",
    "- Split 2: (2,4) and (2,0) [one region has 2 of one class and 4 of other, other region pure]\n",
    "- Misclassiﬁcation rate for the two splits are same  \n",
    "- In split 1, we’ll want to split each node again, and   \n",
    "  - we’ll end up with a leaf node with a single element.node   \n",
    "- In split 2, we’re already done with the node (2,0).\n",
    "\n",
    "## Splitting Criteria\n",
    "- Eventually we want **pure leaf nodes** (i.e. as close to a single class as possible)  \n",
    "- We’ll ﬁnd splitting variables and split point minimizing some node impurity measure.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Class Node Impurity Measures\n",
    "- Consider binary classiﬁcation   \n",
    "- Let $p$ be the relative frequency of class 1.  \n",
    "- Here are three node impurity measures as a function of $p$  \n",
    "<div align=\"center\"><img src = \"./impurity.jpg\" width = '500' height = '100' align = center /></div>   \n",
    "\n",
    "## Classiﬁcation Trees: Node Impurity Measures  \n",
    "- Consider leaf node $m$ representing region $R_m$, with $N_m$ observations  \n",
    "- Three measures $Q_m(T)$ of node impurity for leaf node $m$:  \n",
    "  - Misclassiﬁcation error:\n",
    "$$\n",
    "1-\\hat{p}_{m k(m)}\n",
    "$$  \n",
    "  - Gini index:\n",
    "$$\n",
    "\\sum_{k=1}^{K} \\hat{p}_{m k}\\left(1-\\hat{p}_{m k}\\right)\n",
    "$$  \n",
    "  - Entropy or deviance (equivalent to using information gain):\n",
    "$$\n",
    "-\\sum_{k=1}^{K} \\hat{p}_{m k} \\log \\hat{p}_{m k}\n",
    "$$  \n",
    "\n",
    "## Class Distributions: Pre-split\n",
    "<div align=\"center\"><img src = \"./pre-split.jpg\" width = '500' height = '100' align = center /></div>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distributions: Split Search\n",
    "<div align=\"center\"><img src = \"./split search.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "(Maximizing information gain is equivalent to minimizing entropy.)\n",
    "\n",
    "## Splitting nodes: How exactly do we do this?\n",
    "- Let $R_L$ and $R_R$ be regions corresponding to a potential node split.  \n",
    "- Suppose we have $N_L$ points in $R_L$ and $N_R$ points in $R_R$  \n",
    "- Let $Q(R_L)$ and $Q(R_R)$ be the node impurity measures  \n",
    "- Then ﬁnd split that minimizes the weighted average of node impurities  \n",
    "$$\n",
    "N_{L} Q\\left(R_{L}\\right)+N_{R} Q\\left(R_{R}\\right)\n",
    "$$  \n",
    "\n",
    "## Classiﬁcation Trees: Node Impurity Measures\n",
    "- For building the tree, Gini and Entropy seem to be more eﬀective  \n",
    "- They push for more pure nodes, not just misclassiﬁcation rate  \n",
    "- A good split may not change misclassiﬁcation rate at all!\n",
    "- Two class problem: 4 observations in each class.   \n",
    "- Split 1: (3,1) and (1,3) [each region has 3 of one class and 1 of other]   \n",
    "- Split 2: (2,4) and (2,0) [one region has 2 of one class and 4 of other, other region pure]  \n",
    "- Misclassiﬁcation rate for two splits are same.  \n",
    "- Gini and entropy split prefer Split 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees in General  \n",
    "## Missing Features\n",
    "- What to do about missing features?  \n",
    "  - Throw out inputs with missing features   \n",
    "  - Impute missing values with feature means  \n",
    "  - If a categorical feature, let “missing” be a new category.  \n",
    "- For trees, we can use **surrogate splits**   \n",
    "  - For every internal node, form a list of surrogate features and split points  \n",
    "  - Goal is to approximate the original split as well as possible  \n",
    "  - Surrogates ordered by how well they approximate the original split  \n",
    "    - In terms of how many examples are sent in the same direction by each split  \n",
    "\n",
    "## Categorical Features\n",
    "- Suppose we have a categorical feature with q possible values (unordered). \n",
    "- We want to ﬁnd the best split into 2 groups  \n",
    "- There are $2^{q−1}−1$ distinct splits  \n",
    "- Is this tractable? Maybe not in general. But...   \n",
    "- For binary classiﬁcation $Y = \\{0,1\\}$, there is an eﬃcient algorithm.  \n",
    "  - Assign each category a number, the proportion of class 0.  \n",
    "  - Then ﬁnd optimal split as though it were a numeric feature.  \n",
    "  - Proved to be equivalent to search over all splits in (Breiman et al. 1984).   \n",
    "- Otherwise, can use approximations.  \n",
    "- Statistical issues?   \n",
    "  - If a category has a very large number of categories, we can overﬁt.   \n",
    "  - Extreme example: Row Number could lead to perfect classiﬁcation with a single split.\n",
    "\n",
    "## Trees vs Linear Models\n",
    "Trees have to work much harder to capture linear relations  \n",
    "<div align=\"center\"><img src = \"./treeVSlinear.jpg\" width = '500' height = '100' align = center /></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability  \n",
    "- Trees are certainly easy to explain.  \n",
    "- You can show a tree on a slide.  \n",
    "- Small trees seem interpretable.  \n",
    "- For large trees, maybe not so easy.\n",
    "\n",
    "## Trees for Nonlinear Feature Discovery\n",
    "- Suppose tree T gives partition $R_1,...,R_m$  \n",
    "- Predictions are\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M} c_{m} 1\\left(x \\in R_{m}\\right)\n",
    "$$  \n",
    "- Each region $R_m$ can be viewed as giving a feature function \n",
    "$$\n",
    "x \\mapsto 1\\left(x \\in R_{m}\\right)\n",
    "$$  \n",
    "Can use these nonlinear features in e.g. lasso regression  \n",
    "\n",
    "## Comments about Trees\n",
    "- Trees make no use of geometry  \n",
    "  - No inner products or distances  \n",
    "  - called a “nonmetric” method  \n",
    "  - **Feature scale irrelevant**  \n",
    "- Predictions are not continuous  \n",
    "  - not so bad for classiﬁcation  \n",
    "  - may not be desirable for regression\n",
    "\n",
    "# Tree Pruning   \n",
    "## Stopping Conditions for Building the Big Tree\n",
    "- First step is to build the “big tree”.  \n",
    "- Keep splitting nodes until every node either has  \n",
    "  - Zero error OR  \n",
    "  - Node has C or fewer examples (typically C =5 or C =1)\n",
    "  \n",
    "## Pruning the Tree  \n",
    "- Consider an internal node $n$.  \n",
    "- To prune the subtree rooted at $n$  \n",
    "  - eliminate all descendents of $n$  \n",
    "  - $n$ becomes a terminal node\n",
    "<div align=\"center\"><img src = \"./treePrune.jpg\" width = '500' height = '100' align = center /></div>  \n",
    "Subtree $T \\subset T_0$  \n",
    "\n",
    "## Empirical Risk and Tree Complexity\n",
    "- Suppose we want to prune a big tree $T_0$.  \n",
    "- Let $\\hat{R}(T)$ be the empirical risk of $T$ (i.e. square error on training)   \n",
    "- Clearly, for any subtree $T \\subset T_0$, $\\hat{R}(T) \\geqslant \\hat{R}\\left(T_{0}\\right)$  \n",
    "- Let $|T|$ be the number of terminal nodes in $T$.  \n",
    "- $|T|$ is our measure of complexity for a tree.\n",
    "\n",
    "## Cost Complexity (or Weakest Link) Pruning\n",
    "- Deﬁnitions  \n",
    "The **cost complexity criterion** with parameter $\\alpha$ is  \n",
    "$$\n",
    "C_{\\alpha}(T)=\\hat{R}(T)+\\alpha|T|\n",
    "$$  \n",
    "- Trades oﬀ between empirical risk and complexity of tree. (Bias and variance)  \n",
    "- Cost complexity pruning  \n",
    "- For each $\\alpha$, ﬁnd the subtree $T \\subset T_0$ minimizing $C_{\\alpha}(T)$ (on training data).  \n",
    "- Use cross validation to ﬁnd the right choice of $\\alpha$.  \n",
    "\n",
    "## Do we need to search over all subtrees?\n",
    "- $C_{\\alpha}(T)$ has familiar regularized ERM form, but  \n",
    "- Cannot just diﬀerentiate w.r.t. parameters of a tree $T$.  \n",
    "- To minimize $C_{\\alpha}(T)$ over subtrees $T \\subset T_0$,  \n",
    "- seems like we need to evaluate exponentially many1 subtrees  \n",
    "- In particular, we can include or exclude any subset of internal nodes that are parents of leaf nodes.)\n",
    "- Amazingly, we only need to try $N_{Int}$, where NInt is the number of internal nodes of $T_0$.  \n",
    "\n",
    "## Cost Complexity Greedy Pruning Algorithm\n",
    "- Find a proper subtree $T_1 \\subset T_0$ that minimizes $\\hat{R}\\left(T_{1}\\right)-\\hat{R}\\left(T_{0}\\right)$\n",
    "  - Can get $T_1$ by removing a single pair of leaf nodes, and their internal node parent becomes a leaf node.  \n",
    "  - This $T_1$ will have 1 fewer internal node than $T_0$. (And 1 fewer leaf node.)  \n",
    "- Then ﬁnd proper subtree $T_2 \\subset T_1$ that minimizes $\\hat{R}\\left(T_{2}\\right)-\\hat{R}\\left(T_{1}\\right)$  \n",
    "- Repeat until we have removed all interal nodes are left with just a single node (a leaf node).  \n",
    "- If $N_{Int}$ is the number of internal nodes of $T_0$, then we end up with a nested sequence of trees:   \n",
    "$$\n",
    "\\mathcal{T}=\\left\\{T_{0} \\supset T_{1} \\supset T_{2} \\supset \\cdots \\supset T_{\\left|N_{\\mathrm{Int}}\\right|}\\right\\}\n",
    "$$  \n",
    "- Breiman et al. (1984) proved that this is all you need. That is:  \n",
    "$$\n",
    "\\left\\{\\underset{T \\subset T_{0}}{\\arg \\min } C_{\\alpha}(T) \\mid \\alpha \\geqslant 0\\right\\} \\subset \\mathcal{T}\n",
    "$$  \n",
    "- Only need to evaluate $N_{Int}$ trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
