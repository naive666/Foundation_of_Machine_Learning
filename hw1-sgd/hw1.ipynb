{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need feature normalization?\n",
    "- In some models, features with large value are generally treated as 'more important', and the model might put more weight on that feature even if it is not so important. However, not every model we need to normalize features, in the model regarding decision trees, feature normalization is not required. In some typical models, such as ridge regression, lasso regression, SVM, and other general linear models, the difference between feature normalization and not is quite obvious. \n",
    "### Some methods of feature normalization\n",
    "- Min-Max Normalization  \n",
    "\n",
    "  <font size=4.5>$x^{*} = \\frac{x - x_{min}}{x_{max} - x_{min}}$</font>  \n",
    "  \n",
    "- Z-Score  \n",
    "  <font size=4.5>$x^{*} = \\frac{x - \\mu}{\\sigma}$</font>\n",
    "### Some tips:\n",
    "- Don't forget to normalize the data in test set using the same method as train set. \n",
    "- Not every model requires feature normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Setup\n",
    "1. Write down the expression of $J(\\theta)$ in matrix form\n",
    "\n",
    "   $\\hat{y} = X\\theta + b \\in \\mathbb{R}^{m}$  \n",
    "   \n",
    "   $J(\\theta) = \\frac{1}{m}(y - \\hat{y})^{T}(y - \\hat{y})$\n",
    "\n",
    "\n",
    "2. Write down the Gradient of $J(\\theta)$ in matrix form  \n",
    "\n",
    "   $\\nabla J(\\theta)=\\frac{1}{m}(X \\theta-y)^{T} \\frac{\\partial(X \\theta-y)}{\\partial \\theta}=\\frac{1}{m}(X \\theta-y)^{T} X$  \n",
    "\n",
    "\n",
    "3. $J(\\theta+\\eta \\Delta) \\approx J(\\theta)+\\nabla J(\\theta)^{T} \\eta \\Delta$  \n",
    "\n",
    "\n",
    "4. $\\theta^{\\prime}=\\theta-\\eta \\nabla J(\\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized  - test set after normalization\n",
    "\n",
    "    \"\"\"\n",
    "    train_num_inst, train_num_feat=train.shape\n",
    "    train_normalized=np.zeros((train_num_inst,train_num_feat))\n",
    "    for i in range(train_num_feat):\n",
    "        train_normalized[:,i]=(train[:,i]-min(train[:,i]))/(max(train[:,i])-min(train[:,i]))\n",
    "        #print(i)\n",
    "        \n",
    "    test_num_inst, test_num_feat=test.shape\n",
    "    test_normalized=np.zeros((test_num_inst,test_num_feat))\n",
    "    for i in range(test_num_feat):\n",
    "        test_normalized[:,i]=(test[:,i]-min(test[:,i]))/(max(test[:,i])-min(test[:,i]))\n",
    "    \n",
    "    \n",
    "    return train_normalized,test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', delimiter=',')\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into Train and Test\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', delimiter=',')\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "print('Split into Train and Test')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling all to [0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , ..., 0.13241261, 0.13241261,\n",
       "        0.13241261],\n",
       "       [1.        , 1.        , 1.        , ..., 0.92975578, 0.92975578,\n",
       "        0.92975578],\n",
       "       [1.        , 1.        , 1.        , ..., 0.91055382, 0.91055382,\n",
       "        0.91055382],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.        , ..., 0.04886354, 0.04886354,\n",
       "        0.04886354],\n",
       "       [1.        , 1.        , 1.        , ..., 0.58441163, 0.58441163,\n",
       "        0.58441163],\n",
       "       [0.        , 0.        , 0.        , ..., 0.03210449, 0.03210449,\n",
       "        0.03210449]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Scaling all to [0, 1]\")\n",
    "X_train_norm,X_test_norm = feature_normalization(X_train, X_test)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Square Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regresison (Linear Regression with $\\mathit{l}_{2}$ regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The objective function of ridge regression is:**    \n",
    "$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x_{i}\\right)-y_{i}\\right)^{2}+\\lambda \\theta^{T} \\theta$\n",
    "\n",
    "\n",
    "\n",
    "- **What are the advantages of $\\mathit{l}_{2}$ regularization?**    \n",
    "Observing the penalty term $\\lambda \\theta^{T}\\theta$, it is quadratic, and therefore differentiable. Apart from this, $\\mathit{l}_{2}$ regularization is more stable than $\\mathit{l}_{1}$ regularization\n",
    "\n",
    "\n",
    "- **What are the disadvantages of $\\mathit{l}_{2}$ regularization?**  \n",
    "It is computational expensive to calculate the derivative, and thus it takes more RAM. Even if the disadvantage exists, $\\mathit{l}_{2}$ regularization is the most commonly used regularization  \n",
    "\n",
    "\n",
    "- **Compute the gradient of $J(\\theta)$ in matrix form**  \n",
    "$\\nabla J(\\theta)=\\frac{1}{m}(x \\theta-y)^{T} \\frac{\\partial(x \\theta-y)}{\\partial \\theta}+\\frac{\\partial \\lambda \\theta^{T} \\theta}{\\partial \\theta}=\\frac{1}{m}(x \\theta-y)^{T} x+2 \\lambda \\theta^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.randn(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    num_instances=y.shape[0]\n",
    "    y_predict=np.dot(X,theta)\n",
    "    #y=y.reshape(y.shape[0],1)\n",
    "    differences = y_predict-y\n",
    "    square_loss=(1/(2*num_instances))*np.dot(differences,differences)\n",
    "    \n",
    "    return square_loss\n",
    "    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.4866033262572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_square_loss(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    #y=y.reshape(y.shape[0],1)\n",
    "    n=y.shape[0]\n",
    "    gradient=(1/n)*(np.dot(np.dot(X.T,X),theta)-np.dot(X.T,y))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=compute_square_loss_gradient(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Gradient Checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm.  Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1)\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicate whether the gradient is correct or not\n",
    "\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #the true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    e=np.eye(num_features)\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    for i in range(num_features):\n",
    "        approx_grad[i]=(compute_square_loss(X, y, theta+epsilon*(e[:,i]))-compute_square_loss(X, y, theta-epsilon*(e[:,i])))/(2*epsilon)\n",
    "    \n",
    "    distance=np.linalg.norm(approx_grad-true_gradient)\n",
    "    \n",
    "    if distance<=tolerance:\n",
    "       \n",
    "        return True\n",
    "    else:\n",
    "        \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_checker(X_train, y_train, theta, epsilon=0.01, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Gradient Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned\n",
    "    the true gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    true_gradient = gradient_func(X, y, theta) #the true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    e=np.eye(num_features)\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    for i in range(num_features):\n",
    "        approx_grad[i]=(objective_func(X, y, theta+epsilon*(e[:,i]))-objective_func(X, y, theta-epsilon*(e[:,i])))/(2*epsilon)\n",
    "    \n",
    "    distance=np.linalg.norm(approx_grad-true_gradient)\n",
    "    \n",
    "    if distance<=tolerance:\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        \n",
    "        return False\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the square loss objective\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_iter - number of iterations to run\n",
    "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features)\n",
    "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
    "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.zeros(num_features) #initialize theta\n",
    "    theta_hist[0]=theta\n",
    "    for i in range(num_iter):\n",
    "        if grad_checker(X,y,theta_hist[i,:])==False:\n",
    "                print(\"Alpha:\",alpha)\n",
    "                print(\"Gradient Fail! at step\",i)\n",
    "                break\n",
    "        theta_hist[i+1,:]=theta_hist[i,:]-alpha*compute_square_loss_gradient(X, y, theta_hist[i,:])\n",
    "        loss_hist[i]=compute_square_loss(X, y, theta_hist[i,:])\n",
    "    return theta_hist,loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05\n",
      "Gradient Fail! at step 10\n",
      "Alpha: 0.1\n",
      "Gradient Fail! at step 7\n",
      "Alpha: 0.5\n",
      "Gradient Fail! at step 4\n"
     ]
    }
   ],
   "source": [
    "theta_hist_001,loss_hist_001=batch_grad_descent(X_train, y_train, alpha=0.01, num_iter=1000, check_gradient=False)\n",
    "theta_hist_005,loss_hist_005 = batch_grad_descent(X_train,y_train,alpha=0.05,check_gradient=True)\n",
    "theta_hist_01,loss_hist_01= batch_grad_descent(X_train,y_train,alpha=0.1,check_gradient=True)\n",
    "theta_hist_05,loss_hist_05 = batch_grad_descent(X_train,y_train,alpha=0.5,check_gradient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss_hist_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X10XNV57/Hvo9FYkl/Ab4LYyI4xJrwFm4ACdhy6TEiocalN81ZYbfFqXlxICM1q08ZJLk1vkpWV3NB0XZISXzf4gtPUyW2JgaaYhLDiS5OLIbZjglPsmFCDZRvwC+CRZVkz0nP/OGdGM6ORNNKMNNI5v89aWjOzz545zxzJz9neZ5+9zd0REZH4qKt1ACIiMrqU+EVEYkaJX0QkZpT4RURiRolfRCRmlPhFRGJGiV9EJGaU+EVEYkaJX0QkZuprHUApM2fO9Hnz5tU6DBGRcWPHjh1H3b25nLpjMvHPmzeP7du31zoMEZFxw8xeLLeuunpERGJGiV9EJGaU+EVEYmZM9vGLyPiQTqdpa2ujs7Oz1qHERmNjIy0tLSSTyWF/hhK/iAxbW1sbU6ZMYd68eZhZrcOJPHfn2LFjtLW1ce655w77c9TVIyLD1tnZyYwZM5T0R4mZMWPGjIr/h6XELyIVUdIfXdU43vFL/Nv/N+zaVOsoRERqJn6Jf+f9sPuBWkchIiNs3rx5HD16tOI6Q7Vjxw4uvfRSFixYwB133EGpdc3dnTvuuIMFCxawcOFCdu7cmdu2fPlypk6dyg033FDVuPLFL/GnT0GysdZRiEhE3Xbbbaxfv559+/axb98+Hn300T51tmzZktu+fv16brvttty2v/qrv+I73/nOiMYYz8Rf31TrKESkim688UauuOIKLrnkEtavX1+wbf/+/Vx44YWsXr2ahQsX8v73v5+Ojo7c9m984xtcfvnlXHrppezZsweAp59+mne84x287W1v4x3veAd79+4tK47Dhw9z4sQJlixZgplxyy238OCDD/ap99BDD3HLLbdgZixevJjXX3+dw4cPA3DttdcyZcqU4R6KssRvOGemE5JK/CLV9t//7df856ETVf3Mi2efwed//5JB623YsIHp06dz6tQp3v72t/O+972vYPvevXu59957Wbp0KR/60Ie45557+NSnPgXAzJkz2blzJ/fccw933XUX3/72t7nwwgt54oknqK+v5yc/+Qmf/exneeCBB9i7dy9/+Id/WDKGrVu3cvDgQVpaWnJlLS0tHDx4sE/dgwcPMmfOnD71Zs2aVdZxqVT8En9aiV8kau6++242b94MwIEDB9i3b1/B9jlz5rB06VIA/viP/5i77747l/jf+973AnDFFVfwgx/8AIA33niD1atXs2/fPsyMdDoNwAUXXMCuXbv6jaNUf36pUTjl1hspgyZ+M5sDbATeBPQA6939f5rZdOD7wDxgP/BBd3+txPtXA/8tfPkld7+/OqEPU+YU1KuPX6TaymmZj4StW7fyk5/8hCeffJKJEyeybNmyPuPci5Nq/uuGhgYAEokEmUwGgDvvvJNrrrmGzZs3s3//fpYtWwYwaIu/paWFtra2XFlbWxuzZ8/uU7elpYUDBw4MWm+klNPHnwH+0t0vAhYDHzezi4G1wOPufj7wePi6QHhy+DxwFXAl8Hkzm1at4Iespxu6u9TiF4mQN954g2nTpjFx4kT27NnDtm3b+tR56aWXePLJJwHYtGkT73znOwf9zHPOOQeA++67L1eebfGX+pk6dSqzZs1iypQpbNu2DXdn48aNrFq1qs/nr1y5ko0bN+LubNu2jTPPPHPUunmgjMTv7ofdfWf4PAU8B5wDrAKyrff7gRtLvP13gcfc/Xj4v4HHgOXVCHxY0qeCR7X4RSJj+fLlZDIZFi5cyJ133snixYv71Lnooou4//77WbhwIcePHy8YRVPKX//1X/OZz3yGpUuX0t3dPaR4vvWtb/GRj3yEBQsWcN5553H99dcDsG7dOtatWwfAihUrmD9/PgsWLOCjH/0o99xzT+79V199NR/4wAd4/PHHaWlp4Uc/+tGQ9l8OK9XX1G9ls3nAE8BbgZfcfWrettfcfVpR/U8Bje7+pfD1ncApd7+rxGevAdYAzJ0794oXXyx7TYHynTwKXzsPrv8aXLWm+p8vEjPPPfccF110Ua3DGND+/fu54YYb2L17d61DqZpSx93Mdrh7aznvL3s4p5lNBh4APunu5V66L3W1ouSZxt3Xu3uru7c2N5e1etjQZVv8GscvIjFWVuI3syRB0v+uu/8gLH7FzGaF22cBr5Z4axswJ+91C3Bo+OFWKBNe8NE4fpHYmDdvXqRa+9UwaOK34PL3vcBz7v71vE0PA6vD56uBh0q8/UfAdWY2Lbyoe11YVhtq8YuIlNXiXwr8CfAuM9sV/qwAvgK8x8z2Ae8JX2NmrWb2bQB3Pw58EfhF+POFsKw2chd31eIXkfgadBy/u/+M0n31ANeWqL8d+Eje6w3AhuEGWFWZbItfiV9E4itec/Wkwz5+dfWISIzFK/Fn1NUjEhdjeVrmPXv2sGTJEhoaGrjrrj6j20dcvBK/WvwiMsLKmZZ5+vTpBfMFjbaYJf5wKla1+EUiZbxNy3zWWWfx9re/nWQyWcG3Hr54zc6Z6dviz/RkOJk+yeTkZBJ1iRoFJhIBW9bCy89W9zPfdClc/5VBq423aZlrLV6JPzeOf2Ku6PnXn+cD//YB/n7Z3/PuN7+7RoGJSCXG27TMtRavxJ/pBAwSE3JFqa4UAJMnTK5RUCIRUUbLfCSMx2mZay1eiT99KhjDn/dLb+9qB2ByUolfZDwayrTMS5Ysqcq0zP2ZOnVqblrmq666io0bN/KJT3xieF9sBMXs4m7fRVja00r8IuPZeJyW+eWXX6alpYWvf/3rfOlLX6KlpYUTJ6q7bOVAhjQt82hpbW317du3V/+DH/wYvLAV/uI/c0Wb9mziy099mZ9+8KfMbJpZ/X2KRJimZa6NUZuWORKyXT15sl09UyaM7Kr2IiJjRaQTf9dLL/H6gw/S3X4yKMh09hnDn0qnSNYlaUg01CBCERlpmpa5r0gn/o4dOzm89jN0vx6uAZ8+1eeu3faudrX2RSRWIp34Pd0FgGXvjst0lry4Oyk5abRDExGpmWgn/nBMrtWHo1bTHSX7+DWiR0TiJNKJnz6Jv7Nv4k+rq0dE4qWcpRc3mNmrZrY7r+z7eatx7Tezknc0hNueDeuNwPjMgXm6KPFnTvW9uNuVUotfJILG8rTM7s4dd9zBggULWLhwITt37sxtSyQSXHbZZVx22WWsXLmyqrFllXPn7n3AN4GN2QJ3z92zbGZ/B7wxwPuvcffqHtkyZbt6yPbxpzv7XtxNt2u6BhGpmuy0zIsXL2bFihU8+uijuZu4srZs2ZKbtvmpp57itttu46mnngKgqalpwLuDq2HQFr+7PwGUXCc3XIj9g8CmKsdVFZ4JJlbqbfH3Hc6pUT0i4994m5b5oYce4pZbbsHMWLx4Ma+//jqHDx+u4AgMTaVz9VwNvOLu+/rZ7sCPzcyB/+Xu6/uph5mtAdYAzJ07t8Kwwp1nMlBXh9WF57d0R0GLv8d7OJk+qVE9IlXw1ae/yp7je6r6mRdOv5BPX/npQeuNt2mZDx48yJw5c/rUmzVrFp2dnbS2tlJfX8/atWu58cYbyzpWQ1Fp4r+ZgVv7S939kJmdBTxmZnvC/0H0EZ4U1kMwZUOFcQXS6d7WfncGejIFLf6OdAeOMyWpFr/IeDbepmUeqN5LL73E7NmzeeGFF3jXu97FpZdeynnnnTfwARiiYSd+M6sH3gtc0V8ddz8UPr5qZpuBK4GSiX8keDqTN4Y/Oxd/b+LPTdCmPn6RipXTMh8J43Fa5paWFg4cOFCyXvZx/vz5LFu2jF/+8pdVT/yVDOd8N7DH3dtKbTSzSWY2JfscuA4Y1fumPZMpHMoJBYlfc/GLjH9DmZYZqMq0zKV+pk6dyqxZs3LTMrs7GzduZNWqVX0+f+XKlWzcuBF3Z9u2bZx55pnMmjWL1157jdOnTwNw9OhRfv7zn3PxxRcP99D0q5zhnJuAJ4ELzKzNzD4cbrqJom4eM5ttZo+EL88GfmZmzwBPA//u7n1XHR5Bnsn0jujJtvjz7tzNtvjV1SMyfo3HaZlXrFjB/PnzWbBgAR/96Ee55557gGDWzdbWVhYtWsQ111zD2rVrRyTxR3pa5kOf+xwnf/7/OH/rT+HIXviHK+F998Kl7wfgibYn+PjjH+efVvwTi5oXVbw/kbjRtMy1oWmZB1LQ1dO3xX8yHczaqRu4RCROIp34PZ0pHMMPpfv4lfhFIkvTMvcV8cSfxpJFLf4So3p0A5eIxEm0E3/Bxd2wxZ9/cbernYQlaCq6m1dEJMoin/itPjtPT98Wf6orxaTkpJI3WIiIRFXEE3960Iu76uYRkbiJdOKn4OJutsU/Mbc5lU5pnh6RiBrL0zJv3bqVM888Mzf98he+8IWqxjCYSufqGdM8k6FuYpjoc3fuFvbxa0SPiFRTOdMyA1x99dX88Ic/rEGEEW/xBxd3i1r89YWjetTVIzL+jbdpmWst2i3+dDrv4m4nWB0kkrntqa4U502t7uRHInH18pe/zOnnqjstc8NFF/Kmz3520HrjbVpmgCeffJJFixYxe/Zs7rrrLi655JJyD0vFop34i+/crW+CvBE87Wl19YhEwXiblvnyyy/nxRdfZPLkyTzyyCPceOONfWIeSRFP/OnCaZnz+vfdnZNdGtUjUi3ltMxHwniclvmMM87IPV+xYgUf+9jHOHr0KDNnzhzCNx++SCf+glE96c6CET2d3Z1kPKNRPSLj3FCmZV6yZElVpmXuz9SpU3PTMl911VVs3LiRT3ziE33qvfzyy5x99tmYGU8//TQ9PT3MmDGjzG9cuchf3LX8i7tFd+2CpmQWGe/G47TM//qv/8pb3/pWFi1axB133MH3vve9Ub2RNNItfs9koKDF35v4U2ktwiISBQ0NDWzZsqVP+f79+wFob2+nrq4ul3RL1QFobW1l69atACxZsoTf/OY3uW1f/OIXy46ntbW15KRwt956a+757bffzu233172Z1Zb9Fv8uVE9HYVDObs0QZuIxFM5K3BtMLNXzWx3XtnfmtlBM9sV/qzo573LzWyvmT1vZmurGXg5PH+x9Uxnn5u3QFMyi0SdpmXuq5wW/33A8hLlf+/ul4U/jxRvNLME8A/A9cDFwM1mVv01xAbQZzhn3sVdLbQuUh1jcRW/KKvG8R408bv7E8DxYXz2lcDz7v6Cu3cB3wP6rjo8Qtwd8ufjz3SWXG9XLX6R4WtsbOTYsWNK/qPE3Tl27BiNjY2DVx5AJRd3bzezW4DtwF+6+2tF288BDuS9bgOuqmB/QxNeic+N4093ll59Sy1+kWHLjls/cuRIrUOJjcbGxoK7g4djuIn/W8AXAQ8f/w74UFGdUmOT+m0WmNkaYA3A3LlzhxlW3o7CGzGo72c4Z9jin1Svcfwiw5VMJjn33HNrHYYM0bBG9bj7K+7e7e49wD8SdOsUawPm5L1uAQ4N8Jnr3b3V3Vubm5uHE1bh54WJv2AhlmThqJ5JyUkk6hIV70tEZDwZVuI3s1l5L/8AKHXJ/BfA+WZ2rplNAG4CHh7O/obDw7k1CufqyRvH35VS/76IxNKgXT1mtglYBsw0szbg88AyM7uMoOtmP/BnYd3ZwLfdfYW7Z8zsduBHQALY4O6/HpFvUUIu8SfroTsN3l0wqkerb4lIXA2a+N395hLF9/ZT9xCwIu/1I0CfoZ6jItfVU5+33m7hnbuap0dE4iiyd+4WXNzNhDP1Fc3VoxE9IhJHkU/8Vp8MpmuAwou76XZN0CYisRTdxJ8OE38y2bvebvHFXbX4RSSGopv4M3kXd7Pr7RZf3FWLX0RiKLKJv/DibtjiDy/uprvTnO4+rRa/iMRSZBN/wTj+bIs/nJY5Oxe/RvWISBxFN/Hnj+opGs6pufhFJM6im/jT+aN6Srf4deeuiMRRdBN/wcXdbB9/kPjV4heROItw4s8fzpnt6gkTv+biF5EYi2ziLxjVU3Tnbm7ZRY3qEZEYimzi95LDOdXiFxGJbuLvCvr4yU7ZYAlIBHPza/UtEYmz6Cb+XB9/2NVTtAhLY6KRZF2yVuGJiNRMhBN/3g1cxatvpTUzp4jEV2QTf5+Lu/VFiV/9+yISU4MmfjPbYGavmtnuvLKvmdkeM/uVmW02s6n9vHe/mT1rZrvMbHs1Ax+MFy/Ekiyci19j+EUkrspp8d8HLC8qewx4q7svBH4DfGaA91/j7pe5e+vwQhyewmmZi9bb1epbIhJjgyZ+d38COF5U9mN3DyfDYRvQMgKxVSQ3V08yGUzSVnRxVy1+EYmravTxfwjY0s82B35sZjvMbE0V9lU2T6chkcDMgnH8RYlfffwiEleDLrY+EDP7HJABvttPlaXufsjMzgIeM7M94f8gSn3WGmANwNy5cysJCwhG9Vh9+PUyp2DijNw2jeoRkTgbdovfzFYDNwB/5O5eqo67HwofXwU2A1f293nuvt7dW929tbm5ebhh9cpkehN/ujN3cbe7p5uOTIdW3xKR2BpW4jez5cCngZXu3tFPnUlmNiX7HLgO2F2q7kjwdH7iP5UbzpmbrkEtfhGJqXKGc24CngQuMLM2M/sw8E1gCkH3zS4zWxfWnW1mj4RvPRv4mZk9AzwN/Lu7Pzoi36IEz2SCC7sQXtwNJ2jTPD0iEnOD9vG7+80liu/tp+4hYEX4/AVgUUXRVcD7dPUEC61rZk4RibvI3rnrmXQwht89aPGH4/hzE7SpxS8iMRXdxJ8OR/V0p8F7cl09J9MnAa2+JSLxFdnEnxvVkw6vPWu9XRERIMKJ39MZKFhvV6tviYhAlBN/JoPV5623WzycUy1+EYmpiCf+/BZ/2NXTlaK+rp6GREMNoxMRqZ0IJ/5075TM0Lveblc7U5JTgjl8RERiKLKJn3RRi7++9wYu9e+LSJxFNvF7OhzHnx3Vk+zt41f/vojEWXQTfyYc1ZMuavFrLn4RiblIJ36rT+Zd3A2mbNDqWyISdxFP/PkXd9XiFxGBSCf+dNHFXfXxi4hAhBM/6QyWzJuyIdmIu3MyfVKjekQk1iKb+HtH9fS2+DsyHfR4j1bfEpFYi37iz5yCuiQk6nunZFaLX0RirKzEb2YbzOxVM9udVzbdzB4zs33h47R+3rs6rLMvXKd3VHgm09viz7trFzRPj4jEW7kt/vuA5UVla4HH3f184PHwdQEzmw58HriKYKH1z/d3gqg2T6eDpRfzFmHRersiImUmfnd/AjheVLwKuD98fj9wY4m3/i7wmLsfd/fXgMfoewKpOu/uhp6esMWv9XZFRPJV0sd/trsfBggfzypR5xzgQN7rtrBsRHk6DdCb+OsLu3o0jl9E4mykL+6WmgLTS1Y0W2Nm281s+5EjRyraaS7xZ+/cDVv8Wn1LRKSyxP+Kmc0CCB9fLVGnDZiT97oFOFTqw9x9vbu3untrc3NzBWGVaPGH0zWoxS8iUlnifxjIjtJZDTxUos6PgOvMbFp4Ufe6sGxEeToDhIk/05m7uJvqSlFndTSFXT8iInFU7nDOTcCTwAVm1mZmHwa+ArzHzPYB7wlfY2atZvZtAHc/DnwR+EX484WwbEQVtvg7C6ZknpScpEVYRCTW6sup5O4397Pp2hJ1twMfyXu9AdgwrOiGydNdAL3z8Yct/pPpk7prV0RiL5J37ha0+DOdBevtagy/iMRdtBP/hOzFXc3MKSKSFcnET244Z33BxV3NxS8iEtHE75lwVE92IZa8rh6tviUicRfNxJ9t8dcBeMFcPWrxi0jcRTzx9wQFySbcXX38IiJEPfFbb+I/3X2aTE9Go3pEJPYinvi7g4L6ptzMnBrHLyJxF4/En2zU6lsiIqGIJv5M+CR8rG/SBG0iIqGIJv5siz9M/MnGXFePhnOKSNxFO/Hnt/i1+paICBD1xG/BI8mmXB//GRPOqFVYIiJjQrQTv/dN/Lq4KyJxF9HEH07LTPBIfdDHb5j6+EUk9qKZ+DMZqKvDuk8HBclgVM+k5CTqLJJfWUSkbMPOgmZ2gZntyvs5YWafLKqzzMzeyKvzN5WHXIZ0une9XYD6Rk50ndBQThERylyBqxR33wtcBmBmCeAgsLlE1f9w9xuGu59hxZZN/JnOoCBs8at/X0Skel091wK/dfcXq/R5FfH8Fn9iAtQlgpk5NV2DiEjVEv9NwKZ+ti0xs2fMbIuZXVKl/Q2ooMVfr2UXRUTyVZz4zWwCsBL4lxKbdwJvdvdFwDeABwf4nDVmtt3Mth85cqSimLwr3bvQejKYiz/VlVIfv4gI1WnxXw/sdPdXije4+wl3bw+fPwIkzWxmqQ9x9/Xu3ururc3NzRUF1NvV01mwCIvu2hURqU7iv5l+unnM7E1mZuHzK8P9HavCPgfkmQyWrIfMKUhODBZh0Xq7IiJABaN6AMxsIvAe4M/yym4FcPd1wPuB2yyYLe0UcJO7eyX7LIen05Bt8ScbOZU5RcYzavGLiFBh4nf3DmBGUdm6vOffBL5ZyT6GFVfu4u7JwkVY1OIXEYnonbvpwou7motfRKRXxBN/MJwzlQ4naFNXj4hIxBN/5lTBsotq8YuIRDXxZzJYckJ4cVfLLoqI5Itm4k93YfXhcE519YiIFIho4s/r49fFXRGRAhFO/Hkt/q4UCUvQFM7bIyISZ9FN/Inwq4UXdydPmEx4E7GISKxFMvHTlcYSYZJPTtQ8PSIieSKZ+D2T6U389Y2amVNEJE80E386TW5p3WSTEr+ISJ7IJX53DxN/OBdcfaO6ekRE8kQu8dPdDe4FLX5NySwi0ityid/TaYDeFn/Y1aMWv4hIIPKJvyfRECy0rha/iAgQxcSfyQRPLEj8HXXguBK/iEioGout7zezZ81sl5ltL7HdzOxuM3vezH5lZpdXus+B5Fr81g1Ae7jgl7p6REQCFa3Alecadz/az7brgfPDn6uAb4WPI6I48Z/w4PXkCUr8IiIwOl09q4CNHtgGTDWzWSO1M+/KJv4eANo96PpRV4+ISKAaid+BH5vZDjNbU2L7OcCBvNdtYdmI6G3xBwm/vSd4PSWpxC8iAtXp6lnq7ofM7CzgMTPb4+5P5G0vNTOaFxeEJ401AHPnzh12MLnET9DVk+rpAtTVIyKSVXGL390PhY+vApuBK4uqtAFz8l63AIdKfM56d29199bm5ubhB5TJa/EnGkilNRe/iEi+ihK/mU0ysynZ58B1wO6iag8Dt4SjexYDb7j74Ur2O5Bci9/TwSIsSvwiIgUq7eo5G9gcznNfD/yzuz9qZrcCuPs64BFgBfA80AH8aYX7HFBvV086twhLsi5JQ6JhJHcrIjJuVJT43f0FYFGJ8nV5zx34eCX7GVJMBS1+zcwpIlIsenfu5hJ/V26CNt28JSLSK7qJn3SwCEtaLX4RkXzRS/zhXD3mp3tb/BrKKSKSE73En71zt+d077KLunlLRCQneok/18cftPhT6ZRa/CIieaKX+LuCO3Wtp1Orb4mIlBC5xN9zMrhhq67uNJlEAx2ZDnX1iIjkiVzi7061Y42NWNcbnGyYCGieHhGRfJFL/D2pFHWTJ0O6g1SyCdAiLCIi+SKX+LvbUyQmBy399gmNAJwx4YxahiQiMqZELvH3nEhRNzFI+KlEElBXj4hIvsgl/u72FImmCQCk6oOpiJT4RUR6RS7x96TaqWsKWvqpYNZQzkiqq0dEJCtyib87dYJEQ/C1jnkwpn9G04xahiQiMqZELvH3pNqpC3p6OJLpoKm+iYnJibUNSkRkDIlU4vd0Gu/sJJHsAeBYup3mpgqWcRQRiaBhJ34zm2NmPzWz58zs12b25yXqLDOzN8xsV/jzN5WFO7Du9vCu3WQ3JCdxpPMYM5tmjuQuRUTGnUpW4MoAf+nuO8N1d3eY2WPu/p9F9f7D3W+oYD9l6zlxAoBEIg1NUzl66ihvmfaW0di1iMi4MewWv7sfdved4fMU8BxwTrUCG47uVO88PTQGiV8tfhGRQlXp4zezecDbgKdKbF5iZs+Y2RYzu6Qa++tPT3sKgDrr4FTTGbSn22meqD5+EZF8FSd+M5sMPAB80t1PFG3eCbzZ3RcB3wAeHOBz1pjZdjPbfuTIkWHF0p3t6uEkRxuCm7ZmNGoop4hIvooSv5klCZL+d939B8Xb3f2Eu7eHzx8BkmZWsu/F3de7e6u7tzY3D6+V3pPt6iHFsXCeHrX4RUQKVTKqx4B7gefc/ev91HlTWA8zuzLc37Hh7nMw2a6eRPcbHAmna1Afv4hIoUpG9SwF/gR41sx2hWWfBeYCuPs64P3AbWaWAU4BN7m7V7DPAeUu7nKSo4kEoMQvIlJs2Inf3X8G2CB1vgl8c7j7GKqe1AnqJk3E6uCI9ZCwBNMapo3W7kVExoVI3bnbnWqnblIwPcMxMkxvnE6iLlHjqERExpZIJf6eVIrExAYAjnSfVjePiEgJkUr83akUdU1B4j/a3aHELyJSQqQSf08qRV1jgk4z2k4d4exJZ9c6JBGRMSdSib87lSJBOw9OnkQq08Hvnft7tQ5JRGTMqWQ455iTOnqQVNNJNjTP5tIZb+GKs6+odUgiImNOZBL/a8dfInG6mx9Ob+Kwd/LbX17G5Tse46wpjUxsSNCUDH4aJySYkKjDDOrMSJhRVxc8rzMjUWeYEZZbWE5QDlhYL3h/8NoMDAtfE24P6mfr1AVvDl7n1c19HlBXF2yzvPf01rXezx5K3TBO8rbVhdsgv04Zdeuy3ym739599pYVxTlIXREZfZFJ/NOmz2XC1h8y67928u7jB5nfcj0vn+jiSOo0p9LddKa7OdGZpqOrm0y3093juDs9Dt0ePO/uCV739Dg97nR77+ugTq2/ZfQUnyQITz59ywpPKLmTXVGZlaxbWAZ9T3i9J8+ik1iV62KlToqDfBfCk26pBsYQ6hZ/74EaI9n4Bm5glF+X3D77b2D0vjd8X13pfZWqX/w77+84W9HvreTfA737i6rIJH6ASc3n8afN543oPjw8AfS444RK3M1rAAAF+0lEQVSPTvBDcKLInlBK1cUJtlGiTl7d3vKwbk/4fgapm7/vQer2xt63bnY/PT0UxV78vcPX4UnTBzxG2dhKxVN8jArr9ve9g/h66w70vcuqm3e8s3W7w4ZAye9C7zEa9HuHdfv9LiX+dvqNL6+ujJz+GibZk07xiaKu6ARlefULeghKlQEzJjXwf25dMuLfK1KJfzTkfpkD37QsMmqKTxIDNUaKT94DNUayJ5uBGiNOGXXzTnx9GjkF5b0n4z77KjgZFtUvqNP7/Sg+mdJ/fQgaLwUn7II4+jZMik/MxQ2JPu/P+z6F9XvrTWkcnZSsxC8yzmUbIwAJNUikDJEazikiIoNT4hcRiRklfhGRmFHiFxGJGSV+EZGYUeIXEYkZJX4RkZhR4hcRiRkbwbXPh83MjgAvDvPtM4GjVQynWhTX0I3V2BTX0CiuoRtObG929+ZyKo7JxF8JM9vu7q21jqOY4hq6sRqb4hoaxTV0Ix2bunpERGJGiV9EJGaimPjX1zqAfiiuoRursSmuoVFcQzeisUWuj19ERAYWxRa/iIgMIDKJ38yWm9leM3vezNbWMI45ZvZTM3vOzH5tZn8elv+tmR00s13hz4oaxbffzJ4NY9gelk03s8fMbF/4OG2UY7og77jsMrMTZvbJWhwzM9tgZq+a2e68spLHxwJ3h39zvzKzy2sQ29fMbE+4/81mNjUsn2dmp/KO3bpRjqvf352ZfSY8ZnvN7HdHOa7v58W038x2heWjebz6yxGj93cWrPwyvn+ABPBbYD4wAXgGuLhGscwCLg+fTwF+A1wM/C3wqTFwrPYDM4vK/gewNny+FvhqjX+XLwNvrsUxA34HuBzYPdjxAVYAWwhWzVsMPFWD2K4D6sPnX82LbV5+vRrEVfJ3F/5beAZoAM4N/90mRiuuou1/B/xNDY5Xfzli1P7OotLivxJ43t1fcPcu4HvAqloE4u6H3X1n+DwFPAecU4tYhmAVcH/4/H7gxhrGci3wW3cf7g18FXH3J4DjRcX9HZ9VwEYPbAOmmtms0YzN3X/s7pnw5TagZaT2P5S4BrAK+J67n3b3/wKeJ/j3O6pxWbCS+geBTSOx74EMkCNG7e8sKon/HOBA3us2xkCyNbN5wNuAp8Ki28P/qm0Y7e6UPA782Mx2mNmasOxsdz8MwR8lcFaNYgO4icJ/jGPhmPV3fMba392HCFqGWeea2S/N7P+a2dU1iKfU726sHLOrgVfcfV9e2agfr6IcMWp/Z1FJ/KUWGq3pcCUzmww8AHzS3U8A3wLOAy4DDhP8N7MWlrr75cD1wMfN7HdqFEcfZjYBWAn8S1g0Vo5Zf8bM352ZfQ7IAN8Niw4Dc939bcBfAP9sZmeMYkj9/e7GyjG7mcIGxqgfrxI5ot+qJcoqOmZRSfxtwJy81y3AoRrFgpklCX6h33X3HwC4+yvu3u3uPcA/MkL/vR2Mux8KH18FNodxvJL9r2P4+GotYiM4Ge1091fCGMfEMaP/4zMm/u7MbDVwA/BHHnYKh10px8LnOwj60t8yWjEN8Lur+TEzs3rgvcD3s2WjfbxK5QhG8e8sKon/F8D5ZnZu2Gq8CXi4FoGEfYf3As+5+9fzyvP75P4A2F383lGIbZKZTck+J7gwuJvgWK0Oq60GHhrt2EIFrbCxcMxC/R2fh4FbwlEXi4E3sv9VHy1mthz4NLDS3TvyypvNLBE+nw+cD7wwinH197t7GLjJzBrM7NwwrqdHK67Qu4E97t6WLRjN49VfjmA0/85G4yr2aPwQXPn+DcGZ+nM1jOOdBP8N+xWwK/xZAXwHeDYsfxiYVYPY5hOMqHgG+HX2OAEzgMeBfeHj9BrENhE4BpyZVzbqx4zgxHMYSBO0tD7c3/Eh+C/4P4R/c88CrTWI7XmC/t/s39q6sO77wt/xM8BO4PdHOa5+f3fA58Jjthe4fjTjCsvvA24tqjuax6u/HDFqf2e6c1dEJGai0tUjIiJlUuIXEYkZJX4RkZhR4hcRiRklfhGRmFHiFxGJGSV+EZGYUeIXEYmZ/w8HVtXuwqX4gAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(200),np.log(loss_hist[:200]),label=\"alpha=0.01\")\n",
    "plt.plot(range(200),np.log(loss_hist_01[:200]),label=\"alpha=0.1\")\n",
    "plt.plot(range(200),np.log(loss_hist_005[:200]),label=\"alpha=0.05\")\n",
    "plt.plot(range(200),np.log(loss_hist_05[:200]),label=\"alpha=0.5\")\n",
    "plt.legend(loc = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hist_5,loss_hist_5=batch_grad_descent(X_train, y_train, alpha=0.005, num_iter=1000, check_gradient=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21bff7a00f0>]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss_hist_5)),np.log(loss_hist_5),label=\"alpha=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "1) $J(\\theta) = \\frac{1}{m}(X\\theta-y)^{T}(X\\theta-y)+\\lambda\\theta^{T}\\theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $\\triangledown J(\\theta) = \\frac{1}{m}(X^TX\\theta-X^Ty)+2\\lambda\\theta^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularized_loss(X,y,theta,lambda_reg):\n",
    "    num_instances=y.shape[0]\n",
    "    y_predict=np.dot(X,theta)\n",
    "    #y=y.reshape(y.shape[0],1)\n",
    "    differences = y_predict-y\n",
    "    square_loss=(1/(2*num_instances))*np.dot(differences,differences)+lambda_reg*np.dot(theta,theta)\n",
    "    return square_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 48)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    n=X.shape[0]\n",
    "    gradient_ridge=(1/n)*(np.dot(np.dot(X.T,X),theta)-np.dot(X.T,y))+2*lambda_reg*theta\n",
    "    return gradient_ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.28077893,  -9.14335642,  -8.4655793 ,  -8.2366148 ,\n",
       "        -7.82321374,  -7.5951364 ,  -6.61276415,  -6.75228758,\n",
       "        -6.9818213 ,  -5.96551875,  -4.91428957,  -4.47782702,\n",
       "        -3.29042777,  -2.60406641,  -2.21903578,  -1.88592946,\n",
       "        -1.34845177,  -0.26550076,  -7.37628333,  -7.47213347,\n",
       "       -38.18929205,  -6.81210909,  -6.71145625, -34.3470937 ,\n",
       "        -5.94079531,  -5.83338795, -29.17735801,  -4.90598117,\n",
       "        -5.19814195, -26.35431336,  -4.3713358 ,  -4.32312618,\n",
       "       -23.13284794,   7.29263034,   7.02098944,  34.68227125,\n",
       "         5.96589353,   6.13870023,  30.29035801,   5.47231228,\n",
       "         5.53061729,  26.1636343 ,   4.81225064,   4.72162541,\n",
       "        22.65428378,   3.8960197 ,   4.18774606,  19.97950809])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_regularized_square_loss_gradient(X_train, y_train, theta,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        numIter - number of iterations to run\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features)\n",
    "        loss_hist - the history of loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    (num_instances, num_features) = X.shape\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #Initialize loss_hist\n",
    "    \n",
    "    theta_hist[0]=theta\n",
    "    for i in range(num_iter):\n",
    "        if grad_checker(X,y,theta_hist[i,:])==False:\n",
    "                print(\"Alpha:\",alpha)\n",
    "                print(\"Gradient Fail! at step\",i)\n",
    "                #break\n",
    "        theta_hist[i+1,:]=theta_hist[i,:]-alpha*compute_regularized_square_loss_gradient(X, y, theta_hist[i,:])\n",
    "        loss_hist[i]=compute_regularized_loss(X, y, theta_hist[i,:])\n",
    "    return theta_hist,loss_hist\n",
    "    \n",
    "    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05\n",
      "Gradient Fail! at step 10\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 11\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 12\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 13\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 14\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 15\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 16\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 17\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 18\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 19\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 20\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 21\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 22\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 23\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 24\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 25\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 26\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 27\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 28\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 29\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 30\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 31\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 32\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 33\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 34\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 35\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 36\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 37\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 38\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 39\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 40\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 41\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 42\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 43\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 44\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 45\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 46\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 47\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 48\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 49\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 50\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 51\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 52\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 53\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 54\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 55\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 56\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 57\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 58\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 59\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 60\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 61\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 62\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 63\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 64\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 65\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 66\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 67\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 68\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 69\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 70\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 71\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 72\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 73\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 74\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 75\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 76\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 77\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 78\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 79\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 80\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 81\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 82\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 83\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 84\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 85\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 86\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 87\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 88\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 89\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 90\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 91\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 92\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 93\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 94\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 95\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 96\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 97\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 98\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 99\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 100\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 101\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 102\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 103\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 104\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 105\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 106\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 107\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 108\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 109\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 110\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 111\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 112\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 113\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 114\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 115\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 116\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 117\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 118\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 119\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 120\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 121\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 122\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 123\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 124\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 125\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 126\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 127\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 128\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 129\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 130\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 131\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 132\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 133\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 134\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 135\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 136\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 137\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 138\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 139\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 140\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 141\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 142\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 143\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 144\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 145\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 146\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 147\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 148\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 149\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 150\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 151\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 152\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 153\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 154\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 155\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 156\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 157\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 158\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 159\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 160\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 161\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 162\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 163\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 164\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 165\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 166\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 167\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 168\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 169\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 170\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 171\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 172\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 173\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 174\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 175\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 176\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 177\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 178\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 179\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 180\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 181\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 182\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 183\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 184\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 185\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 186\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 187\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 188\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 189\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 190\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 191\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 192\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 193\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 194\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 195\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 196\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 197\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 198\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 199\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 200\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 201\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 202\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 203\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 204\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 205\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 206\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 207\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 208\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 209\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 210\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 211\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 212\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 213\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 214\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 215\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 216\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 217\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 218\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 219\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 220\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 221\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 222\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 223\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 224\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 225\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 226\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 227\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 228\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 229\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 230\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 231\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 232\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 233\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 234\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 235\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 236\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 237\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 238\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 239\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 240\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 241\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 242\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 243\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 244\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 245\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 246\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 247\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 248\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 249\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 250\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 251\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 252\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 253\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 254\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 255\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 256\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 257\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 258\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 259\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05\n",
      "Gradient Fail! at step 261\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 262\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 263\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 264\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 265\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 266\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 267\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 268\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 269\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 270\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 271\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 272\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 273\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 274\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 275\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 276\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 277\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 278\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 279\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 280\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 281\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 282\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 283\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 284\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 285\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 286\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 287\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 288\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 289\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 290\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 291\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 292\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 293\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 294\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 295\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 296\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 297\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 298\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 299\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 300\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 301\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 302\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 303\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 304\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 305\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 306\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 307\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 308\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 309\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 310\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 311\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 312\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 313\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 314\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 315\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 316\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 317\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 318\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 319\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 320\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 321\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 322\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 323\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 324\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 325\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 326\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 327\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 328\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 329\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 330\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 331\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 332\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 333\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 334\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 335\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 336\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 337\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 338\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 339\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 340\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 341\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 342\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 343\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 344\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 345\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 346\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 347\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 348\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 349\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 350\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 351\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 352\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 353\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 354\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 355\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 356\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 357\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 358\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 359\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 360\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 361\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 362\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 363\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 364\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 365\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 366\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 367\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 368\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 369\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 370\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 371\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 372\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 373\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 374\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 375\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 376\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 377\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 378\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 379\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 380\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 381\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 382\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 383\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 384\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 385\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 386\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 387\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 388\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 389\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 390\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 391\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05\n",
      "Gradient Fail! at step 393\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 394\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 395\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 396\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 397\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 398\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 399\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 400\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 401\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 402\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 403\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 404\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 405\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 406\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 407\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 408\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 409\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 410\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 411\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 412\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 413\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 414\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 415\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 416\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 417\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 418\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 419\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 420\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 421\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 422\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 423\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 424\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 425\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 426\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 427\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 428\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 429\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 430\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 431\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 432\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 433\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 434\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 435\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 436\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 437\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 438\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 439\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 440\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 441\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 442\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 443\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 444\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 445\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 446\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 447\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 448\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 449\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 450\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 451\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 452\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 453\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 454\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 455\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 456\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 457\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 458\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 459\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 460\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 461\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 462\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 463\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 464\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 465\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 466\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 467\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 468\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 469\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 470\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 471\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 472\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 473\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 474\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 475\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 476\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 477\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 478\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 479\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 480\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 481\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 482\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 483\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 484\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 485\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 486\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 487\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 488\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 489\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 490\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 491\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 492\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 493\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 494\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 495\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 496\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 497\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 498\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 499\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 500\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 501\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 502\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 503\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 504\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 505\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 506\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 507\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 508\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 509\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 510\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 511\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 512\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 513\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 514\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 515\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 516\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 517\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 518\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 519\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 520\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 521\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 522\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 523\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 524\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 525\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 526\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 527\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 528\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 529\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 530\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 531\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 532\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 533\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 534\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 535\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 536\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 537\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 538\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 539\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 540\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 541\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 542\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 543\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 544\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 545\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 546\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 547\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 548\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 549\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 550\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 551\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 552\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 553\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 554\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 555\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 556\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 557\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 558\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 559\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 560\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 561\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 562\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 563\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 564\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 565\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 566\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 567\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 568\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 569\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 570\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 571\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 572\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 573\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 574\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 575\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 576\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 577\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 578\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 579\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 580\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 581\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 582\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 583\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 584\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 585\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 586\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 587\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 588\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 589\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 590\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 591\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 592\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 593\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 594\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 595\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 596\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 597\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 598\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 599\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 600\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 601\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 602\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 603\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 604\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 605\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 606\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 607\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 608\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 609\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 610\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 611\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 612\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 613\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 614\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 615\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 616\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 617\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 618\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 619\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 620\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 621\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 622\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 623\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 624\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 625\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 626\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 627\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 628\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 629\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 630\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 631\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 632\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 633\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 634\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 635\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 636\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 637\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 638\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 639\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 640\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 641\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 642\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 643\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 644\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 645\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 646\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 647\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 648\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 649\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 650\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 651\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 652\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 653\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 654\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 655\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 656\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 657\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 658\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 659\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 660\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 661\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 662\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 663\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 664\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 665\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 666\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 667\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 668\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 669\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 670\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 671\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 672\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 673\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 674\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 675\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 676\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 677\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 678\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 679\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 680\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 681\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 682\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 683\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 684\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 685\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 686\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 687\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 688\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 689\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 690\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 691\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 692\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 693\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 694\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 695\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 696\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 697\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 698\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 699\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 700\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 701\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 702\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 703\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 704\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 705\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 706\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 707\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 708\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 709\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 710\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 711\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 712\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 713\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 714\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 715\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 716\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 717\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 718\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 719\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 720\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 721\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 722\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 723\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 724\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 725\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 726\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 727\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 728\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 729\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 730\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 731\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 732\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 733\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 734\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 735\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 736\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 737\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 738\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 739\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 740\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 741\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 742\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 743\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 744\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 745\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 746\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 747\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 748\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 749\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 750\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 751\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 752\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 753\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 754\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 755\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 756\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 757\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 758\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 759\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 760\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 761\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 762\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 763\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 764\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 765\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 766\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 767\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 768\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 769\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 770\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 771\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 772\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 773\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 774\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 775\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 776\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 777\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 778\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 779\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 780\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 781\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 782\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 783\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 784\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 785\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 786\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 787\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 788\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 789\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 790\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 791\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 792\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 793\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 794\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 795\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 796\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 797\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 798\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 799\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 800\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 801\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 802\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 803\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 804\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 805\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 806\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 807\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 808\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 809\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 810\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 811\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 812\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 813\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 814\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 815\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 816\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 817\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 818\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 819\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 820\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 821\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 822\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 823\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 824\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 825\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 826\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 827\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 828\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 829\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 830\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 831\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 832\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 833\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 834\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 835\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 836\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 837\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 838\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 839\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 840\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 841\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 842\n",
      "Alpha: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Fail! at step 843\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 844\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 845\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 846\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 847\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 848\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 849\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 850\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 851\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 852\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 853\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 854\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 855\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 856\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 857\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 858\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 859\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 860\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 861\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 862\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 863\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 864\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 865\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 866\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 867\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 868\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 869\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 870\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 871\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 872\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 873\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 874\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 875\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 876\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 877\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 878\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 879\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 880\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 881\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 882\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 883\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 884\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 885\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 886\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 887\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 888\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 889\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 890\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 891\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 892\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 893\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 894\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 895\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 896\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 897\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 898\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 899\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 900\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 901\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 902\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 903\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 904\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 905\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 906\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 907\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 908\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 909\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 910\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 911\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 912\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 913\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 914\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 915\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 916\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 917\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 918\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 919\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 920\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 921\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 922\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 923\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 924\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 925\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 926\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 927\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 928\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 929\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 930\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 931\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 932\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 933\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 934\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 935\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 936\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 937\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 938\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 939\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 940\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 941\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 942\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 943\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 944\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 945\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 946\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 947\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 948\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 949\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 950\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 951\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 952\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 953\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 954\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 955\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 956\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 957\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 958\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 959\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 960\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 961\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 962\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 963\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 964\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 965\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 966\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 967\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 968\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 969\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 970\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 971\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 972\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 973\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 974\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 975\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 976\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 977\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 978\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 979\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 980\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 981\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 982\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 983\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 984\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 985\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 986\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 987\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 988\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 989\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 990\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 991\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 992\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 993\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 994\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 995\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 996\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 997\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 998\n",
      "Alpha: 0.05\n",
      "Gradient Fail! at step 999\n"
     ]
    }
   ],
   "source": [
    "regularized_theta_hist,regularized_loss_hist=batch_grad_descent(X_train, y_train,0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "- Stochastic Gradient Descent is very useful when the data size so big that directly differentiating is implausible. And also it is an unbiased estimation. Indeed, the mini-batch gradient regardless the batch size is an unbiased estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features)\n",
    "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_instances, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_instances, num_instances)) #Initialize loss_hist\n",
    "   \n",
    "    \n",
    "    #TODO\n",
    "    for i in range(num_iter):\n",
    "        permutation=np.random.permutation(num_instances)\n",
    "        X_shuffled=X[permutation,:]\n",
    "        y_shuffled=y[permutation]\n",
    "        \n",
    "        for i in range(num_instances):\n",
    "            if alpha=='1/sqrt(t)':\n",
    "                step_size = 1.0/np.sqrt((i+1.0))\n",
    "            elif alpha=='1/t':\n",
    "                step_size = 1.0/(i+1.0)\n",
    "            else:\n",
    "                step_size = alpha\n",
    "            one_instance_gradient=compute_regularized_square_loss_gradient(X[i].reshape(1,num_features),np.array([y[i]]),theta,lambda_reg)\n",
    "            theta_hist[i]=theta\n",
    "            theta=theta-step_size*one_instance_gradient\n",
    "            loss_hist[i]=compute_regularized_loss(X[i].reshape(1,num_features),np.array([y[i]]),theta_hist[i,:],lambda_reg)\n",
    "        \n",
    "    return loss_hist,theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_sto,theta_hist_sto=stochastic_grad_descent(X, y, alpha=0.01, lambda_reg=1, num_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss_hist_sto)),np.log(loss_hist_sto))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('data.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1)))) # Add bias term\n",
    "\n",
    "    # TODO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
