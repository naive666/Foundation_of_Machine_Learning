{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Gaussian Linear Regression  \n",
    "- Input Space: $R^d$  \n",
    "- Output Space: $R^d$  \n",
    "- Family of conditional probability densities  \n",
    "$$\n",
    "y \\mid x, w \\quad \\sim \\quad \\mathcal{N}\\left(w^{T} x, \\sigma^{2}\\right)\n",
    "$$  \n",
    "for some known $\\sigma^2$  \n",
    "- Parameter Space $R^d$  \n",
    "- Data $\\mathcal{D} = \\{y_1,...y_n\\}$  \n",
    "- Assume $y_i$’s are conditionally independent, given $x_i$’s and $w$.  \n",
    "\n",
    "## Gaussian Likelihood and MLE  \n",
    "- The likelihood of $w \\in R^d$ for the data $D$ is given by the likelihood function  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&L_{\\mathcal{D}}(w)=\\prod_{i=1} p\\left(y_{i} \\mid x_{i}, w\\right) \\quad \\text { by conditional independence }\\\\\n",
    "&=\\prod_{i=1}^{n}\\left[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(y_{i}-w^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- You should see in your head1 that the MLE is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w}_{\\mathrm{MLE}} &=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\max } L_{\\mathcal{D}}(w) \\\\\n",
    "&=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\sum_{i=1}^{n}\\left(y_{i}-w^{T} x_{i}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Conditional Probability Models  \n",
    "## Bayesian Conditional Models\n",
    "- Input space $X =R^d$ Outcome space $Y =R$  \n",
    "- Two components to Bayesian conditional model:  \n",
    "  - A parametric family of conditional densities:  \n",
    "$$\n",
    "\\{p(y \\mid x, \\theta): \\theta \\in \\Theta\\}\n",
    "$$  \n",
    "  - A prior distribution for $\\theta\\in \\Theta$.  \n",
    "  \n",
    "## The Posterior Distribution\n",
    "- The posterior distribution for $\\theta$ is  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(\\theta \\mid \\mathcal{D}, x_{1}, \\ldots, x_{n}\\right) & \\propto p\\left(\\mathcal{D} \\mid \\theta, x_{1}, \\ldots, x_{n}\\right) p(\\theta) \\\\\n",
    "&=\\underbrace{L_{\\mathcal{D}}(\\theta)}_{\\text {likelihood prior }} \\underbrace{p(\\theta)}_{ }\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "## Gaussian Example: Priors and Posteriors\n",
    "- Choose a Gaussian prior distribution $p(w)$ on $R^d$:  \n",
    "$$\n",
    "w \\sim \\mathcal{N}\\left(0, \\Sigma_{0}\\right)\n",
    "$$  \n",
    "for some covariance matrix $\\Sigma_{0} \\succ 0$ (i.e. $\\Sigma_0$ is spd).  \n",
    "- Posterior distribution\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(w \\mid \\mathcal{D}, x_{1}, \\ldots, x_{n}\\right)=& p\\left(w \\mid \\mathcal{D}, x_{1}, \\ldots, x_{n}\\right) \\\\\n",
    "\\propto & L_{\\mathcal{D}}(w) p(w) \\\\\n",
    "=& \\prod_{i=1}^{n}\\left[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(y_{i}-w^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}\\right)\\right](\\text { likelihood }) \\\\\n",
    "&\\left.\\times\\left|2 \\pi \\Sigma_{0}\\right|^{-1 / 2} \\exp \\left(-\\frac{1}{2} w^{T} \\Sigma_{0}^{-1} w\\right)\\right)(\\text { prior })\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "## Predictive Distributions\n",
    "- We have a parametric family of conditional densities:  \n",
    "$$\n",
    "\\{p(y \\mid x, \\theta): \\theta \\in \\Theta\\}\n",
    "$$  \n",
    "- Each $p(y | x,\\theta)$ is a conditional density, but also a prediction function  \n",
    "  - For $x \\in X$, the action produced is a probability density on $y$.  \n",
    "- In Bayesian statistics we have two distributions on $\\Theta$:  \n",
    "  - the prior distribution $p(\\theta)$  \n",
    "  - the posterior distribution $p(\\theta | D,x_1,...,x_n)$.  \n",
    "- Each distribution on $\\Theta$ induces a **distributions over prediction functions**  \n",
    "- For any give $x$, this gives a single distribution on $y$.  \n",
    "- This distribution is called a **predictive distribution**  \n",
    "\n",
    "# Gaussian Regression Example  \n",
    "## Example in 1-Dimension: Setup\n",
    "- Input space $X = [−1,1]$ Output space $Y =R$  \n",
    "- Given $x$, the world generates $y$ as  \n",
    "$$\n",
    "y=w_{0}+w_{1} x+\\varepsilon\n",
    "$$  \n",
    "where $\\varepsilon \\sim \\mathcal{N}\\left(0,0.2^{2}\\right)$  \n",
    "- Written another way, the **conditional probability model** is  \n",
    "$$\n",
    "y \\mid x, w_{0}, w_{1} \\sim \\mathcal{N}\\left(w_{0}+w_{1} x, 0.2^{2}\\right)\n",
    "$$  \n",
    "- What’s the parameter space? $R^2$  \n",
    "- **Prior distribution:** $w=\\left(w_{0}, w_{1}\\right) \\sim \\mathcal{N}\\left(0, \\frac{1}{2} l\\right)$  \n",
    "## Example in 1-Dimension: Prior Situation  \n",
    "<div align=\"center\"><img src = \"./1d.jpg\" width = '500' height = '100' align = center /></div>    \n",
    "- On right, $y(x)=\\mathbb{E}[y \\mid x, w]=w_{0}+w_{1} x$, for randomly chosen $w \\sim p(w)=\\mathcal{N}\\left(0, \\frac{1}{2} /\\right)$  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Regression Continued  \n",
    "## Closed Form for Posterior\n",
    "- Model:  \n",
    "$$\n",
    "\\begin{array}{r}\n",
    "w \\quad \\sim \\quad \\mathcal{N}\\left(0, \\Sigma_{0}\\right) \\\\\n",
    "y_{i} \\mid x, w \\quad \\text { i.i.d. } \\quad \\mathcal{N}\\left(w^{T} x_{i}, \\sigma^{2}\\right)\n",
    "\\end{array}\n",
    "$$  \n",
    "- Design matrix $X$ Response column vector $y$  \n",
    "- **Posterior distribution is a Gaussian distribution:** \n",
    "$$\n",
    "\\begin{aligned}\n",
    "w \\mid \\mathcal{D} & \\sim \\mathcal{N}\\left(\\mu_{P}, \\Sigma_{P}\\right) \\\\\n",
    "\\mu_{\\mathbf{P}} &=\\left(X^{T} X+\\sigma^{2} \\Sigma_{0}^{-1}\\right)^{-1} X^{T} y \\\\\n",
    "\\Sigma_{\\mathbf{P}} &=\\left(\\sigma^{-2} X^{T} X+\\Sigma_{0}^{-1}\\right)^{-1}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- Posterior Variance $\\Sigma_P$ gives us a natural uncertainty measure  \n",
    "- For the prior variance $\\Sigma_{0}=\\frac{\\sigma^{2}}{\\lambda} I$, we get  \n",
    "$$\n",
    "\\mu_{P}=\\left(X^{T} X+\\lambda l\\right)^{-1} X^{T} y\n",
    "$$  \n",
    "which is of course the ridge regression solution  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Variance vs. Traditional Uncertainty\n",
    "- Traditional regression: OLS estimator (also the MLE) is a random variable – why?  \n",
    "  - Because estimator is a function of data $D$ and data is random  \n",
    "- Common assumption: data are iid with Gaussian noise: $y=w^{T} x+\\varepsilon,$ with $\\varepsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$  \n",
    "- Then OLS estimator $\\hat{w}$ has a sampling distribution that is Gaussian with mean $w$ and  \n",
    "$$\n",
    "\\operatorname{Cov}(\\hat{w})=\\left(\\sigma^{-2} X^{T} X\\right)^{-1}\n",
    "$$  \n",
    "- By comparison, the posterior variance is  \n",
    "$$\n",
    "\\Sigma_{P}=\\left(\\sigma^{-2} X^{T} X+\\Sigma_{0}^{-1}\\right)^{-1}\n",
    "$$  \n",
    "\n",
    "## Posterior Mean and Posterior Mode (MAP)   \n",
    "- Posterior density for $\\Sigma_0 = \\frac{\\sigma^2}{\\lambda}I$  \n",
    "$$\n",
    "p(w \\mid \\mathcal{D}) \\propto \\underbrace{\\exp \\left(-\\frac{\\lambda}{2 \\sigma^{2}}\\|w\\|^{2}\\right)}_{\\text {prior }} \\underbrace{\\prod_{i=1}^{n} \\exp \\left(-\\frac{\\left(y_{i}-w^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}\\right)}_{\\text {likelihood }}\n",
    "$$  \n",
    "- To ﬁnd MAP, suﬃcient to minimize the negative log posterior  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{w}_{\\mathrm{MAP}} &=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min }[-\\log p(w \\mid \\mathcal{D})] \\\\\n",
    "&=\\underset{w \\in \\mathbf{R}^{d}}{\\arg \\min } \\underbrace{\\sum_{i=1}^{n}\\left(y_{i}-w^{T} x_{i}\\right)^{2}}_{\\text {log-likelihood }}+\\underbrace{\\lambda\\|w\\|^{2}}_{\\text {log-prior }}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "\n",
    "## Predictive Distribution\n",
    "- Given a new input point $x_{new}$, how to predict $y_{new}$ ?  \n",
    "- Predictive distribution  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(y_{\\text {new }} \\mid x_{\\text {new }}, \\mathcal{D}\\right) &=\\int p\\left(y_{\\text {new }} \\mid x_{\\text {new }}, w, \\mathcal{D}\\right) p(w \\mid \\mathcal{D}) d w \\\\\n",
    "&=\\int p\\left(y_{\\text {new }} \\mid x_{\\text {new }}, w\\right) p(w \\mid \\mathcal{D}) d w\n",
    "\\end{aligned}\n",
    "$$  \n",
    "- For Gaussian regression, predictive distribution has closed form.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\text {new }} \\mid x_{\\text {new }}, \\mathcal{D} & \\sim \\mathcal{N}\\left(\\eta_{\\text {new }}, \\sigma_{\\text {new }}\\right) \\\\\n",
    "\\eta_{\\text {new }} &=\\mu_{\\mathrm{P}}^{T} x_{\\text {new }} \\\\\n",
    "\\sigma_{\\text {new }} &=\\underbrace{x_{\\text {new }}^{T} \\Sigma_{\\text {P }} x_{\\text {new }}}_{\\text {from variance in } w}+\\underbrace{\\sigma^{2}}_{\\text {inherent variance in } y}\n",
    "\\end{aligned}\n",
    "$$  \n",
    "With predictive distributions, can give mean prediction with error bands:\n",
    "<div align=\"center\"><img src = \"./narrow.jpg\" width = '500' height = '100' align = center /></div>    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
