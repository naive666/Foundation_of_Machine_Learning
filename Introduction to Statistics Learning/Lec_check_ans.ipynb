{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Check Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 0-1 loss:\n",
    "$$\n",
    "L(\\alpha(x),y) = \n",
    "\\begin{cases}\n",
    "1, \\alpha(x) \\neq y\\\\\n",
    "0, \\alpha(x) = y\n",
    "\\end{cases}\n",
    "$$\n",
    "Bayes decision function:  \n",
    "$$\n",
    "\\hat{\\alpha}(x) = \\arg\\min_{\\alpha \\in \\mathcal{A}}\\mathbb{E}[L(\\alpha(x), y)]\n",
    "$$\n",
    "and we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{R}(\\alpha) &= \\sum_{x,y}L(\\alpha(x), y)P(x,y)\\\\\n",
    "&= \\sum_{x}P(x)\\sum_{y}L(\\alpha(x), y)P(y|x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "therefore,\n",
    "$$\n",
    "\\hat{\\alpha}(x) = \\arg\\min_{\\alpha \\in \\mathcal{A}}\\mathbb{E}[L(\\alpha(x), y)]\\\\\n",
    "\\Leftrightarrow\\\\\n",
    "\\hat{\\alpha}(x) = \\arg\\min_{\\alpha \\in \\mathcal{A}}\\sum_{y}L(\\alpha(x), y)P(y|x)\n",
    "$$\n",
    "for a given x\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{y}L(\\alpha(x), y)P(y|x) &= \\sum_{y = \\alpha(x)}L(\\alpha(x), y)P(y|x) + \\sum_{y \\neq \\alpha(x)}L(\\alpha(x), y)P(y|x)\\\\\n",
    "&= \\sum_{y \\neq \\alpha(x)}L(\\alpha(x), y)P(y|x)\\\\\n",
    "&= \\sum_{y \\neq \\alpha(x)}P(y|x)\\\\\n",
    "&= P(y \\neq \\alpha(x)|x)\\\\\n",
    "&= 1 - P(y = \\alpha(x)|x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "\\arg\\min_{\\alpha \\in \\mathcal{A}}\\sum_{y}L(\\alpha(x), y)P(y|x)\\\\\n",
    "\\Leftrightarrow\\\\\n",
    "\\arg\\max_{\\alpha}P(y = \\alpha(x)|x)\n",
    "$$\n",
    "which is MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Let's see Question 3 in homework1:  \n",
    "Let $y$ be a random variable with a known distribution, and consider the square loss function $\\ell(a, y)=(a-y)^{2}$.  We want to ﬁnd the action $a^{∗}$ that has minimal risk. That is, we want to ﬁnd $a^{*}=\\arg \\min _{a} \\mathbb{E}(a-y)^{2}$, where the expectation is with respect to $y$. Show that $a^{*}=\\mathbb{E} y$, and the Bayes risk (i.e. the risk of $a^{∗}$) is $Var(y)$. In other words, if you want to try to predict the value of a random variable, the best you can do (for minimizing expected square loss) is to predict the mean of the distribution. Your expected loss for predicting the mean will be the variance of the distribution.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{R}(\\alpha) &= E[L(\\alpha(x), y)]\\\\\n",
    "&= E[(\\alpha(x) - y)^2] \\\\\n",
    "&= E[\\alpha(x)^2 - 2\\alpha(x)y + y^2]\\\\\n",
    "&= E[\\alpha(x)^2] - 2E[\\alpha(x)y] + E[y^2]\\\\\n",
    "&= \\alpha(x)^2 - 2\\alpha(x)E[y] + E[y^2]\\\\\n",
    "&= \\alpha(x)^2 - 2\\alpha(x)E[y] + var[y] + E[y]^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "when we want $\\mathcal{R}(\\alpha)$ attains its minimum, we can use treat $\\alpha$ as the variable and apply the properties of quadric fucntion:  \n",
    "$$\\alpha(x) = -\\frac{-2E[y]}{2} = E[y]$$\n",
    "and plug into the fumula\n",
    "$$\\mathcal{R}(\\alpha) = Var[y]$$\n",
    "<br/>\n",
    "\n",
    "Now let’s introduce an input. Recall that the **expected loss** or **“risk”** of a decision function $f: \\mathcal{X} \\rightarrow \\mathcal{A}$ is \n",
    "$$R(f)=\\mathbb{E} \\ell(f(x), y)$$\n",
    "where $(x, y) \\sim P_{\\mathcal{X} \\times \\mathcal{Y}}$, and the **Bayes decision function** $f^{*}: \\mathcal{X} \\rightarrow \\mathcal{A}$ is a function that achieves the _minimal risk_ among all possible functions\n",
    "$$R\\left(f^{*}\\right)=\\inf _{f} R(f)$$\n",
    "Here we consider the regression setting, in which $\\mathcal{A}=\\mathcal{Y}=\\mathbf{R}$, We will show for the square loss $\\ell(a, y)=(a-y)^{2}$, the Bayes decision function is $f^{*}(x)=\\mathbb{E}[y | x]$, where the expectation is over $y$. As before, we assume we know the data-generating distribution $P_{\\mathcal{X} \\times \\mathcal{Y}}$  \n",
    "\n",
    "- (a) We’ll approach this problem by ﬁnding the optimal action for any given $x$. If somebody tells us $x$, we know that the corresponding $y$ is coming from the conditional distribution $y | x$. For a particular $x$, what value should we predict (i.e. what action a should we produce) that has **minimal expected loss**? Express your answer as a decision function $f(x)$, which gives the best action for any given $x$? In mathematical notation, we’re looking for $f^{*}(x)=\\arg \\min _{a} \\mathbb{E}\\left[(a-y)^{2} | x\\right]$, where the expectation is with respect to $y$. (Hint: There is really nothing to do here except write down the answer, based on the previous question. But make sure you understand what’s happening...)  \n",
    "\n",
    "**Answer**:  \n",
    "Based on previous question, $f^{*}(x) = \\mathbb{E}[y | x]$\n",
    "<br/>\n",
    "\n",
    "-  (b) In the previous problem we produced a decision function $f^{∗}(x)$ that minimized the risk for each $x$. In other words, for any other decision function $f(x)$, $f^{∗}(x)$ is going to be at least as good as $f(x)$, for every single $x$. In math, we mean \n",
    "$$\\mathbb{E}\\left[\\left(f^{*}(x)-y\\right)^{2} | x\\right] \\leq \\mathbb{E}\\left[(f(x)-y)^{2} | x\\right],$$\n",
    "for all $x$. To show that $f^{∗}(x)$ is the Bayes decision function, we need to show that \n",
    "$$\\mathbb{E}\\left[\\left(f^{*}(x)-y\\right)^{2}\\right] \\leq \\mathbb{E}\\left[(f(x)-y)^{2}\\right],$$\n",
    "for all $f$.\n",
    "\n",
    "**Answer**  \n",
    "By **Law of iterated expectations** ($E[E(Y | X)] = E[Y]$), since  \n",
    "$$E[Y] = \\sum_{i}E[Y | X = x_{i}]P(X = x_{i}) = E[E(Y | X)],$$\n",
    "then we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[\\left(f^{*}(x)-y\\right)^{2} | x\\right] &\\leq \\mathbb{E}\\left[(f(x)-y)^{2} | x\\right]\\\\\n",
    "\\mathbb{E}[\\mathbb{E}\\left[\\left(f^{*}(x)-y\\right)^{2} | x\\right]] &\\leq \\mathbb{E}[\\mathbb{E}\\left[(f(x)-y)^{2} | x\\right]]\\\\ \n",
    "\\mathbb{E}\\left[\\left(f^{*}(x)-y\\right)^{2} \\right] &\\leq \\mathbb{E}\\left[(f(x)-y)^{2} \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Summary:**  \n",
    "The first question suggests when we don't have input, an appropriate way to \"Guess\" the prediction $y$ is to choose $\\mathbb{E}[Y]$ for minimizing the square loss, but when we have input $X$, we could sightly modify our prediction. Let's look at an example:  \n",
    "\n",
    "**Example:**  \n",
    "Our outcome set is $\\mathcal{Y} = \\{1, 5, 2, 1, 2, 4, 5, 6, 6, 3\\}$, as we know nothing about the input $X$, our action $a^{*} = \\mathbb{E}[Y]$, and in this case, we can minimize the square loss.  \n",
    "However, if we know the input $X$, and our dataset is $\\mathcal{D} = \\{(1,1),(1,5),(1,2),(2,1),(2,2),(3,4),(3,5),(5,6),(5,6),(5,3)\\}$, then if the input $x = 1$, what is the prediction $f^{*}(x)$? It is $\\mathbb{E}[Y | x = 1] = \\frac{1 + 5 + 2}{3} = \\frac{8}{3}$ in this case.  \n",
    "\n",
    "**Problem 2:**  \n",
    "Let's go back to problem 2:  \n",
    "As we know $f^{*}(x)=\\mathbb{E}[Y | X=x]$, Thus the Bayes risk is given by  \n",
    "$$\\mathbb{E}\\left[\\left(f^{*}(X)-Y\\right)^{2}\\right] = \\mathbb{E}\\left[(\\mathbb{E}[Y | X]-Y)^{2}\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[(\\mathbb{E}[Y | X]-Y)^{2} | X\\right]\\right] = \\mathbb{E}[\\operatorname{Var}(Y | X)]$$,  \n",
    "This is reasonable since for a single $x$, the loss is $\\operatorname{Var}(Y | X = x)$  \n",
    "The law of total variance states that:  \n",
    "$$\\operatorname{Var}(Y)=\\mathbb{E}[\\operatorname{Var}(Y | X)]+\\operatorname{Var}[\\mathbb{E}(Y | X)],$$  \n",
    "therefore\n",
    "$$\\mathbb{E}[\\operatorname{Var}(Y | X)]=\\operatorname{Var}(Y)-\\operatorname{Var}[\\mathbb{E}(Y | X)] \\leq \\operatorname{Var}(Y)$$  \n",
    "Recall from Homework 1 that $\\operatorname{Var}(Y)$ is the Bayes risk when we estimate $Y$ without any input $X$. This shows that using $X$ in our estimation reduces the Bayes risk, and that the improvement is measured by $\\operatorname{Var}[\\mathbb{E}(Y | X)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)  \n",
    "- $f^{*}(x) = \\mathbb{E}[Y | x] = \\sum yP(y | X = x) = \\frac{1 + x}{2}$  \n",
    "- We will disscuss details in the homework. $f^{*}(x)$ is the conditional median of $Y$ given $X = x$\n",
    "- $f^{*}(x) = \\arg\\max_{y}P(Y = y | X = x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4)  \n",
    "Empirical Risk:  \n",
    "$$R_{emp} = \\frac{1}{n}\\sum_{i = 0}^nl(\\alpha(x_i), y_i)\\\\\n",
    "E[R_{emp}] = \\frac{1}{n}E[\\sum_{i = 0}^nl(\\alpha(x_i), y_i)]\\\\\n",
    "E[R_{emp}] = \\frac{1}{n}\\sum_{i = 0}^n E[l(\\alpha(x_i), y_i)]\\\\\n",
    "E[R_{emp}] = \\frac{1}{n}\\sum_{i = 0}^n [l(\\alpha(x_i), y_i)]\\\\\n",
    "E[R_{emp}] = \\frac{1}{n}\\lim_{n \\to \\infty}\\sum_{i = 0}^n [l(\\alpha(x_i), y_i)]\\\\\n",
    "E[R_{emp}] = \\mathcal{R}(\\alpha)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6)  \n",
    "(a) Based on the previous questions, the bayesian risk is \n",
    "$$\\mathcal{R}(\\alpha)  = \\int_{x}f(x)\\int_{y}L(\\alpha(x), y)P(y|x),$$   \n",
    "where \n",
    "$$\\alpha(x) = \\arg\\max_{y}f(Y = y | X = x) = a + bx$$ \n",
    "is the bayesian decision rule. In this case,  \n",
    "and the p.d.f of $x \\in [-10, 10]$\n",
    "$$\n",
    "f(x) = \\frac{1}{20}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int_{y}L(\\alpha(x), y)f(y|x) &= 1 - f(y = \\alpha(x)|x)\\\\\n",
    "&= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "and therefore:\n",
    "$$\n",
    "\\mathcal{R}(\\alpha) = 1\n",
    "$$\n",
    "(b)  \n",
    "$\\mathcal{R}(\\alpha) = \\mathbb{E}[\\operatorname{Var}(Y | X)] = \\mathbb{E}[1]$  \n",
    "<br/>\n",
    "(c)\n",
    "let $f^{*}(x) = E[Y | X = x]$,  \n",
    "$f(0) = 1.5, f(1) = 3, f(2.5) = 3.1, f(-4) = 2.1$,  \n",
    "the empirical risk for square loss is $\\frac{1}{10}$  \n",
    "<br/>\n",
    "\n",
    "(d)\n",
    "Using **Least Square** to do linear regression, the empirical risk is 0.2473  \n",
    "<br/>\n",
    "\n",
    "(e)\n",
    "The empirical risk is 0.1928\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
